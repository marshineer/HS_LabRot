{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "# from numpy.linalg import svd as svd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define plot font sizes\n",
    "label_font = 18\n",
    "title_font = 24\n",
    "legend_font = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model Class - Classical Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrosophilaRNN(nn.Module):\n",
    "    def __init__(self, *, n_kc=200, n_mbon=20, n_fbn=60, n_ext=2, n_out=1, n_seed=None):\n",
    "        super().__init__()\n",
    "        # Set the seeds\n",
    "        if n_seed is not None:\n",
    "            np.random.seed(n_seed)\n",
    "            torch.manual_seed(n_seed)\n",
    "        # Set constants\n",
    "        W_KC_MBON_max = 0.05\n",
    "        self.KC_MBON_min = 0. # Minimum synaptic weight\n",
    "        self.KC_MBON_max = W_KC_MBON_max # Maximum synaptic weight\n",
    "#         self.W_KC_MBON_0 = Variable(torch.ones((self.N_MBON, self.N_KC)) * W_KC_MBON_max, requires_grad=False)\n",
    "        self.W_KC_MBON_0 = Variable(torch.ones((n_mbon, n_kc)) * W_KC_MBON_max, requires_grad=False)\n",
    "        self.tau_w = 5  # Time scale of KC->MBON LTD/LTP (plasticity)\n",
    "        self.tau_r = 1  # Time scale of output circuitry activity\n",
    "        self.n_int = 1  # Number of task intervals\n",
    "        # Set the sizes of layers\n",
    "        n_dan = n_mbon\n",
    "        self.N_KC = n_kc\n",
    "        self.N_MBON = n_mbon\n",
    "        self.N_FBN = n_fbn\n",
    "        self.N_DAN = n_dan\n",
    "        self.N_recur = n_mbon + n_fbn + n_dan\n",
    "        self.N_ext = n_ext\n",
    "        self.N_out = n_out\n",
    "        # Define network variables used to store data\n",
    "        self.train_odors = None\n",
    "        self.test_odors = None\n",
    "        # Define updatable network parameters\n",
    "        sqrt2 = torch.sqrt(torch.tensor(2, dtype=torch.float))\n",
    "        mean_MBON = torch.zeros((self.N_recur, n_mbon))\n",
    "        mean_FBN = torch.zeros((self.N_recur, n_fbn))\n",
    "        mean_DAN = torch.zeros((self.N_recur, n_dan))\n",
    "        W_MBON = torch.normal(mean_MBON, torch.sqrt(1 / (sqrt2 * n_mbon)),\n",
    "                              generator=n_seed)\n",
    "        W_FBN = torch.normal(mean_FBN, torch.sqrt(1 / (sqrt2 * n_fbn)),\n",
    "                             generator=n_seed)\n",
    "        W_DAN = torch.normal(mean_DAN, torch.sqrt(1 / (sqrt2 * n_dan)),\n",
    "                             generator=n_seed)\n",
    "        self.W_recur = nn.Parameter(torch.cat((W_MBON, W_FBN, W_DAN), dim=1),\n",
    "                                    requires_grad=True)\n",
    "        self.W_ext = nn.Parameter(torch.randn(n_fbn, n_ext),\n",
    "                                  requires_grad=True)\n",
    "        mean_readout = torch.zeros((n_out, n_mbon))\n",
    "        std_readout = 1 / torch.sqrt(torch.tensor(n_mbon, dtype=torch.float))\n",
    "        self.W_readout = nn.Parameter(torch.normal(mean_readout, std_readout,\n",
    "                                                   generator=n_seed),\n",
    "                                      requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones(self.N_recur) * 0.1,\n",
    "                                 requires_grad=True)\n",
    "        self.beta = nn.Parameter(torch.ones(n_kc) * 0.01,\n",
    "                                 requires_grad=True)\n",
    "\n",
    "    def forward(self, r_KC, r_ext, time, n_batch, W0, r0=None, **kwargs):\n",
    "        \"\"\" Defines the forward pass of the RNN\n",
    "\n",
    "        The KC->MBON weights are constrained to the range [0, 0.05].\n",
    "        MBONs receive external input from Kenyon cells (r_KC i.e. 'odors').\n",
    "        Feedback neurons (FBNs) receive external input (r_ext i.e. 'context').\n",
    "        DAN->MBON weights are permanently set to zero.\n",
    "        DANs receive no external input.\n",
    "\n",
    "        Inputs\n",
    "            r_KC = activity of the Kenyon cell inputs (representing odors)\n",
    "            r_ext = context inputs (representing the conditioning context)\n",
    "            time = time vector for a single interval\n",
    "            n_batch = number of trials in mini-batch\n",
    "            W0 = initial weights for KC->MBON connections\n",
    "            r0 = initial activities for output circuitry neurons\n",
    "\n",
    "        Returns\n",
    "            r_recur: list of torch.ndarray(batch_size, N_MBON + N_FBN + N_DAN)\n",
    "                = time series of activities in the output circuitry\n",
    "            Wt: list of torch.ndarray(batch_size, N_recur, N_recur)\n",
    "                = time series of KC->MBON weights (dopaminergic plasticity)\n",
    "            readout: list of torch.ndarray(batch_size, 1)\n",
    "                = time series of valence readouts (behaviour)\n",
    "        \"\"\"\n",
    "\n",
    "        # Define the time step of the simulation\n",
    "        dt = np.diff(time)[0]\n",
    "\n",
    "        # Initialize output circuit firing rates for each trial\n",
    "        if r0 is not None:\n",
    "            r_init = r0\n",
    "        else:\n",
    "            r_init = torch.ones(n_batch, self.N_recur) * 0.1\n",
    "            r_init[:, :self.N_MBON] = 0\n",
    "        r_recur = [r_init]\n",
    "\n",
    "        # Initialize the eligibility traces and readout\n",
    "        r_bar_KC = r_KC[:, :, 0]\n",
    "        r_bar_DAN = r_recur[-1][:, -self.N_DAN:]\n",
    "        readout = [torch.einsum('bom, bm -> bo',\n",
    "                                self.W_readout.repeat(n_batch, 1, 1),\n",
    "                                r_recur[-1][:, :self.N_MBON]).squeeze()]\n",
    "\n",
    "        # Set the weights DAN->MBON to zero\n",
    "        W_recur = self.W_recur.clone()\n",
    "        W_recur[:self.N_MBON, -self.N_DAN:] = 0\n",
    "\n",
    "        # Set the KC->MBON weights\n",
    "        W_KC_MBON, wt = W0\n",
    "\n",
    "        # Update activity for each time step\n",
    "        for t in range(time.shape[0] - 1):\n",
    "            # Define the input to the output circuitry\n",
    "            I_KC_MBON = torch.einsum('bmk, bk -> bm',\n",
    "                                     W_KC_MBON, r_KC[:, :, t])\n",
    "            I_FBN = torch.einsum('bfe, be -> bf',\n",
    "                                 self.W_ext.repeat(n_batch, 1, 1),\n",
    "                                 r_ext[:, :, t])\n",
    "            I_tot = torch.zeros((n_batch, self.N_recur))\n",
    "            I_tot[:, :self.N_MBON] = I_KC_MBON\n",
    "            I_tot[:, self.N_MBON:self.N_MBON + self.N_FBN] = I_FBN\n",
    "\n",
    "            # Update the output circuitry activity (see Eq. 1)\n",
    "            Wr_prod = torch.einsum('bsr, br -> bs',\n",
    "                                   W_recur.repeat(n_batch, 1, 1),\n",
    "                                   r_recur[-1])\n",
    "            dr = (-r_recur[-1] + F.relu(Wr_prod + self.bias.repeat(n_batch, 1)\n",
    "                                        + I_tot)) / self.tau_r\n",
    "            r_recur.append(r_recur[-1] + dr * dt)\n",
    "\n",
    "            # Update KC->MBON plasticity variables\n",
    "            out = self.wt_update(W_KC_MBON, wt, dt, r_bar_KC, r_bar_DAN,\n",
    "                                 r_KC[:, :, t], r_recur[-1][:, -self.N_DAN:],\n",
    "                                 n_batch, **kwargs)\n",
    "            W_KC_MBON, wt, r_bar_KC, r_bar_DAN = out\n",
    "\n",
    "            # Calculate the readout (see Eq. 2)\n",
    "            readout.append(torch.einsum('bom, bm -> bo',\n",
    "                                        self.W_readout.repeat(n_batch, 1, 1),\n",
    "                                        r_recur[-1][:, :self.N_MBON]).squeeze())\n",
    "\n",
    "        return r_recur, (W_KC_MBON.detach(), wt.detach()), readout\n",
    "\n",
    "    def wt_update(self, W_KC_MBON, wt, dt, r_bar_KC, r_bar_DAN, r_KC, r_DAN,\n",
    "                  n_batch, **kwargs):\n",
    "        \"\"\" Updates the KC->MBON plasticity variables\n",
    "\n",
    "        Synaptic weights from the Kenyon cells to the mushroom body output neurons\n",
    "        (MBONs) are updated dynamically. All other weights are network parameters.\n",
    "        The synaptic connections between Kenyon Cells (KCs) and MBONs are updated\n",
    "        using a LTP/LTD rule (see Figure 1B of Jiang 2020), which models dopamine-\n",
    "        gated neural plasticity on short time scale (behavioural learning).\n",
    "\n",
    "        Parameters\n",
    "            W_KC_MBON: list = KC->MBON weight matrices\n",
    "            wt = dynamic plasticity update\n",
    "            dt = time step of simulation\n",
    "            r_bar_KC = eligibility trace of Kenyon cell activity\n",
    "            r_bar_DAN = eligibility trace of dopaminergic cell activity\n",
    "            r_KC = current activity of Kenyon cells\n",
    "            r_DAN = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the eligibility traces (represent LTP/LTD)\n",
    "        r_bar_KC = r_bar_KC + (r_KC - r_bar_KC) * dt / self.tau_w\n",
    "        r_bar_DAN = r_bar_DAN + (r_DAN - r_bar_DAN) * dt / self.tau_w\n",
    "        # Update the dynamic weight variable\n",
    "        dw = self.calc_dw(r_bar_KC, r_bar_DAN, r_KC, r_DAN, n_batch, **kwargs)\n",
    "        wt += dw * dt\n",
    "        # Update the KC->MBON weights (see Eq. 8)\n",
    "        dW = (-W_KC_MBON + wt) / self.tau_w\n",
    "        W_tp1 = W_KC_MBON + dW * dt\n",
    "        # Clip the KC->MBON weights to the range [0, 0.05]\n",
    "        W_KC_MBON = torch.clamp(W_tp1, self.KC_MBON_min, self.KC_MBON_max)\n",
    "\n",
    "        return W_KC_MBON, wt, r_bar_KC, r_bar_DAN\n",
    "\n",
    "    def calc_dw(self, r_bar_KC, r_bar_DAN, r_KC, r_DAN, n_batch, nps=True, **kwargs):\n",
    "        \"\"\" Calculates the dynamic weight update (see Eq 4).\n",
    "\n",
    "        Parameters\n",
    "            r_bar_KC = eligibility trace of Kenyon cell activity\n",
    "            r_bar_DAN = eligibility trace of dopaminergic cell activity\n",
    "            r_KC = current activity of Kenyon cells\n",
    "            r_DAN = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "            nps = indicates whether non-specific potentiation is included\n",
    "\n",
    "        Returns\n",
    "            update to dynamic plasticity variables wt\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the LTD/LTP terms\n",
    "        prod1 = torch.einsum('bd, bk -> bdk', r_bar_DAN, r_KC)\n",
    "        prod2 = torch.einsum('bd, bk -> bdk', r_DAN, r_bar_KC)\n",
    "        \n",
    "        # Include non-specific potentiation (unless control condition)\n",
    "        if nps:\n",
    "            # Rectify the potentiation parameter\n",
    "            beta = F.relu(self.beta.clone())\n",
    "            # Constrain the potentiation parameter to be positive\n",
    "            prod3 = torch.einsum('bd, bk -> bdk', r_bar_DAN,\n",
    "                                 beta.repeat(n_batch, 1))\n",
    "        else:\n",
    "            prod3 = torch.zeros_like(prod2)\n",
    "            \n",
    "        return prod1 - prod2 + prod3\n",
    "        \n",
    "    def train_net(self, *, opti, T_int=200, T_stim=2, dt=0.5, n_epoch=5000, n_batch=30, clip=0.001, **kwargs):\n",
    "        \"\"\" Trains a network on classical conditioning tasks.\n",
    "\n",
    "        Tasks include first-order or second-order conditioning, and extinction.\n",
    "        Tasks consist of two (first-order) or three (second-order and extinction)\n",
    "        intervals. Each task has its own input generating function. Stimuli are\n",
    "        presented between 5-15s of each interval. Neuron activities are reset\n",
    "        between intervals to prevent associations being represented through\n",
    "        persistent activity.\n",
    "\n",
    "        Parameters\n",
    "            opti = RNN network optimizer\n",
    "            T_int = length of task intervals\n",
    "            T_stim = length of time each stimulus is presented\n",
    "            dt = time step of simulations\n",
    "            n_epoch = number of epochs to train over\n",
    "            n_batch = number of trials in mini-batch\n",
    "            clip = maximum gradient allowed during training\n",
    "\n",
    "        Returns\n",
    "            r_out_epoch = output circuit neuron activities for final epoch\n",
    "            Wt_epoch = KC->MBON weights for final epoch\n",
    "            vt_epoch = readout (i.e. valence) for final epoch\n",
    "            vt_opt = target valence for final epoch\n",
    "            loss_hist = list of losses for all epochs\n",
    "            ls_stims = list of stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Interval time vector\n",
    "        time_int = torch.arange(0, T_int + dt/10, dt)\n",
    "\n",
    "        # Generate a list of stimulus presentation times\n",
    "        stim_times = self.gen_stim_times(T_stim, T_int, dt, n_epoch, n_batch,\n",
    "                                         **kwargs)\n",
    "        # Length of stimulus in indices\n",
    "        stim_len = int(T_stim / dt)\n",
    "\n",
    "        # List to store losses\n",
    "        loss_hist = []\n",
    "        \n",
    "        # Initialize the KC-MBON weights\n",
    "        W_KC_MBON = None\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            # Lists to store activities, weights, readouts and target valences\n",
    "            r_outs = []\n",
    "#             Wts = []\n",
    "            vts = []\n",
    "\n",
    "            # Set the intial KC->MBON weight values for each trial\n",
    "            W_KC_MBON = self.init_w_kc_mbon(W_KC_MBON, n_batch, (epoch, n_epoch))\n",
    "\n",
    "            # Generate odor (r_KC), context (r_ext), and target valence (vt_opt)\n",
    "            st_epoch = stim_times[epoch]\n",
    "            net_inputs = self.gen_inputs(st_epoch, stim_len, time_int.shape[0], \n",
    "                                         n_batch, **kwargs)\n",
    "            r_KC, r_ext, vt_opt, ls_stims = net_inputs\n",
    "\n",
    "            # For each interval in the task\n",
    "            for i in range(self.n_int):\n",
    "                # Run the forward model\n",
    "                net_outs = self(r_KC[i], r_ext[i], time_int, n_batch, W_KC_MBON)\n",
    "                # Set the initial KC->MBON weights for the next interval\n",
    "                W_KC_MBON = net_outs[1]\n",
    "\n",
    "                # Append the interval outputs to lists\n",
    "                r_outs += net_outs[0]\n",
    "#                 Wts += net_outs[1][0]\n",
    "                vts += net_outs[2]\n",
    "\n",
    "            # Concatenate the activities, weights and valences\n",
    "            r_out_epoch = torch.stack(r_outs, dim=-1)\n",
    "#             Wt_epoch = torch.stack(Wts, dim=-1)\n",
    "            vt_epoch = torch.stack(vts, dim=-1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = cond_loss(vt_epoch, vt_opt, r_out_epoch[:, -self.N_DAN:, :])\n",
    "\n",
    "            # Update the network parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print an update\n",
    "            if epoch % 500 == 0:\n",
    "                print(epoch, loss.item())\n",
    "            loss_hist.append(loss.item())\n",
    "\n",
    "#         return r_out_epoch, Wt_epoch, vt_epoch, vt_opt, loss_hist, ls_stims\n",
    "        return r_out_epoch, vt_epoch, vt_opt, loss_hist, ls_stims\n",
    "\n",
    "    def init_w_kc_mbon(self, W_in, n_batch, e_tup):\n",
    "        \"\"\" Initializes the KC->MBON weights for the task.\n",
    "        \n",
    "        KC->MBON weights are reset at the beginning of each epoch.\n",
    "\n",
    "        Parameters\n",
    "            W0 = specified initial weight values or None\n",
    "            n_batch = number of trials in mini-batch\n",
    "            e_tup: tuple = (current epoch, total training epochs)\n",
    "\n",
    "        Returns\n",
    "            W_KC_MBON: list = initial KC->MBON weight matrix\n",
    "            wt = initial dynamic plasticity update\n",
    "        \"\"\"\n",
    "\n",
    "#         W_KC_MBON_0 = torch.ones(n_batch, self.N_MBON, self.N_KC) * self.KC_MBON_max\n",
    "        wt0 = torch.ones(n_batch, self.N_MBON, self.N_KC) * self.KC_MBON_max\n",
    "#         wt0 = self.W_KC_MBON_0.repeat(n_batch, 1, 1)\n",
    "        if W_in is None:\n",
    "#             Wt = W_KC_MBON_0\n",
    "#             wt = wt0\n",
    "#             W_in = (W_KC_MBON_0, wt0)\n",
    "            W_in = (wt0.clone(), wt0.clone())\n",
    "        # Calculate the saturation parameter and modify initial weights\n",
    "        x_sat = min(1, (e_tup[0] / (e_tup[1] / 2)))\n",
    "#         Wt = (1 - x_sat) * W_KC_MBON_0 + x_sat * W_in[0]\n",
    "#         wt = wt0\n",
    "        Wt = W_in[0]\n",
    "        wt = (1 - x_sat) * wt0 + x_sat * W_in[1]\n",
    "        W_in = (Wt, wt)\n",
    "\n",
    "        return W_in\n",
    "\n",
    "    def gen_stim_times(self, T_stim, T_int, dt, n_epoch, n_batch, n_odors=4):\n",
    "        \"\"\" Generates an array of stimulus presentation times for all trials\n",
    "\n",
    "        Parameters\n",
    "            T_stim = length of time each stimulus is presented\n",
    "            T_int = length of task intervals\n",
    "            dt = time step of simulations\n",
    "            n_epoch = number of epochs to train over\n",
    "            n_batch = number of trials in mini-batch\n",
    "\n",
    "        Returns\n",
    "            Array of stimulus presentation times\n",
    "        \"\"\"\n",
    "\n",
    "        # Poisson rate of stimulus presentations\n",
    "        stim_rate = 2 / T_int\n",
    "        \n",
    "        # Initialize stimulus presentation times array\n",
    "        stim_times = [0] * n_epoch\n",
    "\n",
    "        # Generate a list of stimulus presentation times for each trial\n",
    "        for e in range(n_epoch):\n",
    "            batch_times = [0] * n_batch\n",
    "            for b in range(n_batch):\n",
    "                odor_times = [0] * n_odors\n",
    "                for i in range(n_odors):\n",
    "                    trial_times = []\n",
    "                    last_time = 0\n",
    "                    while True:\n",
    "                        stim_isi = -torch.log(torch.rand(1)) / stim_rate\n",
    "                        next_time = last_time + stim_isi\n",
    "                        if next_time < (T_int - 2 * T_stim):\n",
    "                            # Stimulus times are indices (not times)\n",
    "                            trial_times.append((next_time / dt).int())\n",
    "                            last_time += stim_isi\n",
    "                        # Ensure at least one presentation of each stimuli\n",
    "                        elif last_time == 0:\n",
    "                            continue\n",
    "                        else:\n",
    "                            break\n",
    "                    odor_times[i] = torch.stack(trial_times)\n",
    "                batch_times[b] = odor_times\n",
    "            stim_times[e] = batch_times\n",
    "                \n",
    "        return stim_times\n",
    "\n",
    "    def gen_inputs(self, stim_times, stim_len, time_len, n_batch, n_odors=4):\n",
    "        \"\"\" Generates inputs for first-order conditioning tasks.\n",
    "\n",
    "        All trials are either CS+, CS- (US omitted) or CS omitted (control trials\n",
    "        to avoid over-fitting). Of the trials where CS or US is omitted, a second\n",
    "        parameter determines the relative fractions of CS or US trials omitted\n",
    "        (p_omit_CS). See Fig. 2 of Jiang 2020 to determine sequencing of stimuli\n",
    "        during training. To account for the sequential nature of numerical\n",
    "        simulations, the target valence begins one time step after stimulus\n",
    "        onset. Details provided in Jiang 2020 -> Methods -> Conditioning Tasks.\n",
    "\n",
    "        The mix of conditions is listed as follows:\n",
    "            probability of CS+ trials = 1 - p_omit\n",
    "            probability of CS- trials = p_omit * 0.3\n",
    "            probability of control trials = p_omit * 0.7\n",
    "\n",
    "        Parameters\n",
    "            stim_times = indices of stimulus presentations for each interval\n",
    "            stim_len = length of stimulus presentation (in indices)\n",
    "            time_len = size of time vector\n",
    "            n_batch = number of trials in mini-batch\n",
    "            p_omit = probability of omitting either CS or US from trials\n",
    "\n",
    "        Returns\n",
    "            r_KCt_ls = odor (KC) input time series arrays for each interval\n",
    "            r_extt_ls = context (ext) input time series arrays for each interval\n",
    "            vt_opt = target valence for plotting and loss calculations\n",
    "            ls_stims = stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Number of active neurons in an odor\n",
    "        n_ones = int(self.N_KC * 0.1)\n",
    "\n",
    "        # Initialize activity matrices\n",
    "        r_KCt = torch.zeros(n_batch, self.N_KC, time_len)\n",
    "        r_extt = torch.zeros(n_batch, self.N_ext, time_len)\n",
    "\n",
    "        # Initialize lists and arrays to store stimulus time series\n",
    "        ls_CS = []\n",
    "        time_US_all = torch.zeros(n_batch, time_len)\n",
    "        vt_opt = torch.zeros(n_batch, time_len)\n",
    "\n",
    "        # For each batch, randomly generate different odors and presentation times\n",
    "        for i in range(n_odors):\n",
    "            # Initialize the CS time matrix\n",
    "            time_CS = torch.zeros(n_batch, time_len)\n",
    "            time_US = torch.zeros_like(time_CS)\n",
    "\n",
    "            # Conditioned stimuli (CS) = odors\n",
    "            r_KC = torch.zeros(n_batch, self.N_KC)\n",
    "            # Unconditioned stimuli (US) = context\n",
    "#             r_ext = torch.multinomial(torch.ones(n_batch, self.N_ext), self.N_ext)\n",
    "            if i == 0:\n",
    "                r_ext = torch.tensor([1, 0]).repeat(n_batch, 1)\n",
    "            elif i == 1:\n",
    "                r_ext = torch.tensor([0, 1]).repeat(n_batch, 1)\n",
    "            else:\n",
    "                r_ext = torch.tensor([0, 0]).repeat(n_batch, 1)\n",
    "\n",
    "            # For each trial\n",
    "            for b in range(n_batch):\n",
    "                # Define an odor (CS)\n",
    "                r_KC_inds = torch.multinomial(torch.ones(self.N_KC), n_ones)\n",
    "                r_KC[b, r_KC_inds] = 1\n",
    "\n",
    "                for j, st in enumerate(stim_times[b][i]):\n",
    "                    # Set the CS input times\n",
    "                    stim_inds = st + torch.arange(stim_len)\n",
    "                    time_CS[b, stim_inds] = 1\n",
    "\n",
    "                    # For CS+ odors, set US and the valence\n",
    "                    if i < 2:\n",
    "                        # Set the US input times\n",
    "                        time_US[b, (stim_inds + stim_len)] = 1\n",
    "                        # Set a target valence on every presentation but the first\n",
    "                        if j > 0:\n",
    "                            if r_ext[b, 0] == 1:\n",
    "                                vt_opt[b, (stim_inds + 1)] = 1\n",
    "                            else:\n",
    "                                vt_opt[b, (stim_inds + 1)] = -1\n",
    "\n",
    "            # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "            r_KCt += torch.einsum('bm, mbt -> bmt', r_KC, time_CS.repeat(self.N_KC, 1, 1))\n",
    "            r_extt += torch.einsum('bm, mbt -> bmt', r_ext, time_US.repeat(self.N_ext, 1, 1))\n",
    "            ls_CS += time_CS\n",
    "            time_US_all += time_US\n",
    "\n",
    "        # Make a list of stimulus times to plot\n",
    "        ls_stims = ls_CS + [time_US_all]\n",
    "\n",
    "        return [r_KCt], [r_extt], vt_opt, ls_stims\n",
    "        \n",
    "        \n",
    "# Clipping weights between [0, 0.05]\n",
    "# https://discuss.pytorch.org/t/how-to-do-constrained-optimization-in-pytorch/60122\n",
    "# https://discuss.pytorch.org/t/set-constraints-on-parameters-or-layers/23620\n",
    "# https://discuss.pytorch.org/t/restrict-range-of-variable-during-gradient-descent/1933/4\n",
    "\n",
    "# Setting DAN->MBON weights to zero\n",
    "# https://pytorch.org/docs/stable/generated/torch.triu.html\n",
    "\n",
    "# Broadcasting using einsum\n",
    "# https://github.com/pytorch/pytorch/issues/15671\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function for conditioning tasks\n",
    "def cond_loss(vt, vt_opt, r_DAN, lam=0.1):\n",
    "    \"\"\" Calculates the loss for conditioning tasks.\n",
    "    \n",
    "    Composed of an MSE cost based on the difference between output and\n",
    "    target valence, and a regularization cost that penalizes excess\n",
    "    dopaminergic activity. Reference Eqs. (3) and (9) in Jiang 2020.\n",
    "    \n",
    "    Parameters\n",
    "        vt = time dependent valence output of network\n",
    "        vt_opt = target valence (must be a torch tensor)\n",
    "        r_DAN = time series of dopaminergic neuron activities\n",
    "        lam = regularization constant\n",
    "    \n",
    "    Returns\n",
    "        loss_tot = scalar loss used in backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the baseline DAN activity\n",
    "    DAN_baseline = 0.1\n",
    "    \n",
    "    # Calculate the MSE loss of the valence\n",
    "    v_sum = torch.mean((vt - vt_opt)**2, dim=1)\n",
    "    v_loss = torch.mean(v_sum)\n",
    "    \n",
    "    # Calculate regularization term\n",
    "    r_sum = torch.sum(F.relu(r_DAN - 0.1)**2, dim=1)\n",
    "    r_loss = lam * torch.mean(r_sum, dim=1)\n",
    "    \n",
    "    # Calculate the summed loss (size = n_batch)\n",
    "    loss = v_loss + r_loss\n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    loss_tot = torch.mean(loss)\n",
    "    \n",
    "    return loss_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trial(network, dt=0.5, nps=True):\n",
    "    \"\"\" Plots a figure similar to Figure 2 from Jiang 2020.\n",
    "    \n",
    "    Runs the network using a novel combination of conditioned and unconditioned stimuli,\n",
    "    then prints the results. Top: time series of the various stimuli (CS and US), as well as\n",
    "    the target valence and readout. Bottom: activity of eight randomly chosen mushroom body\n",
    "    output neurons (MBONs).\n",
    "    \n",
    "    Paramters\n",
    "        network = previously trained RNN\n",
    "        dt = time step of the simulation/plot\n",
    "        nps = boolean indicating whether to include non-specific potentiation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define plot font sizes\n",
    "    label_font = 18\n",
    "    title_font = 24\n",
    "    legend_font = 12\n",
    "\n",
    "    # Run the network\n",
    "    r_out, Wt, vt, vt_opt, loss_hist, stim_ls = train_net(network, dt=dt, n_epochs=1, n_batch=1, train=False, nps=nps)\n",
    "    r_out = r_out.detach().numpy().squeeze()\n",
    "    vt = vt.detach().numpy().squeeze()\n",
    "    vt_opt = vt_opt.detach().numpy().squeeze()\n",
    "    plot_US = stim_ls[-1].numpy().squeeze()\n",
    "    plot_time = np.arange(plot_US.size) * dt\n",
    "\n",
    "    # Plot the conditioning and test\n",
    "    CS_labels = ['CS1+', 'CS2+', 'CS1-', 'CS2-']\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), sharex=True, gridspec_kw={'height_ratios': [1, 4]})\n",
    "    ax1.plot(plot_time, vt, label='Readout')\n",
    "    ax1.plot(plot_time, vt_opt, label='Target')\n",
    "    for i in range(len(stim_ls) - 1):\n",
    "        plot_CS = stim_ls[i].numpy().squeeze()\n",
    "        ax1.plot(plot_time, plot_CS, label=CS_labels[i])\n",
    "    ax1.plot(plot_time, plot_US, label='US')\n",
    "    ax1.set_ylabel('Value', fontsize=label_font)\n",
    "    ax1.set_title('Continual Learning', fontsize=title_font)\n",
    "    ax1.legend(fontsize=legend_font)\n",
    "\n",
    "    # Plot the activities of a few MBONs\n",
    "    plot_neurs = np.random.choice(network.N_MBON, size=8, replace=False)\n",
    "    r_max = np.max(r_out)\n",
    "    for i, n in enumerate(plot_neurs):\n",
    "        ax2.plot(plot_time, (r_out[n, :] / r_max) + (i * 2 / 3), '-k')\n",
    "    ax2.set_xlabel('Time', fontsize=label_font)\n",
    "    ax2.set_ylabel('Normalized Activity', fontsize=label_font)\n",
    "    ax2.set_yticks([])\n",
    "    fig.tight_layout();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([60, 2])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([100])\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "classic_net = DrosophilaRNN()\n",
    "for param in classic_net.parameters():\n",
    "    print(param.shape)\n",
    "#     print(param)\n",
    "# print(classic_net.N_DAN)\n",
    "\n",
    "# Define the model's optimizer\n",
    "lr = 0.001\n",
    "optimizer = optim.RMSprop(classic_net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.080031633377075\n",
      "500 0.00406885938718915\n",
      "1000 0.0035918056964874268\n",
      "1500 0.006271624471992254\n",
      "2000 0.005959145724773407\n",
      "2500 0.01612963154911995\n",
      "3000 0.02860703133046627\n",
      "3500 0.022461295127868652\n",
      "4000 0.0200502946972847\n",
      "4500 0.02460894174873829\n"
     ]
    }
   ],
   "source": [
    "train_bool = True\n",
    "if train_bool:\n",
    "    r_out, vt, vt_opt, loss_hist, _ = classic_net.train_net(opti=optimizer, n_epoch=5000, n_odors=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_bool:\n",
    "    # Plot the loss function\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "    axes[0].plot(loss_hist)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=label_font)\n",
    "    axes[0].set_ylabel('Loss', fontsize=label_font)\n",
    "    axes[1].plot(loss_hist[5:])\n",
    "    axes[1].set_xlabel('Epoch', fontsize=label_font)\n",
    "    axes[1].set_ylabel('Loss', fontsize=label_font)\n",
    "    fig.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classic_net.state_dict(), 'trained_N9_continual_learning_wt_pass.pt')\n",
    "\n",
    "# https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some results to compare to Fig 2A/B in paper\n",
    "if not train_bool:\n",
    "#     classic_net.load_state_dict(torch.load('trained_N9_continual_learning_wt_inc.pt'))\n",
    "    classic_net.eval()\n",
    "\n",
    "# if train_task == 'first-order':\n",
    "#     print_trial(classic_net, task1=train_task, task2='CS+')\n",
    "#     print_trial(classic_net, task1=train_task, task2='CS-')\n",
    "# elif train_task == 'all_tasks':\n",
    "#     print_trial(classic_net, task1=train_task, task2='extinct')\n",
    "#     print_trial(classic_net, task1=train_task, task2='CS2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.014901109971106052\n",
      "0.01320392092806287 0.006412794194254094\n"
     ]
    }
   ],
   "source": [
    "# r_out, Wt, vt, vt_opt, loss_hist, _ = train_net(classic_net, task=train_task, n_epochs=200, train=False)\n",
    "# r_out, Wt, vt, vt_opt, loss_hist, _ = classic_net.train_net(opti=optimizer, n_epoch=100)\n",
    "r_out, vt, vt_opt, loss_hist, _ = classic_net.train_net(opti=optimizer, n_epoch=200)\n",
    "print(np.mean(loss_hist), np.std(loss_hist))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
