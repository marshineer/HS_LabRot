{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "# from numpy.linalg import svd as svd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define plot font sizes\n",
    "label_font = 18\n",
    "title_font = 24\n",
    "legend_font = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, KC_size = 200, MBON_size = 20, DAN_size = 20, FBN_size = 60, ext_size = 2, out_size = 1, net_seed=1234):\n",
    "        super(RNN, self).__init__()\n",
    "        # Set the seeds\n",
    "#         np.random.seed(net_seed)\n",
    "#         torch.manual_seed(net_seed)\n",
    "        # Set constants\n",
    "        self.dt = 0.5 # seconds\n",
    "        self.KC_MBON_min = 0. # Minimum synaptic weight\n",
    "        self.KC_MBON_max = 0.05 # Maximum synaptic weight\n",
    "        self.tau_W = 5 # Time scale of KC->MBON LTD/LTP (plasticity)\n",
    "        self.tau_r = 1 # Time scale of output circuitry activity\n",
    "        # Set the sizes of layers\n",
    "        self.N_KC = KC_size\n",
    "        self.N_MBON = MBON_size\n",
    "        self.N_DAN = DAN_size\n",
    "        self.N_recur = FBN_size + MBON_size + DAN_size\n",
    "        self.N_ext = ext_size\n",
    "        self.N_out = out_size\n",
    "        # Define dynamic variables\n",
    "        self.W_KC_MBON = Variable(torch.ones(MBON_size, KC_size) * self.KC_MBON_max, requires_grad=False)\n",
    "        self.wt = \n",
    "        # Define updatable network parameters\n",
    "        self.W_recur = nn.Parameter(torch.randn(self.N_recur, self.N_recur), requires_grad=True)\n",
    "        self.W_readout = nn.Parameter(torch.randn(out_size, MBON_size), requires_grad=True)\n",
    "        self.W_ext = nn.Parameter(torch.randn(FBN_size, ext_size), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones(self.N_recur) * 0.1, requires_grad=True)\n",
    "        # Initialize neuron activity variables\n",
    "#         self.r_MBON = Variable(torch.zeros(MBON_size), requires_grad=False)\n",
    "#         self.r_DAN = Variable(torch.ones(DAN_size) * 0.1, requires_grad=False)\n",
    "#         self.r_FBN = Variable(torch.ones(FBN_size) * 0.1, requires_grad=False)\n",
    "#         self.r_recur = torch.cat((self.r_MBON, self.r_DAN, self.r_FBN))\n",
    "#         self.r_recur = Variable(torch.ones(self.N_recur) * 0.1, requires_grad=False)\n",
    "#         self.r_recur[:MBON_size] = 0\n",
    "#         r_init = Variable(torch.ones(self.N_recur) * 0.1, requires_grad=False)\n",
    "#         r_init[:MBON_size] = 0\n",
    "#         self.r_recur = []\n",
    "#         self.r_recur.append(r_init)\n",
    "#         self.r_KC = Variable(torch.ones(KC_size) * 0.1, requires_grad=False)\n",
    "        self.r_recur = []\n",
    "        \n",
    "        def KC_MBON_update(self, wt):\n",
    "            \"\"\" Updates the synaptic weights from the Keyon cells to the output neurons.\n",
    "            Updates the synaptic connections between inputs (KCs) and output neurons (MBONs) using a LTP/LTD\n",
    "            rule (see Figure 1B of Jiang 2020). Models dopamine-gated neural plasticity on short time scale.\n",
    "            The KC->MBON weights are constrained to the range [0, 0.05].\n",
    "            \n",
    "            Paramters\n",
    "                wt = dynamic weight update variable (see Eq 4 of Jiang 2020)\n",
    "            \"\"\"\n",
    "            \n",
    "            self.W_KC_MBON = (-self.W_KC_MBON + wt) / self.tau_W\n",
    "            # Constrain weights to lie in [0, 0.05]\n",
    "            pass\n",
    "            \n",
    "            \n",
    "#         def calc_dwdt(self, r_KC, r_DAN, ht = torch.exp):\n",
    "#             \"\"\" Calculates the dynamic weight variable update for KC->MBON connections. \"\"\"\n",
    "            \n",
    "#             # Calculate the eligibility traces\n",
    "#             DAN_trace = ht[:-(t+1)] * r_DAN[t] * dt / tau_w\n",
    "#             self.wt = self.wt + (r_DAN_trace * r_KC - r_KC_trace * r_DAN) * self.dt\n",
    "            \n",
    "            \n",
    "        def forward(self, r_KC, r_ext, T, dt=0.5, et_kernel=torch.exp):\n",
    "            \"\"\" Defines the forward pass of the RNN\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            # Define the length of the simulation\n",
    "            t_steps = int(T / dt)\n",
    "            time = np.arange(t_steps) * dt\n",
    "            \n",
    "            # Define the low-pass filter for eligibility traces\n",
    "            \n",
    "            \n",
    "            # Initialize a list to store the activities\n",
    "            r_init = torch.ones(self.N_recur) * 0.1\n",
    "            r_init[:self.N_MBON] = 0\n",
    "            r_out_circ = torch.zeros(self.N_recur, t_steps+1)\n",
    "#             r_out_circ = []\n",
    "#             r_out_circ.append(r_init)\n",
    "            \n",
    "            # Set the weights DAN->MBON to zero\n",
    "#             self.W_recur[:self.N_MBON, -self.N_DAN:] = 0\n",
    "            \n",
    "            # Update activity for each time step\n",
    "            for t in range(t_steps):\n",
    "                # Define the input to the output circuitry\n",
    "                I_KC_MBON = self.W_KC_MBON @ r_KC\n",
    "                I_FBAN = self.W_ext @ r_ext\n",
    "                I = torch.zeros(self.N_recur, requires_grad=False)\n",
    "                I[:self.N_MBON] = I_KC_MBON\n",
    "                I[self.N_MBON:self.N_MBON + self.N_FBN] = I_FBN\n",
    "\n",
    "                # Update the output circuitry activity\n",
    "                dr = F.relu(-self.r_recur + self.W_recur @ self.r_recur + self.b + I)\n",
    "#                 rtp1 = self.r_recur[t] + dr * dt / self.tau_r\n",
    "#                 self.r_recur.append(rtp1)\n",
    "#                 rtp1 = r_out_circ[t] + dr * dt / self.tau_r\n",
    "#                 r_out_circ.append(rtp1)\n",
    "                rtp1 = r_out_circ[:, t] + dr * dt / self.tau_r\n",
    "                r_out_circ[:, t+1] = rtp1\n",
    "        \n",
    "                # Update KC->MBON plasticity variables\n",
    "                # Calculate the eligibility traces\n",
    "                DAN_trace = ht[:-(t+1)] * r_DAN[t] * dt / tau_w\n",
    "                KC_trace = ht[:-(t+1)] * r_KC[t] * dt / tau_w\n",
    "            \n",
    "            self.r_recur.append(r_out_circ)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "# torch.einsum(\"abc, cd -> abd\", ht, w)\n",
    "# a: batch size\n",
    "# b: time steps (not there in for loops)\n",
    "# c: dims\n",
    "# d: dims (==c if recurrent weights)\n",
    "\n",
    "# Clipping weights between [0, 0.05]\n",
    "# https://discuss.pytorch.org/t/how-to-do-constrained-optimization-in-pytorch/60122\n",
    "# https://discuss.pytorch.org/t/set-constraints-on-parameters-or-layers/23620\n",
    "# https://discuss.pytorch.org/t/restrict-range-of-variable-during-gradient-descent/1933/4\n",
    "\n",
    "# Setting DAN->MBON weights to zero\n",
    "# https://pytorch.org/docs/stable/generated/torch.triu.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
