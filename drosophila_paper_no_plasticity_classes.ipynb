{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "# from numpy.linalg import svd as svd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define plot font sizes\n",
    "label_font = 18\n",
    "title_font = 24\n",
    "legend_font = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model Class - Conditioning Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrosophilaRNN(nn.Module):\n",
    "    def __init__(self, *, n_kc=200, n_mbon=20, n_fbn=60, n_ext=2, n_out=1, n_seed=None):\n",
    "        super().__init__()\n",
    "        # Set the seeds\n",
    "        if n_seed is not None:\n",
    "            np.random.seed(n_seed)\n",
    "            torch.manual_seed(n_seed)\n",
    "        # Set constants\n",
    "        W_KC_MBON_max = 0.05\n",
    "        self.KC_MBON_min = 0. # Minimum synaptic weight\n",
    "        self.KC_MBON_max = W_KC_MBON_max # Maximum synaptic weight\n",
    "        self.W_KC_MBON_0 = Variable(torch.ones((n_mbon, n_kc)) * W_KC_MBON_max, requires_grad=False)\n",
    "        self.tau_w = 5 # Time scale of KC->MBON LTD/LTP (plasticity)\n",
    "        self.tau_r = 1 # Time scale of output circuitry activity\n",
    "        self.n_int = 1  # Number of task intervals\n",
    "        # Set the sizes of layers\n",
    "        n_dan = n_mbon\n",
    "        self.N_KC = n_kc\n",
    "        self.N_MBON = n_mbon\n",
    "        self.N_FBN = n_fbn\n",
    "        self.N_DAN = n_dan\n",
    "        self.N_recur = n_mbon + n_fbn + n_dan\n",
    "        self.N_ext = n_ext\n",
    "        self.N_out = n_out\n",
    "        # Define network variables used to store data\n",
    "        self.train_odors = None\n",
    "        self.test_odors = None\n",
    "        # Define updatable network parameters\n",
    "        sqrt2 = torch.sqrt(torch.tensor(2, dtype=torch.float))\n",
    "        mean_MBON = torch.zeros((self.N_recur, n_mbon))\n",
    "        mean_FBN = torch.zeros((self.N_recur, n_fbn))\n",
    "        mean_DAN = torch.zeros((self.N_recur, n_dan))\n",
    "        W_MBON = torch.normal(mean_MBON, torch.sqrt(1 / (sqrt2 * n_mbon)),\n",
    "                              generator=n_seed)\n",
    "        W_FBN = torch.normal(mean_FBN, torch.sqrt(1 / (sqrt2 * n_fbn)),\n",
    "                             generator=n_seed)\n",
    "        W_DAN = torch.normal(mean_DAN, torch.sqrt(1 / (sqrt2 * n_dan)),\n",
    "                             generator=n_seed)\n",
    "        self.W_recur = nn.Parameter(torch.cat((W_MBON, W_FBN, W_DAN), dim=1),\n",
    "                                    requires_grad=True)\n",
    "        self.W_ext = nn.Parameter(torch.randn(n_fbn, n_ext),\n",
    "                                  requires_grad=True)\n",
    "        mean_readout = torch.zeros((n_out, n_mbon))\n",
    "        std_readout = 1 / torch.sqrt(torch.tensor(n_mbon, dtype=torch.float))\n",
    "        self.W_readout = nn.Parameter(torch.normal(mean_readout, std_readout,\n",
    "                                                   generator=n_seed),\n",
    "                                      requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones(self.N_recur) * 0.1,\n",
    "                                 requires_grad=True)\n",
    "\n",
    "    def forward(self, r_KC, r_ext, time, n_batch, W0, r0=None, **kwargs):\n",
    "        \"\"\" Defines the forward pass of the RNN\n",
    "\n",
    "        The KC->MBON weights are constrained to the range [0, 0.05].\n",
    "        MBONs receive external input from Kenyon cells (r_KC i.e. 'odors').\n",
    "        Feedback neurons (FBNs) receive external input (r_ext i.e. 'context').\n",
    "        DAN->MBON weights are permanently set to zero.\n",
    "        DANs receive no external input.\n",
    "\n",
    "        Inputs\n",
    "            r_KC = activity of the Kenyon cell neurons (representing odors)\n",
    "            r_ext = context signals (representing the conditioning context)\n",
    "            time = time vector for a single interval\n",
    "            n_batch = number of trials in mini-batch\n",
    "            W0 = initial weights for KC->MBON connections\n",
    "            r0 = initial activities for output circuitry neurons\n",
    "\n",
    "        Returns\n",
    "            r_recur: list of torch.ndarray(batch_size, N_MBON + N_FBN + N_DAN)\n",
    "                = time series of activities in the output circuitry\n",
    "            Wt: list of torch.ndarray(batch_size, N_recur, N_recur)\n",
    "                = time series of KC->MBON weights (dopaminergic plasticity)\n",
    "            readout: list of torch.ndarray(batch_size, 1)\n",
    "                = time series of valence readouts (behaviour)\n",
    "        \"\"\"\n",
    "\n",
    "        # Define the time step of the simulation\n",
    "        dt = np.diff(time)[0]\n",
    "\n",
    "        # Initialize output circuit firing rates for each trial\n",
    "        if r0 is not None:\n",
    "            r_init = r0\n",
    "        else:\n",
    "            r_init = torch.ones(n_batch, self.N_recur) * 0.1\n",
    "            r_init[:, :self.N_MBON] = 0\n",
    "        r_recur = [r_init]\n",
    "\n",
    "        # Initialize the eligibility traces and readout\n",
    "        r_bar_KC = r_KC[:, :, 0]\n",
    "        r_bar_DAN = r_recur[-1][:, -self.N_DAN:]\n",
    "        readout = [torch.einsum('bom, bm -> bo',\n",
    "                                self.W_readout.repeat(n_batch, 1, 1),\n",
    "                                r_recur[-1][:, :self.N_MBON]).squeeze()]\n",
    "\n",
    "        # Set the weights DAN->MBON to zero\n",
    "        W_recur = self.W_recur.clone()\n",
    "        W_recur[:self.N_MBON, -self.N_DAN:] = 0\n",
    "\n",
    "        # Set the KC->MBON weights\n",
    "        W_KC_MBON, wt = W0\n",
    "\n",
    "        # Update activity for each time step\n",
    "        for t in range(time.shape[0] - 1):\n",
    "            # Define the input to the output circuitry\n",
    "            I_KC_MBON = torch.einsum('bmk, bk -> bm',\n",
    "                                     W_KC_MBON, r_KC[:, :, t])\n",
    "            I_FBN = torch.einsum('bfe, be -> bf',\n",
    "                                 self.W_ext.repeat(n_batch, 1, 1),\n",
    "                                 r_ext[:, :, t])\n",
    "            I_tot = torch.zeros((n_batch, self.N_recur))\n",
    "            I_tot[:, :self.N_MBON] = I_KC_MBON\n",
    "            I_tot[:, self.N_MBON:self.N_MBON + self.N_FBN] = I_FBN\n",
    "\n",
    "            # Update the output circuitry activity (see Eq. 1)\n",
    "            Wr_prod = torch.einsum('bsr, br -> bs',\n",
    "                                   W_recur.repeat(n_batch, 1, 1),\n",
    "                                   r_recur[-1])\n",
    "            dr = (-r_recur[-1] + F.relu(Wr_prod + self.bias.repeat(n_batch, 1)\n",
    "                                        + I_tot)) / self.tau_r\n",
    "            r_recur.append(r_recur[-1] + dr * dt)\n",
    "\n",
    "            # Update KC->MBON plasticity variables\n",
    "            out = self.wt_update(W_KC_MBON, wt, dt, r_bar_KC, r_bar_DAN,\n",
    "                                 r_KC[:, :, t], r_recur[-1][:, -self.N_DAN:],\n",
    "                                 n_batch, **kwargs)\n",
    "            W_KC_MBON, wt, r_bar_KC, r_bar_DAN = out\n",
    "\n",
    "            # Calculate the readout (see Eq. 2)\n",
    "            readout.append(torch.einsum('bom, bm -> bo',\n",
    "                                        self.W_readout.repeat(n_batch, 1, 1),\n",
    "                                        r_recur[-1][:, :self.N_MBON]).squeeze())\n",
    "\n",
    "        return r_recur, (W_KC_MBON.detach(), wt.detach()), readout\n",
    "\n",
    "    def wt_update(self, W_KC_MBON, wt, dt, r_bar_KC, r_bar_DAN, r_KC, r_DAN,\n",
    "                  n_batch, **kwargs):\n",
    "        \"\"\" Updates the KC->MBON plasticity variables\n",
    "\n",
    "        Synaptic weights from the Kenyon cells to the mushroom body output neurons\n",
    "        (MBONs) are updated dynamically. All other weights are network parameters.\n",
    "        The synaptic connections between Kenyon Cells (KCs) and MBONs are updated\n",
    "        using a LTP/LTD rule (see Figure 1B of Jiang 2020), which models dopamine-\n",
    "        gated neural plasticity on short time scale (behavioural learning).\n",
    "\n",
    "        Parameters\n",
    "            W_KC_MBON: list = KC->MBON weight matrices\n",
    "            wt = dynamic plasticity update\n",
    "            dt = time step of simulation\n",
    "            r_bar_KC = eligibility trace of Kenyon cell activity\n",
    "            r_bar_DAN = eligibility trace of dopaminergic cell activity\n",
    "            r_KC = current activity of Kenyon cells\n",
    "            r_DAN = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "        \"\"\"\n",
    "\n",
    "        return W_KC_MBON, wt, r_bar_KC, r_bar_DAN\n",
    "\n",
    "    def calc_dw(self, r_bar_KC, r_bar_DAN, r_KC, r_DAN, n_batch, **kwargs):\n",
    "        \"\"\" Calculates the dynamic weight update (see Eq 4).\n",
    "\n",
    "        Parameters\n",
    "            r_bar_KC = eligibility trace of Kenyon cell activity\n",
    "            r_bar_DAN = eligibility trace of dopaminergic cell activity\n",
    "            r_KC = current activity of Kenyon cells\n",
    "            r_DAN = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "\n",
    "        Returns\n",
    "            update to dynamic plasticity variables wt\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the LTD/LTP terms\n",
    "        prod1 = torch.einsum('bd, bk -> bdk', r_bar_DAN, r_KC)\n",
    "        prod2 = torch.einsum('bd, bk -> bdk', r_DAN, r_bar_KC)\n",
    "\n",
    "        return prod1 - prod2\n",
    "            \n",
    "    def train_net(self, *, opti, T_int=30, T_stim=2, dt=0.5, n_epoch=5000, n_batch=30, clip=0.001, **kwargs):\n",
    "        \"\"\" Trains a network on classical conditioning tasks.\n",
    "\n",
    "        Tasks include first-order or second-order conditioning, and extinction. Tasks consist of\n",
    "        two (first-order) or three (second-order and extinction) intervals. Each task has its own\n",
    "        input generating function. Stimuli are presented between 5-15s of each interval. Neuron\n",
    "        activities are reset between intervals to prevent associations being represented through\n",
    "        persistent activity.\n",
    "\n",
    "        Parameters\n",
    "            opti = RNN network optimizer\n",
    "            T_int = length of task intervals (eg conditioning, test, extinction)\n",
    "            T_stim = length of time each stimulus is presented\n",
    "            dt = time step of simulations\n",
    "            n_epoch = number of epochs to train over\n",
    "            n_batch = number of trials in mini-batch\n",
    "            clip = maximum gradient allowed during training\n",
    "\n",
    "        Returns\n",
    "            r_out_epoch = output circuit neuron activities for final epoch\n",
    "            Wt_epoch = KC->MBON weights for final epoch\n",
    "            vt_epoch = readout (i.e. valence) for final epoch\n",
    "            vt_opt = target valence for final epoch\n",
    "            loss_hist = list of losses for all epochs\n",
    "            ls_stims = list of stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Interval time vector\n",
    "        time_int = torch.arange(0, T_int + dt/10, dt)\n",
    "\n",
    "        # Generate a list of stimulus presentation times\n",
    "        stim_times = self.gen_stim_times(T_stim, T_int, dt, n_epoch, n_batch,\n",
    "                                         **kwargs)\n",
    "        # Length of stimulus in indices\n",
    "        stim_len = int(T_stim / dt)\n",
    "\n",
    "        # List to store losses\n",
    "        loss_hist = []\n",
    "        \n",
    "        # Initialize the KC-MBON weights\n",
    "        W_KC_MBON = None\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            # Lists to store activities, weights, readouts and target valences\n",
    "            r_outs = []\n",
    "#             Wts = []\n",
    "            vts = []\n",
    "\n",
    "            # Set the intial KC->MBON weight values for each trial\n",
    "            W_KC_MBON = self.init_w_kc_mbon(W_KC_MBON, n_batch, (epoch, n_epoch))\n",
    "\n",
    "            # Generate odor (r_KC), context (r_ext), and target valence (vt_opt)\n",
    "            st_epoch = stim_times[epoch]\n",
    "            net_inputs = self.gen_inputs(st_epoch, stim_len, time_int.shape[0], \n",
    "                                         n_batch, **kwargs)\n",
    "            r_KC, r_ext, vt_opt, ls_stims = net_inputs\n",
    "\n",
    "            # For each interval in the task\n",
    "            for i in range(self.n_int):\n",
    "                # Run the forward model\n",
    "                net_outs = self(r_KC[i], r_ext[i], time_int, n_batch, W_KC_MBON)\n",
    "                # Set the initial KC->MBON weights for the next interval\n",
    "                W_KC_MBON = net_outs[1]\n",
    "\n",
    "                # Append the interval outputs to lists\n",
    "                r_outs += net_outs[0]\n",
    "#                 Wts += net_outs[1][0]\n",
    "                vts += net_outs[2]\n",
    "\n",
    "            # Concatenate the activities, weights and valences\n",
    "            r_out_epoch = torch.stack(r_outs, dim=-1)\n",
    "#             Wt_epoch = torch.stack(Wts, dim=-1)\n",
    "            vt_epoch = torch.stack(vts, dim=-1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = cond_loss(vt_epoch, vt_opt, r_out_epoch[:, -self.N_DAN:, :])\n",
    "\n",
    "            # Update the network parameters\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), clip)\n",
    "            opti.step()\n",
    "\n",
    "            # Print an update\n",
    "            if epoch % 500 == 0:\n",
    "                print(epoch, loss.item())\n",
    "            loss_hist.append(loss.item())\n",
    "\n",
    "#         return r_out_epoch, Wt_epoch, vt_epoch, vt_opt, loss_hist, ls_stims\n",
    "        return r_out_epoch, vt_epoch, vt_opt, loss_hist, ls_stims\n",
    "\n",
    "    def init_w_kc_mbon(self, W0, n_batch, e_tup):\n",
    "        \"\"\" Initializes the KC->MBON weights.\n",
    "        \n",
    "        KC->MBON weights are static for this task.\n",
    "\n",
    "        Parameters\n",
    "            W0 = specified initial weight values or None\n",
    "            n_batch = number of trials in mini-batch\n",
    "            e_tup: tuple = (current epoch, total training epochs)\n",
    "\n",
    "        Returns\n",
    "            W_KC_MBON: list = static KC->MBON weight matrix\n",
    "            wt = dynamic plasticity (zeros since there is no plasticity)\n",
    "        \"\"\"\n",
    "\n",
    "#         wt = torch.zeros(n_batch, self.N_MBON, self.N_KC)\n",
    "#         W_KC_MBON = self.W_KC_MBON_0.repeat(n_batch, 1, 1)\n",
    "        wt0 = self.W_KC_MBON_0.repeat(n_batch, 1, 1)\n",
    "\n",
    "#         return W_KC_MBON, wt\n",
    "        return wt0.clone(), wt0.clone()\n",
    "\n",
    "    def gen_stim_times(self, T_stim, T_int, dt, n_epoch, n_batch, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Present the stimuli between 5-15s and 20-30s of the interval\n",
    "        stim_min1 = 5\n",
    "        stim_max1 = 15 - T_stim\n",
    "        stim_range1 = int((stim_max1 - stim_min1) / dt)\n",
    "        stim_offset1 = int(stim_min1 / dt)\n",
    "        stim_min2 = 20\n",
    "        stim_max2 = 30 - T_stim\n",
    "        stim_range2 = int((stim_max2 - stim_min2) / dt)\n",
    "        stim_offset2 = int(stim_min2 / dt)\n",
    "        \n",
    "        # Initialize stimulus presentation times array\n",
    "        stim_times = torch.zeros(n_epoch, n_batch, 2)\n",
    "\n",
    "        for i in range(n_batch):\n",
    "            # Randomly determine the time of each stimulus presentation\n",
    "            stim_times[:, i, 0] = torch.multinomial(torch.ones(stim_range1), n_epoch, replacement=True) + stim_offset1\n",
    "            stim_times[:, i, 1] = torch.multinomial(torch.ones(stim_range2), n_epoch, replacement=True) + stim_offset2\n",
    "\n",
    "        return stim_times\n",
    "\n",
    "    def gen_inputs(self, stim_times, stim_len, time_len, n_batch, **kwargs):\n",
    "        \"\"\" Generates inputs for first-order conditioning tasks.\n",
    "\n",
    "        All trials are either CS+, CS- (US omitted) or CS omitted (control trials to avoid over-fitting).\n",
    "        Of the trials where CS or US is omitted, a second parameter determines the relative fractions of\n",
    "        CS or US trials omitted (p_omit_CS). See Figure 2 of Jiang 2020 to determine sequencing of stimuli\n",
    "        during training. To account for the sequential nature of numerical simulations, the target valence\n",
    "        is set to begin one time step after stimulus onset.\n",
    "\n",
    "        All trials are CS+ or control trials where CS+ is switched out for a neutral CS in the second\n",
    "        presentation. In the case where the CS is switched, the target valence is zero. To account for the\n",
    "        sequential nature of numerical simulations, the target valence is set to begin one time step after\n",
    "        stimulus onset.\n",
    "\n",
    "        The mix of conditions is listed as follows:\n",
    "            probability of trials where CS+ is switched = 0.5\n",
    "\n",
    "        Parameters\n",
    "            stim_times = randomly selected indices of stimulus presentations for each interval\n",
    "            stim_len = length of stimulus presentation (in indices)\n",
    "            time_len = size of time vector\n",
    "            n_batch = number of trials in mini-batch\n",
    "            p_omit = probability of omitting either CS or US from trials\n",
    "\n",
    "        Returns\n",
    "            r_KCt = odor (KC) input time series array for trial\n",
    "            r_extt = context (ext) input time series array for trial\n",
    "            vt_opt = time series of target valence for plotting and loss calculations\n",
    "            ls_stims = list of stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Set stimulus presentation time dtype\n",
    "        stim_times = stim_times.int()\n",
    "\n",
    "        # Define a set of 10 odors (CS)\n",
    "        n_ones = int(self.N_KC * 0.1)\n",
    "        if self.train_odors is None:\n",
    "            odor_list = torch.zeros(self.n_odors, self.N_KC)\n",
    "#             gen = torch.Generator()\n",
    "#             gen = gen.manual_seed(self.n_seed)\n",
    "#             odor_inds = torch.multinomial(torch.ones(n_odors, self.N_KC), n_ones, generator=gen)\n",
    "            odor_inds = torch.multinomial(torch.ones(self.n_odors, self.N_KC), n_ones)\n",
    "#             neut_inds = torch.multinomial(torch.ones(self.N_KC), n_ones, generator=gen)\n",
    "#             neut_odor = torch.zeros(self.N_KC)\n",
    "#             neut_odor[neut_inds] = 1\n",
    "            for n in range(self.n_odors):\n",
    "                # Define an odor (CS)\n",
    "                odor_list[n, odor_inds[n, :]] = 1\n",
    "            self.train_odors = odor_list\n",
    "\n",
    "        # Conditioned stimuli (CS) = odors\n",
    "        odor_select = torch.multinomial(torch.ones(self.n_odors), n_batch, replacement=True)\n",
    "        r_KC = torch.zeros(n_batch, self.N_KC)\n",
    "        for b in range(n_batch):\n",
    "            # Define an odor (CS) for each trial\n",
    "            r_KC[b, :] = self.train_odors[odor_select[b], :]\n",
    "        # Unconditioned stimuli (US) = context\n",
    "        r_ext = torch.multinomial(torch.ones(n_batch, self.N_ext), self.N_ext)\n",
    "\n",
    "        # Determine whether CS2+ is switched (switch on half of trials)\n",
    "        switch_inds = torch.rand(n_batch) < 0.5\n",
    "\n",
    "        # Initialize activity matrices\n",
    "        r_KCt = torch.zeros(n_batch, self.N_KC, time_len)\n",
    "        r_extt = torch.zeros(n_batch, self.N_ext, time_len)\n",
    "        time_CS_both = torch.zeros(n_batch, time_len)\n",
    "        time_US = torch.zeros_like(time_CS_both)\n",
    "        vt_opt = torch.zeros_like(time_CS_both)\n",
    "\n",
    "        # For each stimulus presentation\n",
    "        for i in range(2):\n",
    "            # Initialize time matrices\n",
    "            time_CS = torch.zeros(n_batch, time_len)\n",
    "\n",
    "            for b in range(n_batch):\n",
    "                stim_inds = stim_times[b, i] + torch.arange(stim_len)\n",
    "                # Set the CS time\n",
    "                time_CS[b, stim_inds] = 1\n",
    "                # Set the US time\n",
    "                if i == 0:\n",
    "                    time_US[b, stim_inds + stim_len] = 1\n",
    "                # Set the CS+/CS2 and target valence times\n",
    "                if i == 1:\n",
    "                    # In half the trials, switch the odor (target valence is zero)\n",
    "                    if switch_inds[b]:\n",
    "                        CS2_inds = torch.multinomial(torch.ones(self.N_KC), n_ones)\n",
    "                        r_KC[b, CS2_inds] = 1\n",
    "                    # If the odor is not switched, set the target valence\n",
    "                    else:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            vt_opt[b, (stim_inds + 1)] = 1\n",
    "                        else:\n",
    "                            vt_opt[b, (stim_inds + 1)] = -1\n",
    "\n",
    "            # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "            r_KCt += torch.einsum('bm, mbt -> bmt', r_KC, time_CS.repeat(self.N_KC, 1, 1))\n",
    "            r_extt += torch.einsum('bm, mbt -> bmt', r_ext, time_US.repeat(self.N_ext, 1, 1))\n",
    "            time_CS_both += time_CS\n",
    "\n",
    "        # Make a list of stimulus times to plot\n",
    "        ls_stims = [time_CS_both, time_US]\n",
    "\n",
    "        return [r_KCt], [r_extt], vt_opt, ls_stims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPlasticityRNN(DrosophilaRNN):\n",
    "    def __init__(self, *, n_odors=10):\n",
    "        super().__init__()\n",
    "#         # Set constants\n",
    "#         W_KC_MBON_max = 0.05\n",
    "#         self.W_KC_MBON = Variable(torch.rand(self.N_MBON, self.N_KC) * W_KC_MBON_max, requires_grad=False)\n",
    "        # Set the number of odors to train on\n",
    "        self.n_odors = n_odors\n",
    "\n",
    "test_RNN = NoPlasticityRNN(n_odors=10)\n",
    "# print(test_RNN.W_KC_MBON.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function for conditioning tasks\n",
    "def cond_loss(vt, vt_opt, r_DAN, lam=0.1):\n",
    "    \"\"\" Calculates the loss for conditioning tasks.\n",
    "    Composed of an MSE cost based on the difference between output and\n",
    "    target valence, and a regularization cost that penalizes excess\n",
    "    dopaminergic activity. Reference Eq. (3) and (9) in Jiang 2020.\n",
    "    \n",
    "    Parameters\n",
    "        vt = time dependent valence output of network\n",
    "        vt_opt = target valence (must be a torch tensor)\n",
    "        r_DAN = time series of dopaminergic neuron activities\n",
    "        lam = regularization constant\n",
    "    \n",
    "    Returns\n",
    "        loss_tot = scalar loss used in backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the baseline DAN activity\n",
    "    DAN_baseline = 0.1\n",
    "    \n",
    "    # Calculate the MSE loss of the valence\n",
    "    v_sum = torch.mean((vt - vt_opt)**2, dim=1)\n",
    "    v_loss = torch.mean(v_sum)\n",
    "    \n",
    "    # Calculate regularization term\n",
    "    r_sum = torch.sum(F.relu(r_DAN - 0.1)**2, dim=1)\n",
    "    r_loss = lam * torch.mean(r_sum, dim=1)\n",
    "    \n",
    "    # Calculate the summed loss (size = n_batch)\n",
    "    loss = v_loss + r_loss\n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    loss_tot = torch.mean(loss)\n",
    "    \n",
    "    return loss_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a training function for first-order conditioning\n",
    "# def train_net(network, T_int=40, T_stim=2, dt=0.5, n_epochs=2000, n_batch=30, n_odors=10, clip=0.0001, train=True, plot=None):\n",
    "#     \"\"\" Trains a network on classical conditioning tasks.\n",
    "    \n",
    "#     Tasks include first-order or second-order conditioning, or extinction. Tasks consist of\n",
    "#     a single intervals, and since there is no plasticity, activites are not reset between\n",
    "#     consecutive stimulus presentations (network relies on persistent activity to represent\n",
    "#     associations). Each task has its own input generating function. Stimuli are presented\n",
    "#     between 5-15s and 20-30s respectively.\n",
    "    \n",
    "#     Parameters\n",
    "#         network = RNN network to be trained or ran\n",
    "#         task = type of conditioning task to be trained ('first-order', 'all_tasks')\n",
    "#         T_int = length of task intervals (eg conditioning, test, extinction)\n",
    "#         T_stim = length of time each stimulus is presented\n",
    "#         dt = time step of simulations\n",
    "#         n_epochs = number of epochs to train over\n",
    "#         n_batch = number of trials in mini-batch\n",
    "#         p_ctrl = the fraction of trials that are control (to prevent over-fitting)\n",
    "#         clip = maximum gradient allowed during training\n",
    "#         train = boolean indicating whether to perform backprop\n",
    "#         plot = type of task to run (for plotting purposes)\n",
    "        \n",
    "#     Returns\n",
    "#         r_out_epoch = output circuit neuron activities for final epoch\n",
    "#         vt_epoch = readout (i.e. valence) for final epoch\n",
    "#         vt_opt = target valence for final epoch\n",
    "#         loss_hist = list of losses for all epochs\n",
    "#         ls_stims = list of stimulus time series\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Present the stimulus between 5-15s and 20-30s of the interval\n",
    "#     stim_min1 = 5\n",
    "#     stim_max1 = 15 - T_stim\n",
    "#     stim_range1 = int((stim_max1 - stim_min1) / dt)\n",
    "#     stim_offset1 = int(stim_min1 / dt)\n",
    "#     stim_min2 = 20\n",
    "#     stim_max2 = 30 - T_stim\n",
    "#     stim_range2 = int((stim_max2 - stim_min2) / dt)\n",
    "#     stim_offset2 = int(stim_min2 / dt)\n",
    "#     # Length of stimulus in indices\n",
    "#     stim_len = int(T_stim / dt)\n",
    "#     # Interval time vector\n",
    "#     time_int = torch.arange(0, T_int + dt/10, dt)\n",
    "\n",
    "#     # Neuron population sizes\n",
    "#     n_KC = network.N_KC\n",
    "#     n_ext = network.N_ext\n",
    "#     n_MBON = network.N_MBON\n",
    "#     # Max KC->MBON weight values\n",
    "#     W_KC_MBON_max = network.KC_MBON_max\n",
    "    \n",
    "#     # Define a set of 10 odors (CS)\n",
    "#     n_ones = int(n_KC * 0.1)\n",
    "#     odor_list = torch.zeros(n_odors, n_KC)\n",
    "#     gen = torch.Generator()\n",
    "#     gen = gen.manual_seed(network.n_seed)\n",
    "#     odor_inds = torch.multinomial(torch.ones(n_odors, n_KC), n_ones, generator=gen)\n",
    "#     neut_inds = torch.multinomial(torch.ones(n_KC), n_ones, generator=gen)\n",
    "#     neut_odor = torch.zeros(n_KC)\n",
    "#     neut_odor[neut_inds] = 1\n",
    "#     for n in range(n_odors):\n",
    "#         # Define an odor (CS)\n",
    "#         odor_list[n, odor_inds[n, :]] = 1\n",
    "\n",
    "#     # List to store losses\n",
    "#     loss_hist = []\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         # Lists to store activities, weights, readouts and target valences\n",
    "#         r_outs = []\n",
    "#         vts = []\n",
    "# #         vals = []\n",
    "\n",
    "#         # Randomly determine the time of each stimulus presentation\n",
    "#         stim_times1 = torch.multinomial(torch.ones(stim_range1), n_batch, replacement=True) + stim_offset1\n",
    "#         stim_times2 = torch.multinomial(torch.ones(stim_range2), n_batch, replacement=True) + stim_offset2\n",
    "#         stim_times = torch.stack((stim_times1.view(-1, 1), stim_times2.view(-1, 1)), dim=1)\n",
    "        \n",
    "#         # Generate the odor (r_KC) and context (r_ext) inputs, and target valence (vt_opt)\n",
    "#         r_KC, r_ext, vt_opt, ls_stims = first_order_inputs(stim_times, stim_len, time_int.size()[0], odor_list, n_KC, n_ext, n_batch, plot)\n",
    "\n",
    "#         # Run the forward model\n",
    "#         r_int, vt = network(r_KC, r_ext, time_int, n_batch)\n",
    "\n",
    "#         # Append the interval outputs to lists\n",
    "#         r_outs += r_int\n",
    "#         vts += vt\n",
    "\n",
    "#         # Concatenate the activities, weights and valences\n",
    "#         r_out_epoch = torch.stack(r_outs, dim=-1)\n",
    "#         vt_epoch = torch.stack(vts, dim=-1)\n",
    "\n",
    "#         # Calculate the loss\n",
    "#         loss = cond_loss(vt_epoch, vt_opt, r_out_epoch[:, -network.N_DAN:, :])\n",
    "\n",
    "#         if train:\n",
    "#             # Update the network parameters\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(network.parameters(), clip)\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # Print an update\n",
    "#         if epoch % 500 == 0:\n",
    "#             print(epoch, loss.item())\n",
    "#         loss_hist.append(loss.item())\n",
    "\n",
    "#     return r_out_epoch, vt_epoch, vt_opt, loss_hist, ls_stims\n",
    "\n",
    "# # https://discuss.pytorch.org/t/proper-way-to-do-gradient-clipping/191/13\n",
    "# # https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48\n",
    "# # https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define functions for each of the conditioning tasks\n",
    "# # Some detail provided in Jiang 2020 -> Methods -> Networks without dopamine-gated plasticity\n",
    "# def first_order_inputs(stim_times, stim_len, time_len, odor_list, n_KC, n_ext, n_batch, plot=None, p_omit=0.3):\n",
    "#     \"\"\" Generates inputs for first-order conditioning tasks.\n",
    "    \n",
    "#     All trials are either CS+, CS- (US omitted) or CS omitted (control trials to avoid over-fitting).\n",
    "#     Of the trials where CS or US is omitted, a second parameter determines the relative fractions of\n",
    "#     CS or US trials omitted (p_omit_CS). See Figure 2 of Jiang 2020 to determine sequencing of stimuli\n",
    "#     during training. To account for the sequential nature of numerical simulations, the target valence\n",
    "#     is set to begin one time step after stimulus onset.\n",
    "    \n",
    "#     All trials are CS+ or control trials where CS+ is switched out for a neutral CS in the second\n",
    "#     presentation. In the case where the CS is switched, the target valence is zero. To account for the\n",
    "#     sequential nature of numerical simulations, the target valence is set to begin one time step after\n",
    "#     stimulus onset.\n",
    "    \n",
    "#     The mix of conditions is listed as follows:\n",
    "#         probability of trials where CS+ is switched = 0.5\n",
    "    \n",
    "#     Parameters\n",
    "#         stim_times = randomly selected indices of stimulus presentations for each interval\n",
    "#         stim_len = length of stimulus presentation (in indices)\n",
    "#         time_len = size of time vector\n",
    "#         odor_list = list of 10 odors used for all training\n",
    "#         n_KC = number of Kenyon cell input neurons\n",
    "#         n_ext = number of contextual input neurons\n",
    "#         n_batch = number of trials in mini-batch\n",
    "#         plot = used when plot function is called, indicates which task to plot\n",
    "#         p_omit = probability of omitting either CS or US from trials\n",
    "        \n",
    "#     Returns\n",
    "#         r_KCt = odor (KC) input time series array for trial\n",
    "#         r_extt = context (ext) input time series array for trial\n",
    "#         vt_opt = time series of target valence for plotting and loss calculations\n",
    "#         ls_stims = list of stimulus time series for plotting\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Conditioned stimuli (CS) = odors\n",
    "#     odor_inds = torch.multinomial(torch.ones(odor_list.shape[0]), n_batch, replacement=True)\n",
    "#     r_KC = torch.zeros(n_batch, n_KC)\n",
    "#     for b in range(n_batch):\n",
    "#         # Define an odor (CS) for each trial\n",
    "#         r_KC[b, :] = odor_list[odor_inds[b], :]\n",
    "#     # Unconditioned stimuli (US) = context\n",
    "#     r_ext = torch.multinomial(torch.ones(n_batch, n_ext), n_ext)\n",
    "    \n",
    "#     # Determine whether CS2+ is switched (switch on half of trials)\n",
    "#     switch_inds = torch.rand(n_batch) < 0.5\n",
    "\n",
    "#     # Initialize activity matrices\n",
    "#     r_KCt = torch.zeros(n_batch, n_KC, time_len)\n",
    "#     r_extt = torch.zeros(n_batch, n_ext, time_len)\n",
    "#     time_CS_both = torch.zeros(n_batch, time_len)\n",
    "#     time_US = torch.zeros_like(time_CS_both)\n",
    "#     vt_opt = torch.zeros_like(time_CS_both)\n",
    "    \n",
    "#     # For each stimulus presentation\n",
    "#     n_ints = 2\n",
    "#     for i in range(n_ints):\n",
    "#         # Initialize time matrices\n",
    "#         time_CS = torch.zeros(n_batch, time_len)\n",
    "        \n",
    "#         for b in range(n_batch):\n",
    "#             stim_inds = stim_times[b, i] + torch.arange(stim_len)\n",
    "#             # Set the CS time\n",
    "#             time_CS[b, stim_inds] = 1\n",
    "#             # Set the US time\n",
    "#             if i == 0:\n",
    "#                 time_US[b, stim_inds + stim_len] = 1\n",
    "#             # Set the CS+/CS2 and target valence times\n",
    "#             if i == 1:\n",
    "#                 # In half the trials, switch the odor (target valence is zero)\n",
    "#                 if (plot is None and switch_inds[b]) or plot == 'CS2':\n",
    "#                     CS2_inds = torch.multinomial(torch.ones(n_KC), int(n_KC * 0.1))\n",
    "#                     r_KC[b, CS2_inds] = 1\n",
    "#                 # If the odor is not switched, set the target valence\n",
    "#                 else:\n",
    "#                     if r_ext[b, 0] == 1:\n",
    "#                         vt_opt[b, (stim_inds + 1)] = 1\n",
    "#                     else:\n",
    "#                         vt_opt[b, (stim_inds + 1)] = -1\n",
    "                        \n",
    "#         # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "#         r_KCt += torch.einsum('bm, mbt -> bmt', r_KC, time_CS.repeat(n_KC, 1, 1))\n",
    "#         r_extt += torch.einsum('bm, mbt -> bmt', r_ext, time_US.repeat(n_ext, 1, 1))\n",
    "#         time_CS_both += time_CS\n",
    "\n",
    "#     # Make a list of stimulus times to plot\n",
    "#     ls_stims = [time_CS_both, time_US]\n",
    "\n",
    "#     return r_KCt, r_extt, vt_opt, ls_stims\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_trial(network, task1:str, task2:str, dt=0.5):\n",
    "def print_trial(network, task, n_odors=10, dt=0.5):\n",
    "    \"\"\" Plots a figure similar to Figure 2 from Jiang 2020.\n",
    "    \n",
    "    Runs the network using a novel combination of conditioned and unconditioned stimuli,\n",
    "    then prints the results. Top: time series of the various stimuli (CS and US), as well as\n",
    "    the target valence and readout. Bottom: activity of eight randomly chosen mushroom body\n",
    "    output neurons (MBONs).\n",
    "    \n",
    "    Paramters\n",
    "        network = previously trained RNN\n",
    "        task = the type of task to be plotted ('CS+' or 'CS2')\n",
    "        dt = time step of the simulation/plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the labels for the plots\n",
    "    if task == 'CS+':\n",
    "        task_title = 'First-Order Conditioning'\n",
    "    elif task == 'CS2':\n",
    "        task_title = 'CS+ Generalization'\n",
    "    \n",
    "    # Define plot font sizes\n",
    "    label_font = 18\n",
    "    title_font = 24\n",
    "    legend_font = 12\n",
    "\n",
    "    # Run the network\n",
    "    r_out, vt, vt_opt, loss_hist, stim_ls = train_net(network, dt=dt, n_epochs=1, n_batch=1, n_odors=n_odors, train=False, plot=task)\n",
    "    r_out = r_out.detach().numpy().squeeze()\n",
    "    vt = vt.detach().numpy().squeeze()\n",
    "    vt_opt = vt_opt.detach().numpy().squeeze()\n",
    "    plot_CS = stim_ls[0].numpy().squeeze()\n",
    "    plot_US = stim_ls[1].numpy().squeeze()\n",
    "    plot_time = np.arange(plot_CS.size) * dt\n",
    "\n",
    "    # Plot the conditioning and test\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True, gridspec_kw={'height_ratios': [1, 4]})\n",
    "    ax1.plot(plot_time, vt, label='Readout')\n",
    "    ax1.plot(plot_time, vt_opt, label='Target')\n",
    "    # Second-order conditioning involves an additional stimulus time series\n",
    "    if task == 'CS+':\n",
    "        ax1.plot(plot_time, plot_CS, label=task)\n",
    "    else:\n",
    "        half_ind = plot_time.size // 2\n",
    "        plot_CSp = np.zeros(plot_time.size)\n",
    "        plot_CSp[:half_ind] = plot_CS[:half_ind]\n",
    "        plot_CS2 = np.zeros(plot_time.size)\n",
    "        plot_CS2[half_ind:] = plot_CS[half_ind:]\n",
    "        ax1.plot(plot_time, plot_CSp, label='CS+')\n",
    "        ax1.plot(plot_time, plot_CS2, label=task)\n",
    "    ax1.plot(plot_time, plot_US, label='US')\n",
    "    ax1.set_ylabel('Value', fontsize=label_font)\n",
    "    ax1.set_title(task_title, fontsize=title_font)\n",
    "    ax1.legend(fontsize=legend_font)\n",
    "\n",
    "    # Plot the activities of a few MBONs\n",
    "    plot_neurs = np.random.choice(network.N_MBON, size=8, replace=False)\n",
    "    r_max = np.max(r_out)\n",
    "    for i, n in enumerate(plot_neurs):\n",
    "        ax2.plot(plot_time, (r_out[n, :] / r_max) + (i * 2 / 3), '-k')\n",
    "    ax2.set_xlabel('Time', fontsize=label_font)\n",
    "    ax2.set_ylabel('Normalized Activity', fontsize=label_font)\n",
    "    ax2.set_yticks([])\n",
    "    fig.tight_layout();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([60, 2])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "# classic_net = DrosophilaRNN()\n",
    "classic_net = NoPlasticityRNN(n_odors=10)\n",
    "# classic_net = NoPlasticityRNN(n_odors=1)\n",
    "for param in classic_net.parameters():\n",
    "    print(param.shape)\n",
    "#     print(param)\n",
    "# print(classic_net.N_DAN)\n",
    "\n",
    "# Define the model's optimizer\n",
    "lr = 0.001\n",
    "optimizer = optim.RMSprop(classic_net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.071109771728516\n",
      "500 0.006819127593189478\n",
      "1000 0.0037600998766720295\n",
      "1500 0.002835210645571351\n"
     ]
    }
   ],
   "source": [
    "train_bool = True\n",
    "if train_bool:\n",
    "#     r_out, vt, vt_opt, loss_hist, _ = train_net(classic_net, n_epochs=2000, n_odors=10)\n",
    "#     r_out, vt, vt_opt, loss_hist, _ = train_net(classic_net, n_epochs=2000, n_odors=1)\n",
    "#     r_out, vt, vt_opt, loss_hist, _ = classic_net.train_net(opti=optimizer, n_epoch=2000)\n",
    "    r_out, vt, vt_opt, loss_hist, _ = classic_net.train_net(opti=optimizer, n_epoch=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_bool:\n",
    "    # Plot the loss function\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "    axes[0].plot(loss_hist)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=label_font)\n",
    "    axes[0].set_ylabel('Loss', fontsize=label_font)\n",
    "    axes[1].plot(loss_hist[5:])\n",
    "    axes[1].set_xlabel('Epoch', fontsize=label_font)\n",
    "    axes[1].set_ylabel('Loss', fontsize=label_font)\n",
    "    fig.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(classic_net.state_dict(), 'trained_N5_no_plasticity_10odors_2000epochs.pt')\n",
    "# torch.save(classic_net.state_dict(), 'trained_N6_no_plasticity_1odors_2000epochs.pt')\n",
    "\n",
    "# https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some results to compare to Fig 2A/B in paper\n",
    "if not train_bool:\n",
    "#     classic_net.load_state_dict(torch.load('trained_N5_no_plasticity_10odor_2000epochs.pt'))\n",
    "#     classic_net.load_state_dict(torch.load('trained_N6_no_plasticity_1odor_2000epochs.pt'))\n",
    "    classic_net.eval()\n",
    "\n",
    "# print_trial(classic_net, task='CS+')\n",
    "# print_trial(classic_net, task='CS2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.002018461236730218\n",
      "0.0020561665325658394 0.00031837440985570044\n"
     ]
    }
   ],
   "source": [
    "# r_out, Wt, vt, vt_opt, loss_hist, _ = train_net(classic_net, task=train_task, n_epochs=200, train=False)\n",
    "r_out, vt, vt_opt, loss_hist, _ = classic_net.train_net(opti=optimizer, n_epoch=200)\n",
    "print(np.mean(loss_hist), np.std(loss_hist))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
