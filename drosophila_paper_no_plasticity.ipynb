{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "# from numpy.linalg import svd as svd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define plot font sizes\n",
    "label_font = 18\n",
    "title_font = 24\n",
    "legend_font = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model Class - Conditioning Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drosophila_RNN(nn.Module):\n",
    "    def __init__(self, KC_size=200, MBON_size=20, DAN_size=20, FBN_size=60, ext_size=2, out_size=1, net_seed=1234):\n",
    "        super(Drosophila_RNN, self).__init__()\n",
    "        # Set the seeds\n",
    "#         np.random.seed(net_seed)\n",
    "#         torch.manual_seed(net_seed)\n",
    "        # Set constants\n",
    "        self.KC_MBON_min = 0. # Minimum synaptic weight\n",
    "        self.KC_MBON_max = 0.05 # Maximum synaptic weight\n",
    "        self.tau_w = 5 # Time scale of KC->MBON LTD/LTP (plasticity)\n",
    "        self.tau_r = 1 # Time scale of output circuitry activity\n",
    "        # Set the sizes of layers\n",
    "        self.N_KC = KC_size\n",
    "        self.N_MBON = MBON_size\n",
    "        self.N_FBN = FBN_size\n",
    "        self.N_DAN = DAN_size\n",
    "        self.N_recur = MBON_size + FBN_size + DAN_size\n",
    "        self.N_ext = ext_size\n",
    "        self.N_out = out_size\n",
    "        # Define updatable network parameters\n",
    "#         seed_num = net_seed\n",
    "        seed_num = None\n",
    "        sqrt2 = torch.sqrt(torch.tensor(2, dtype=torch.float))\n",
    "        mean_MBON = torch.zeros((self.N_recur, MBON_size))\n",
    "        mean_FBN = torch.zeros((self.N_recur, FBN_size))\n",
    "        mean_DAN = torch.zeros((self.N_recur, DAN_size))\n",
    "        W_MBON = torch.normal(mean_MBON, torch.sqrt(1 / (sqrt2 * MBON_size)), generator=seed_num)\n",
    "        W_FBN = torch.normal(mean_FBN, torch.sqrt(1 / (sqrt2 * FBN_size)), generator=seed_num)\n",
    "        W_DAN = torch.normal(mean_DAN, torch.sqrt(1 / (sqrt2 * DAN_size)), generator=seed_num)\n",
    "        self.W_recur = nn.Parameter(torch.cat((W_MBON, W_FBN, W_DAN), dim=1), requires_grad=True)\n",
    "        self.W_ext = nn.Parameter(torch.randn(FBN_size, ext_size), requires_grad=True)\n",
    "        mean_readout = torch.zeros((out_size, MBON_size))\n",
    "        std_readout = 1 / torch.sqrt(torch.tensor(MBON_size, dtype=torch.float))\n",
    "        self.W_readout = nn.Parameter(torch.normal(mean_readout, std_readout, generator=seed_num), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones(self.N_recur) * 0.1, requires_grad=True)\n",
    "        \n",
    "    def forward(self, r_KC, r_ext, time, W0=None, batch_size=30):\n",
    "        \"\"\" Defines the forward pass of the RNN\n",
    "        \n",
    "        Synaptic weights from the Keyon cells to the mushroom body output neurons (MBONs)\n",
    "        are fixed for each trial. All other weights are network parameters.\n",
    "        \n",
    "        The KC->MBON weights are set within the range [0, 0.05].\n",
    "        MBONs receive external input from Keyon cells (r_KC i.e. 'odors').\n",
    "        Feedback neurons (FBNs) receive external contextual input (r_ext i.e. 'context').\n",
    "        DAN->MBON weights are permanently set to zero. DANs receive no external input.\n",
    "\n",
    "        Inputs\n",
    "            r_KC = activity of the Kenyon cell neurons (representing odors)\n",
    "            r_ext = context signals (representing the conditioning context)\n",
    "            time = time vector for a single interval\n",
    "            W0 = initial weights for KC->MBON connections\n",
    "            batch_size = number of trials in batch\n",
    "\n",
    "        Returns\n",
    "            r_recur: list of torch.ndarray(batch_size, N_MBON + N_FBN + N_DAN)\n",
    "                = time series of activities in the output circuitry\n",
    "            readout: list of torch.ndarray(batch_size, 1)\n",
    "                = time series of valence readouts (represents behaviour)\n",
    "        \"\"\"\n",
    "\n",
    "        # Define the time step of the simulation\n",
    "        dt = np.diff(time)[0]\n",
    "\n",
    "        # Initialize output circuit firing rates for each trial\n",
    "        r_init = torch.ones((batch_size, self.N_recur)) * 0.1\n",
    "        r_init[:, :self.N_MBON] = 0\n",
    "        r_recur = []\n",
    "        r_recur.append(r_init)\n",
    "\n",
    "        # Initialize KC->MBON weights\n",
    "        W_KC_MBON = W0\n",
    "        readout = []\n",
    "        readout.append(torch.squeeze(torch.einsum('bom, bm -> bo', self.W_readout.repeat(batch_size, 1, 1), r_recur[-1][:, :self.N_MBON])))\n",
    "\n",
    "        # Set the weights DAN->MBON to zero\n",
    "        W_recur = self.W_recur.clone()\n",
    "        W_recur[:self.N_MBON, -self.N_DAN:] = 0\n",
    "\n",
    "        # Update activity for each time step\n",
    "        for t in range(time.size()[0] - 1):\n",
    "            # Define the input to the output circuitry\n",
    "            I_KC_MBON = torch.einsum('bmk, bk -> bm', W_KC_MBON, r_KC[:, :, t])\n",
    "            I_FBN = torch.einsum('bfe, be -> bf', self.W_ext.repeat(batch_size, 1, 1), r_ext[:, :, t])\n",
    "            I = torch.zeros((batch_size, self.N_recur))\n",
    "            I[:, :self.N_MBON] = I_KC_MBON\n",
    "            I[:, self.N_MBON:self.N_MBON + self.N_FBN] = I_FBN\n",
    "\n",
    "            # Update the output circuitry activity\n",
    "            Wr_prod = torch.einsum('bsr, br -> bs', W_recur.repeat(batch_size, 1, 1), r_recur[-1])\n",
    "            dr = (-r_recur[-1] + F.relu(Wr_prod + self.bias.repeat(batch_size, 1) + I)) / self.tau_r\n",
    "            r_recur.append(r_recur[-1] + dr * dt)\n",
    "\n",
    "            # Calculate the readout\n",
    "            readout.append(torch.squeeze(torch.einsum('bom, bm -> bo', self.W_readout.repeat(batch_size, 1, 1), r_recur[-1][:, :self.N_MBON])))\n",
    "\n",
    "        return r_recur, readout\n",
    "            \n",
    "        \n",
    "# Clipping weights between [0, 0.05]\n",
    "# https://discuss.pytorch.org/t/how-to-do-constrained-optimization-in-pytorch/60122\n",
    "# https://discuss.pytorch.org/t/set-constraints-on-parameters-or-layers/23620\n",
    "# https://discuss.pytorch.org/t/restrict-range-of-variable-during-gradient-descent/1933/4\n",
    "\n",
    "# Setting DAN->MBON weights to zero\n",
    "# https://pytorch.org/docs/stable/generated/torch.triu.html\n",
    "\n",
    "# Broadcasting using einsum\n",
    "# https://github.com/pytorch/pytorch/issues/15671\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function for conditioning tasks\n",
    "def cond_loss(vt, vt_opt, r_DAN, lam=0.1):\n",
    "    \"\"\" Calculates the loss for conditioning tasks.\n",
    "    Composed of an MSE cost based on the difference between output and\n",
    "    target valence, and a regularization cost that penalizes excess\n",
    "    dopaminergic activity. Reference Eq. (3) and (9) in Jiang 2020.\n",
    "    \n",
    "    Parameters\n",
    "        vt = time dependent valence output of network\n",
    "        vt_opt = target valence (must be a torch tensor)\n",
    "        r_DAN = time series of dopaminergic neuron activities\n",
    "        lam = regularization constant\n",
    "    \n",
    "    Returns\n",
    "        loss_tot = scalar loss used in backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the baseline DAN activity\n",
    "    DAN_baseline = 0.1\n",
    "    \n",
    "    # Calculate the MSE loss of the valence\n",
    "    v_sum = torch.mean((vt - vt_opt)**2, dim=1)\n",
    "    v_loss = torch.mean(v_sum)\n",
    "    \n",
    "    # Calculate regularization term\n",
    "    r_sum = torch.sum(F.relu(r_DAN - 0.1)**2, dim=1)\n",
    "    r_loss = lam * torch.mean(r_sum, dim=1)\n",
    "    \n",
    "    # Calculate the summed loss (size = n_batch)\n",
    "    loss = v_loss + r_loss\n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    loss_tot = torch.mean(loss)\n",
    "    \n",
    "    return loss_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a training function for first-order conditioning\n",
    "# def train_net(network, task:str, T_int=40, T_stim=2, dt=0.5, n_epochs=2000, n_batch=30, clip=0.005, train=True, plot=None):\n",
    "def train_net(network, T_int=40, T_stim=2, dt=0.5, n_epochs=2000, n_batch=30, clip=0.001, train=True, plot=None):\n",
    "    \"\"\" Trains a network on classical conditioning tasks.\n",
    "    \n",
    "    Tasks include first-order or second-order conditioning, or extinction. Tasks consist of\n",
    "    a single intervals, and since there is no plasticity, activites are not reset between\n",
    "    consecutive stimulus presentations (network relies on persistent activity to represent\n",
    "    associations). Each task has its own input generating function. Stimuli are presented\n",
    "    between 5-15s and 20-30s respectively.\n",
    "    \n",
    "    Parameters\n",
    "        network = RNN network to be trained or ran\n",
    "        task = type of conditioning task to be trained ('first-order', 'all_tasks')\n",
    "        T_int = length of task intervals (eg conditioning, test, extinction)\n",
    "        T_stim = length of time each stimulus is presented\n",
    "        dt = time step of simulations\n",
    "        n_epochs = number of epochs to train over\n",
    "        n_batch = number of trials in mini-batch\n",
    "        p_ctrl = the fraction of trials that are control (to prevent over-fitting)\n",
    "        clip = maximum gradient allowed during training\n",
    "        train = boolean indicating whether to perform backprop\n",
    "        plot = type of task to run (for plotting purposes)\n",
    "        \n",
    "    Returns\n",
    "        r_out_epoch = output circuit neuron activities for final epoch\n",
    "        vt_epoch = readout (i.e. valence) for final epoch\n",
    "        vt_opt = target valence for final epoch\n",
    "        loss_hist = list of losses for all epochs\n",
    "        ls_stims = list of stimulus time series\n",
    "    \"\"\"\n",
    "    \n",
    "#     # Set variables dependent on task\n",
    "#     if task == 'first-order':\n",
    "# #         n_ints = 2\n",
    "# #         n_epochs = 2000\n",
    "#         f_gen_inputs = first_order_inputs\n",
    "#     elif task == 'all_tasks':\n",
    "# #         n_ints = 3\n",
    "# #         n_epochs = 5000\n",
    "#         f_gen_inputs = all_task_inputs\n",
    "    \n",
    "    # Present the stimulus between 5-15s and 20-30s of the interval\n",
    "    stim_min1 = 5\n",
    "    stim_max1 = 15 - T_stim\n",
    "    stim_range1 = int((stim_max1 - stim_min1) / dt)\n",
    "    stim_offset1 = int(stim_min1 / dt)\n",
    "    stim_min2 = 20\n",
    "    stim_max2 = 30 - T_stim\n",
    "    stim_range2 = int((stim_max2 - stim_min2) / dt)\n",
    "    stim_offset2 = int(stim_min2 / dt)\n",
    "    # Length of stimulus in indices\n",
    "    stim_len = int(T_stim / dt)\n",
    "    # Interval time vector\n",
    "    time_int = torch.arange(0, T_int + dt/10, dt)\n",
    "\n",
    "    # Neuron population sizes\n",
    "    n_KC = network.N_KC\n",
    "    n_ext = network.N_ext\n",
    "    n_MBON = network.N_MBON\n",
    "    # Max KC->MBON weight values\n",
    "    W_KC_MBON_max = network.KC_MBON_max\n",
    "    \n",
    "    # Define a set of 10 odors (CS)\n",
    "    n_ones = int(n_KC * 0.1)\n",
    "    n_odors = 10\n",
    "    odor_list = torch.zeros(n_odors, n_KC)\n",
    "    for n in range(n_odors):\n",
    "        # Define an odor (CS)\n",
    "        odor_inds = torch.multinomial(torch.ones(n_KC), n_ones)\n",
    "        odor_list[n, odor_inds] = 1            \n",
    "\n",
    "    # List to store losses\n",
    "    loss_hist = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Lists to store activities, weights, readouts and target valences\n",
    "        r_outs = []\n",
    "        vts = []\n",
    "        vals = []\n",
    "\n",
    "        # Set KC->MBON weight values (static for duration of trial)\n",
    "        W_KC_MBON = torch.rand((n_batch, n_MBON, n_KC)) * W_KC_MBON_max\n",
    "\n",
    "        # Randomly determine the time of each stimulus presentation\n",
    "        stim_times1 = torch.multinomial(torch.ones(stim_range1), n_batch, replacement=True) + stim_offset1\n",
    "        stim_times2 = torch.multinomial(torch.ones(stim_range2), n_batch, replacement=True) + stim_offset2\n",
    "        stim_times = torch.stack((stim_times1.view(-1, 1), stim_times2.view(-1, 1)), dim=1)\n",
    "        print(stim_times.shape)\n",
    "        \n",
    "        # Generate the odor (r_KC) and context (r_ext) inputs, and target valence (vt_opt)\n",
    "#         r_KC, r_ext, vt_opt, ls_stims = f_gen_inputs(stim_times, stim_len, time_int.size()[0], odor_list, n_KC, n_ext, n_batch, plot)\n",
    "        r_KC, r_ext, vt_opt, ls_stims = first_order_inputs(stim_times, stim_len, time_int.size()[0], odor_list, n_KC, n_ext, n_batch, plot)\n",
    "\n",
    "        # Run the forward model\n",
    "        r_int, vt = network(r_KC, r_ext, time_int, W_KC_MBON, n_batch)\n",
    "\n",
    "        # Append the interval outputs to lists\n",
    "        r_outs += r_int\n",
    "        vts += vt\n",
    "\n",
    "        # Concatenate the activities, weights and valences\n",
    "        r_out_epoch = torch.stack(r_outs, dim=-1)\n",
    "        vt_epoch = torch.stack(vts, dim=-1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = cond_loss(vt_epoch, vt_opt, r_out_epoch[:, -classic_net.N_DAN:, :])\n",
    "\n",
    "        if train:\n",
    "            # Update the network parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(classic_net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print an update\n",
    "        if epoch % 500 == 0:\n",
    "            print(epoch, loss.item())\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "    return r_out_epoch, vt_epoch, vt_opt, loss_hist, ls_stims\n",
    "\n",
    "# https://discuss.pytorch.org/t/proper-way-to-do-gradient-clipping/191/13\n",
    "# https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48\n",
    "# https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for each of the conditioning tasks\n",
    "# Some detail provided in Jiang 2020 -> Methods -> Conditioning Tasks\n",
    "def first_order_inputs(stim_times, stim_len, time_len, odor_list, n_KC, n_ext, n_batch, plot=None, p_omit=0.3):\n",
    "    \"\"\" Generates inputs for first-order conditioning tasks.\n",
    "    \n",
    "    All trials are either CS+, CS- (US omitted) or CS omitted (control trials to avoid over-fitting).\n",
    "    Of the trials where CS or US is omitted, a second parameter determines the relative fractions of\n",
    "    CS or US trials omitted (p_omit_CS). See Figure 2 of Jiang 2020 to determine sequencing of stimuli\n",
    "    during training. To account for the sequential nature of numerical simulations, the target valence\n",
    "    is set to begin one time step after stimulus onset.\n",
    "    \n",
    "    All trials are CS+ or control trials where CS+ is switched out for a neutral CS in the second\n",
    "    presentation. In the case where the CS is switched, the target valence is zero. To account for the\n",
    "    sequential nature of numerical simulations, the target valence is set to begin one time step after\n",
    "    stimulus onset.\n",
    "    \n",
    "    The mix of conditions is listed as follows:\n",
    "        probability of trials where CS+ is switched = 0.5\n",
    "    \n",
    "    Parameters\n",
    "        stim_times = randomly selected indices of stimulus presentations for each interval\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "        time_len = size of time vector\n",
    "        odor_list = list of 10 odors used for all training\n",
    "        n_KC = number of Kenyon cell input neurons\n",
    "        n_ext = number of contextual input neurons\n",
    "        n_batch = number of trials in mini-batch\n",
    "        plot = used when plot function is called, indicates which task to plot\n",
    "        p_omit = probability of omitting either CS or US from trials\n",
    "        \n",
    "    Returns\n",
    "        r_KCt = odor (KC) input time series array for trial\n",
    "        r_extt = context (ext) input time series array for trial\n",
    "        vt_opt = time series of target valence for plotting and loss calculations\n",
    "        ls_stims = list of stimulus time series for plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Conditioned stimuli (CS) = odors\n",
    "    odor_inds = torch.multinomial(torch.ones(odor_list.shape[0]), n_batch, replacement=True)\n",
    "    r_KC = torch.zeros(n_batch, n_KC)\n",
    "    for b in range(n_batch):\n",
    "        # Define an odor (CS) for each trial\n",
    "        r_KC[b, :] = odor_list[odor_inds[b], :]\n",
    "    # Unconditioned stimuli (US) = context\n",
    "    r_ext = torch.multinomial(torch.ones(n_batch, n_ext), n_ext)\n",
    "    \n",
    "    # Determine whether CS2+ is switched (switch on half of trials)\n",
    "    switch_inds = torch.rand(n_batch) < 0.5\n",
    "\n",
    "    # Initialize activity matrices\n",
    "    r_KCt = torch.zeros(n_batch, n_KC, time_len)\n",
    "    r_extt = torch.zeros(n_batch, n_ext, time_len)\n",
    "    time_CS_both = torch.zeros(n_batch, time_len)\n",
    "    time_US = torch.zeros_like(time_CS_both)\n",
    "    vt_opt = torch.zeros_like(time_CS_both)\n",
    "    \n",
    "    # For each stimulus presentation\n",
    "    n_ints = 2\n",
    "    for i in range(n_ints):\n",
    "        # Initialize time matrices\n",
    "        time_CS = torch.zeros(n_batch, time_len)\n",
    "#         time_US = torch.zeros_like(time_CS)\n",
    "        \n",
    "#         val_count = 0\n",
    "        for b in range(n_batch):\n",
    "            stim_inds = stim_times[b, i] + torch.arange(stim_len)\n",
    "            # Set the CS time\n",
    "            time_CS[b, stim_inds] = 1\n",
    "            # Set the US time\n",
    "            if i == 0:\n",
    "#                 time_CS[b, stim_inds] = 1\n",
    "                time_US[b, stim_inds + stim_len] = 1\n",
    "            # Set the CS+/CS2 and target valence times\n",
    "            if i == 1:\n",
    "#                 time_CS[b, stim_inds] = 1\n",
    "                # In half the trials, switch the odor (target valence is zero)\n",
    "                if switch_inds[b] or plot == 'CS2':\n",
    "                    new_CS = torch.multinomial(torch.ones(odor_list.shape[0]), 1)\n",
    "                    r_KC[b, :] = odor_list[new_CS, :]\n",
    "#                     time_CS2 = time_CS\n",
    "                # If the odor is not switched, set the target valence\n",
    "                else:\n",
    "#                     val_count += 1\n",
    "                    if r_ext[b, 0] == 1:\n",
    "                        vt_opt[b, (stim_inds + 1)] = 1\n",
    "                    else:\n",
    "                        vt_opt[b, (stim_inds + 1)] = -1\n",
    "\n",
    "        # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "        r_KCt += torch.einsum('bm, mbt -> bmt', r_KC, time_CS.repeat(n_KC, 1, 1))\n",
    "        r_extt += torch.einsum('bm, mbt -> bmt', r_ext, time_US.repeat(n_ext, 1, 1))\n",
    "        time_CS_both += time_CS\n",
    "#         time_US_both += time_US\n",
    "#         print(val_count)\n",
    "\n",
    "    r_KCt_switch = r_KCt[torch.arange(0, n_batch)[switch_inds][0], :, :]\n",
    "    r_KCt_mask = (r_KCt_switch > 0).squeeze()\n",
    "    indices = torch.arange(0, n_KC * time_len).view(n_KC, -1)\n",
    "    neuron = indices[r_KCt_mask][0] // time_len\n",
    "    print('Switch neuron')\n",
    "#     print(r_KCt_switch[neuron, :])\n",
    "    print(r_KCt_switch[neuron, :] + vt_opt[torch.arange(0, n_batch)[switch_inds][0], :])\n",
    "    r_KCt_switch = r_KCt[torch.arange(0, n_batch)[~switch_inds][0], :, :]\n",
    "    r_KCt_mask = (r_KCt_switch > 0).squeeze()\n",
    "    indices = torch.arange(0, n_KC * time_len).view(n_KC, -1)\n",
    "    neuron = indices[r_KCt_mask][0] // time_len\n",
    "    print('No switch neuron')\n",
    "#     print(r_KCt_switch[neuron, :])\n",
    "    print(r_KCt_switch[neuron, :] + vt_opt[torch.arange(0, n_batch)[~switch_inds][0], :])\n",
    "\n",
    "    # Make a list of stimulus times to plot\n",
    "#     ls_stims = [time_CS_both, time_US_both]\n",
    "    ls_stims = [time_CS_both, time_US]\n",
    "\n",
    "    return r_KCt, r_extt, vt_opt, ls_stims\n",
    "    \n",
    "    \n",
    "# NOT CLEAR WHETHER ALL TASKS NEED TO BE DONE FOR THIS\n",
    "# def all_task_inputs(stim_times, stim_len, time_len, n_KC=200, n_ext=2, n_batch=30, p_omit=0.5):\n",
    "#     \"\"\" Generates inputs for extinction and second-order tasks.\n",
    "#     Trials are either extinction or second-order conditioning. No strictly first-order conditioning\n",
    "#     trials are included (therefore p_exinct = 1 - p_2nd_order).\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Conditioned stimuli (CS) = odors\n",
    "#     r_KC1 = torch.zeros(n_batch, n_KC)\n",
    "#     r_KC2 = torch.zeros_like(r_KC1)\n",
    "#     for b in range(n_batch):\n",
    "#         # Define odors (CS1 and CS2) for each trial\n",
    "#         r_KC_inds = torch.multinomial(torch.ones(n_KC), int(n_KC * 0.1))\n",
    "#         r_KC1[b, r_KC_inds] = 1\n",
    "#         r_KC_inds = torch.multinomial(torch.ones(n_KC), int(n_KC * 0.1))\n",
    "#         r_KC2[b, r_KC_inds] = 1\n",
    "#     # Unconditioned stimuli (US) = context\n",
    "#     r_ext = torch.multinomial(torch.ones(n_batch, n_ext), n_ext)\n",
    "            \n",
    "#     # Determine whether trials are extinction or second-order\n",
    "#     p_extinct = 0.5\n",
    "#     extinct_inds = torch.rand(n_batch) < p_extinct\n",
    "    \n",
    "#     # Determine whether CS or US are randomly omitted\n",
    "#     omit_inds = torch.rand(n_batch) < p_omit\n",
    "#     # If omitted, determine which one is omitted\n",
    "#     p_omit_CS = 0.7\n",
    "#     x_omit_CS = torch.rand(n_batch)\n",
    "#     omit_CS_inds = torch.logical_and(omit_inds, x_omit_CS < p_omit_CS)\n",
    "#     omit_US_inds = torch.logical_and(omit_inds, x_omit_CS > p_omit_CS)\n",
    "\n",
    "#     # Initialize lists to store inputs and target valence\n",
    "#     r_KCt_ls = []\n",
    "#     r_extt_ls = []\n",
    "#     vals = []\n",
    "\n",
    "#     # For each interval\n",
    "#     n_ints = 3\n",
    "#     for i in range(n_ints):\n",
    "#         # Define a binary CS and US time series to mulitply the inputs by\n",
    "#         time_CS1 = torch.zeros(n_batch, time_len)\n",
    "#         time_CS2 = torch.zeros_like(time_CS1)\n",
    "#         time_US = torch.zeros_like(time_CS1)\n",
    "#         # Define the target valences\n",
    "#         val_int = torch.zeros_like(time_CS1)\n",
    "        \n",
    "#         # Set the inputs for each trial\n",
    "#         for b in range(n_batch):            \n",
    "#             # Set the inputs for extinction trials\n",
    "#             if extinct_inds[b]:\n",
    "#                 # Set the CS input times\n",
    "#                 if not omit_CS_inds[b]:\n",
    "#                     stim_inds = stim_times[b, i] + torch.arange(stim_len)\n",
    "#                     time_CS1[b, stim_inds] = 1\n",
    "#                 # Set the US input times\n",
    "#                 if i == 0 and not omit_US_inds[b]:\n",
    "#                     stim_inds = stim_times[b, i] + torch.arange(stim_len) + stim_len\n",
    "#                     time_US[b, stim_inds] = 1\n",
    "#                 # Set the target valence times\n",
    "#                 if i > 0 and not omit_inds[b]:\n",
    "#                     stim_inds = stim_times[b, i] + torch.arange(stim_len)\n",
    "#                     if r_ext[b, 0] == 1:\n",
    "#                         val_int[b, stim_inds] = 1 / i\n",
    "#                     else:\n",
    "#                         val_int[b, stim_inds] = -1 / i\n",
    "                        \n",
    "#             # Set the inputs for second-order conditioning trials\n",
    "#             else:\n",
    "#                 # Set the CS1 input times\n",
    "#                 if not omit_CS_inds[b]:\n",
    "#                     if i == 0:\n",
    "#                         stim_inds1 = stim_times[b, i] + torch.arange(stim_len)\n",
    "#                         time_CS1[b, stim_inds1] = 1\n",
    "#                     if i == 1:\n",
    "#                         stim_inds1 = stim_times[b, i] + torch.arange(stim_len) + stim_len\n",
    "#                         time_CS1[b, stim_inds1] = 1\n",
    "#                         stim_inds2 = stim_times[b, i] + torch.arange(stim_len)\n",
    "#                         time_CS2[b, stim_inds2] = 1\n",
    "#                     if i == 2:\n",
    "#                         stim_inds2 = stim_times[b, i] + torch.arange(stim_len)\n",
    "#                         time_CS2[b, stim_inds2] = 1\n",
    "#                 # Set the US input times\n",
    "#                 if i == 0 and not omit_US_inds[b]:\n",
    "#                     stim_inds = stim_times[b, i] + torch.arange(stim_len) + stim_len\n",
    "#                     time_US[b, stim_inds] = 1\n",
    "#                 # Set the target valence times\n",
    "#                 if i > 0 and not omit_inds[b]:\n",
    "#                     stim_inds = stim_times[b, i] + torch.arange(stim_len) + (i % 2) * stim_len\n",
    "#                     if r_ext[b, 0] == 1:\n",
    "#                         val_int[b, stim_inds] = 1\n",
    "#                     else:\n",
    "#                         val_int[b, stim_inds] = -1\n",
    "\n",
    "#         # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "#         r_KCt = torch.einsum('bm, mbt -> bmt', r_KC1, time_CS1.repeat(n_KC, 1, 1))\n",
    "#         r_KCt += torch.einsum('bm, mbt -> bmt', r_KC2, time_CS2.repeat(n_KC, 1, 1))\n",
    "#         r_extt = torch.einsum('bm, mbt -> bmt', r_ext, time_US.repeat(n_ext, 1, 1))\n",
    "            \n",
    "#         r_KCt_ls.append(r_KCt)\n",
    "#         r_extt_ls.append(r_extt)\n",
    "#         vals.append(val_int)\n",
    "\n",
    "#     # Concatenate target valences\n",
    "#     vt_opt = torch.cat((vals[0], vals[1], vals[2]), dim=-1)\n",
    "        \n",
    "#     return r_KCt_ls, r_extt_ls, vt_opt\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_trial(network, task1:str, task2:str, dt=0.5):\n",
    "def print_trial(network, task, dt=0.5):\n",
    "    \"\"\" Plots a figure similar to Figure 2 from Jiang 2020.\n",
    "    \n",
    "    Runs the network using a novel combination of conditioned and unconditioned stimuli,\n",
    "    then prints the results. Top: time series of the various stimuli (CS and US), as well as\n",
    "    the target valence and readout. Bottom: activity of eight randomly chosen mushroom body\n",
    "    output neurons (MBONs).\n",
    "    \n",
    "    Paramters\n",
    "        network = previously trained RNN\n",
    "        task = the type of task to be plotted ('CS+' or 'CS2')\n",
    "        dt = time step of the simulation/plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the labels for the plots\n",
    "    if task == 'CS+':\n",
    "        task_title = 'First-Order Conditioning'\n",
    "        task_label = 'CS+'\n",
    "    elif task == 'CS2':\n",
    "        task_title = 'CS+ Generalization'\n",
    "        task_label = 'CS2'\n",
    "#     if task == 'first-order':\n",
    "#         if task2 == 'CS+':\n",
    "#             task_title = 'First-Order Conditioning (CS+)'\n",
    "#             task_label = 'CS+'\n",
    "#         elif task2 == 'CS-':\n",
    "#             task_title = 'First-Order Conditioning (CS-)'\n",
    "#             task_label = 'CS-'\n",
    "#     elif task1 == 'all_tasks':\n",
    "#         if task2 == 'extinct':\n",
    "#             task_title = 'Extinction Conditioning'\n",
    "#             task_label = 'CS'\n",
    "#         if task2 == 'CS2':\n",
    "#             task_title = 'Second-Order Conditioning'\n",
    "#             task_label = 'CS1'\n",
    "    \n",
    "    # Define plot font sizes\n",
    "    label_font = 18\n",
    "    title_font = 24\n",
    "    legend_font = 12\n",
    "\n",
    "    # Run the network\n",
    "#     r_out, vt, vt_opt, loss_hist, stim_ls = train_net(network, task=task1, dt=dt, n_epochs=1, n_batch=1, train=False, plot=task2)\n",
    "    r_out, vt, vt_opt, loss_hist, stim_ls = train_net(network, dt=dt, n_epochs=1, n_batch=1, train=False, plot=task)\n",
    "    r_out = r_out.detach().numpy().squeeze()\n",
    "    vt = vt.detach().numpy().squeeze()\n",
    "    vt_opt = vt_opt.detach().numpy().squeeze()\n",
    "    plot_CS = stim_ls[0].numpy().squeeze()\n",
    "    plot_US = stim_ls[1].numpy().squeeze()\n",
    "    plot_time = np.arange(plot_CS.size) * dt\n",
    "\n",
    "    # Plot the conditioning and test\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True, gridspec_kw={'height_ratios': [1, 4]})\n",
    "    ax1.plot(plot_time, vt, label='Readout')\n",
    "    ax1.plot(plot_time, vt_opt, label='Target')\n",
    "    # Second-order conditioning involves an additional stimulus time series\n",
    "    if task == 'CS+':\n",
    "        ax1.plot(plot_time, plot_CS, label='CS+')\n",
    "    else:\n",
    "        plot_CSp = np.zeros(plot_time.size)\n",
    "        half_ind = plot_time.size // 2\n",
    "        plot_CSp[:half_ind] = plot_CS[:half_ind]\n",
    "        plot_CS2 = np.zeros(plot_time.size)\n",
    "        plot_CS2[half_ind:] = plot_CS[half_ind:]\n",
    "        ax1.plot(plot_time, plot_CSp, label='CS+')\n",
    "        ax1.plot(plot_time, plot_CS2, label='CS2')\n",
    "    ax1.plot(plot_time, plot_US, label='US')\n",
    "    ax1.set_ylabel('Value', fontsize=label_font)\n",
    "    ax1.set_title(task_title, fontsize=title_font)\n",
    "    ax1.legend(fontsize=legend_font)\n",
    "\n",
    "    # Plot the activities of a few MBONs\n",
    "    plot_neurs = np.random.choice(network.N_MBON, size=8, replace=False)\n",
    "    r_max = np.max(r_out)\n",
    "    for i, n in enumerate(plot_neurs):\n",
    "        ax2.plot(plot_time, (r_out[n, :] / r_max) + (i * 2 / 3), '-k')\n",
    "    ax2.set_xlabel('Time', fontsize=label_font)\n",
    "    ax2.set_ylabel('Normalized Activity', fontsize=label_font)\n",
    "    ax2.set_yticks([])\n",
    "    fig.tight_layout();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([60, 2])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "classic_net = Drosophila_RNN()\n",
    "for param in classic_net.parameters():\n",
    "    print(param.shape)\n",
    "#     print(param)\n",
    "# print(classic_net.N_DAN)\n",
    "\n",
    "# Define the model's optimizer\n",
    "lr = 0.001\n",
    "optimizer = optim.RMSprop(classic_net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 2, 1])\n",
      "tensor([25])\n",
      "tensor([11])\n",
      "tensor([23])\n",
      "tensor([16])\n",
      "tensor([17])\n",
      "tensor([16])\n",
      "tensor([17])\n",
      "tensor([18])\n",
      "tensor([22])\n",
      "tensor([18])\n",
      "tensor([19])\n",
      "tensor([19])\n",
      "tensor([11])\n",
      "tensor([22])\n",
      "tensor([21])\n",
      "tensor([24])\n",
      "tensor([14])\n",
      "tensor([18])\n",
      "tensor([18])\n",
      "tensor([22])\n",
      "tensor([23])\n",
      "tensor([16])\n",
      "tensor([20])\n",
      "tensor([25])\n",
      "tensor([24])\n",
      "tensor([25])\n",
      "tensor([21])\n",
      "tensor([23])\n",
      "tensor([17])\n",
      "tensor([11])\n",
      "tensor([41])\n",
      "tensor([52])\n",
      "tensor([48])\n",
      "tensor([41])\n",
      "tensor([51])\n",
      "tensor([51])\n",
      "tensor([51])\n",
      "tensor([41])\n",
      "tensor([40])\n",
      "tensor([53])\n",
      "tensor([40])\n",
      "tensor([46])\n",
      "tensor([46])\n",
      "tensor([53])\n",
      "tensor([48])\n",
      "tensor([55])\n",
      "tensor([53])\n",
      "tensor([45])\n",
      "tensor([49])\n",
      "tensor([53])\n",
      "tensor([52])\n",
      "tensor([55])\n",
      "tensor([41])\n",
      "tensor([45])\n",
      "tensor([51])\n",
      "tensor([50])\n",
      "tensor([40])\n",
      "tensor([50])\n",
      "tensor([42])\n",
      "tensor([53])\n",
      "Switch neuron\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "No switch neuron\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 2., 2., 2., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "0 40.450660705566406\n",
      "torch.Size([30, 2, 1])\n",
      "tensor([22])\n",
      "tensor([14])\n",
      "tensor([12])\n",
      "tensor([12])\n",
      "tensor([17])\n",
      "tensor([17])\n",
      "tensor([21])\n",
      "tensor([20])\n",
      "tensor([17])\n",
      "tensor([16])\n",
      "tensor([16])\n",
      "tensor([12])\n",
      "tensor([23])\n",
      "tensor([13])\n",
      "tensor([23])\n",
      "tensor([21])\n",
      "tensor([15])\n",
      "tensor([25])\n",
      "tensor([16])\n",
      "tensor([20])\n",
      "tensor([21])\n",
      "tensor([15])\n",
      "tensor([22])\n",
      "tensor([12])\n",
      "tensor([24])\n",
      "tensor([12])\n",
      "tensor([19])\n",
      "tensor([15])\n",
      "tensor([24])\n",
      "tensor([10])\n",
      "tensor([48])\n",
      "tensor([40])\n",
      "tensor([47])\n",
      "tensor([49])\n",
      "tensor([44])\n",
      "tensor([40])\n",
      "tensor([53])\n",
      "tensor([52])\n",
      "tensor([46])\n",
      "tensor([46])\n",
      "tensor([45])\n",
      "tensor([42])\n",
      "tensor([55])\n",
      "tensor([52])\n",
      "tensor([54])\n",
      "tensor([47])\n",
      "tensor([44])\n",
      "tensor([54])\n",
      "tensor([40])\n",
      "tensor([47])\n",
      "tensor([49])\n",
      "tensor([47])\n",
      "tensor([41])\n",
      "tensor([46])\n",
      "tensor([42])\n",
      "tensor([43])\n",
      "tensor([47])\n",
      "tensor([40])\n",
      "tensor([46])\n",
      "tensor([48])\n",
      "Switch neuron\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "No switch neuron\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 2., 2., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([30, 2, 1])\n",
      "tensor([16])\n",
      "tensor([25])\n",
      "tensor([20])\n",
      "tensor([22])\n",
      "tensor([22])\n",
      "tensor([11])\n",
      "tensor([10])\n",
      "tensor([20])\n",
      "tensor([16])\n",
      "tensor([23])\n",
      "tensor([24])\n",
      "tensor([22])\n",
      "tensor([22])\n",
      "tensor([21])\n",
      "tensor([17])\n",
      "tensor([18])\n",
      "tensor([19])\n",
      "tensor([21])\n",
      "tensor([24])\n",
      "tensor([22])\n",
      "tensor([15])\n",
      "tensor([24])\n",
      "tensor([20])\n",
      "tensor([14])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cc0f8cbe47ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     r_out, vt, vt_opt, loss_hist, _ = train_net(classic_net, task=train_task, n_epochs=5000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     r_out, vt, vt_opt, loss_hist, _ = train_net(classic_net, task=train_task, n_epochs=2000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassic_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-6d7948e3f8c2>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(network, T_int, T_stim, dt, n_epochs, n_batch, clip, train, plot)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Generate the odor (r_KC) and context (r_ext) inputs, and target valence (vt_opt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m#         r_KC, r_ext, vt_opt, ls_stims = f_gen_inputs(stim_times, stim_len, time_int.size()[0], odor_list, n_KC, n_ext, n_batch, plot)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mr_KC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls_stims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_order_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstim_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstim_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0modor_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_KC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Run the forward model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-28aeb107c524>\u001b[0m in \u001b[0;36mfirst_order_inputs\u001b[0;34m(stim_times, stim_len, time_len, odor_list, n_KC, n_ext, n_batch, plot, p_omit)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mstim_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstim_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstim_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstim_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Set the CS time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mtime_CS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstim_inds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[1;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_task = 'first-order'\n",
    "# train_task = 'all_tasks'\n",
    "train_bool = True\n",
    "if train_bool:\n",
    "#     r_out, vt, vt_opt, loss_hist, _ = train_net(classic_net, task=train_task, n_epochs=5000)\n",
    "#     r_out, vt, vt_opt, loss_hist, _ = train_net(classic_net, task=train_task, n_epochs=2000)\n",
    "    r_out, vt, vt_opt, loss_hist, _ = train_net(classic_net, n_epochs=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "axes[0].plot(loss_hist)\n",
    "axes[0].set_xlabel('Epoch', fontsize=label_font)\n",
    "axes[0].set_ylabel('Loss', fontsize=label_font)\n",
    "axes[1].plot(loss_hist[10:])\n",
    "axes[1].set_xlabel('Epoch', fontsize=label_font)\n",
    "axes[1].set_ylabel('Loss', fontsize=label_font)\n",
    "fig.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(classic_net.state_dict(), 'trained_N5_no_plasticity_2000epochs.pt')\n",
    "\n",
    "# https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some results to compare to Fig 2A/B in paper\n",
    "if not train_bool:\n",
    "#     classic_net.load_state_dict(torch.load('trained_N1_first-order_5000epochs.pt'))\n",
    "    classic_net.eval()\n",
    "\n",
    "print_trial(classic_net, task='CS+')\n",
    "print_trial(classic_net, task='CS2')\n",
    "# if train_task == 'first-order':\n",
    "#     print_trial(classic_net, task1=train_task, task2='CS+')\n",
    "#     print_trial(classic_net, task1=train_task, task2='CS-')\n",
    "# elif train_task == 'all_tasks':\n",
    "#     print_trial(classic_net, task1=train_task, task2='extinct')\n",
    "#     print_trial(classic_net, task1=train_task, task2='CS2')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
