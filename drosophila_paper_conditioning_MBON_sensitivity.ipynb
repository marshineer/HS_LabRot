{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "# from numpy.linalg import svd as svd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define plot font sizes\n",
    "label_font = 18\n",
    "title_font = 24\n",
    "legend_font = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model Class - Classical Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drosophila_RNN(nn.Module):\n",
    "    def __init__(self, KC_size=200, MBON_size=20, DAN_size=20, FBN_size=60, ext_size=2, out_size=1, net_seed=1234):\n",
    "        super(Drosophila_RNN, self).__init__()\n",
    "        # Set the seeds\n",
    "#         np.random.seed(net_seed)\n",
    "#         torch.manual_seed(net_seed)\n",
    "        # Set constants\n",
    "        self.KC_MBON_min = 0. # Minimum synaptic weight\n",
    "        self.KC_MBON_max = 0.05 # Maximum synaptic weight\n",
    "        self.tau_w = 5 # Time scale of KC->MBON LTD/LTP (plasticity)\n",
    "        self.tau_r = 1 # Time scale of output circuitry activity\n",
    "        # Set the sizes of layers\n",
    "        self.N_KC = KC_size\n",
    "        self.N_MBON = MBON_size\n",
    "        self.N_FBN = FBN_size\n",
    "        self.N_DAN = DAN_size\n",
    "        self.N_recur = MBON_size + FBN_size + DAN_size\n",
    "        self.N_ext = ext_size\n",
    "        self.N_out = out_size\n",
    "        # Define updatable network parameters\n",
    "#         seed_num = net_seed\n",
    "        seed_num = None\n",
    "        sqrt2 = torch.sqrt(torch.tensor(2, dtype=torch.float))\n",
    "        mean_MBON = torch.zeros((self.N_recur, MBON_size))\n",
    "        mean_FBN = torch.zeros((self.N_recur, FBN_size))\n",
    "        mean_DAN = torch.zeros((self.N_recur, DAN_size))\n",
    "        W_MBON = torch.normal(mean_MBON, torch.sqrt(1 / (sqrt2 * MBON_size)), generator=seed_num)\n",
    "        W_FBN = torch.normal(mean_FBN, torch.sqrt(1 / (sqrt2 * FBN_size)), generator=seed_num)\n",
    "        W_DAN = torch.normal(mean_DAN, torch.sqrt(1 / (sqrt2 * DAN_size)), generator=seed_num)\n",
    "        self.W_recur = nn.Parameter(torch.cat((W_MBON, W_FBN, W_DAN), dim=1), requires_grad=True)\n",
    "        self.W_ext = nn.Parameter(torch.randn(FBN_size, ext_size), requires_grad=True)\n",
    "        mean_readout = torch.zeros((out_size, MBON_size))\n",
    "        std_readout = 1 / torch.sqrt(torch.tensor(MBON_size, dtype=torch.float))\n",
    "        self.W_readout = nn.Parameter(torch.normal(mean_readout, std_readout, generator=seed_num), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones(self.N_recur) * 0.1, requires_grad=True)\n",
    "        \n",
    "            \n",
    "    def forward(self, r_KC, r_ext, time, W0=None, batch_size=30):\n",
    "        \"\"\" Defines the forward pass of the RNN\n",
    "        \n",
    "        Synaptic weights from the Keyon cells to the mushroom body output neurons (MBONs) are updated\n",
    "        dynamically. All other weights are network parameters. The synaptic connections between Keyon\n",
    "        Cells (KCs) and MBONs are updated using a LTP/LTD rule (see Figure 1B of Jiang 2020), which\n",
    "        models dopamine-gated neural plasticity on short time scale (behavioural learning).\n",
    "        \n",
    "        The KC->MBON weights are constrained to the range [0, 0.05].\n",
    "        MBONs receive external input from Keyon cells (r_KC i.e. 'odors').\n",
    "        Feedback neurons (FBNs) receive external contextual input (r_ext i.e. 'context').\n",
    "        DAN->MBON weights are permanently set to zero. DANs receive no external input.\n",
    "\n",
    "        Inputs\n",
    "            r_KC = activity of the Kenyon cell neurons (representing odors)\n",
    "            r_ext = context signals (representing the conditioning context)\n",
    "            time = time vector for a single interval\n",
    "            W0 = initial weights for KC->MBON connections\n",
    "            batch_size = number of trials in batch\n",
    "\n",
    "        Returns\n",
    "            r_recur: list of torch.ndarray(batch_size, N_MBON + N_FBN + N_DAN)\n",
    "                = time series of activities in the output circuitry\n",
    "            Wt: list of torch.ndarray(batch_size, N_MBON + N_FBN + N_DAN, N_MBON + N_FBN + N_DAN)\n",
    "                = time series of KC->MBON weights (represent dopaminergic plasticity)\n",
    "            readout: list of torch.ndarray(batch_size, 1)\n",
    "                = time series of valence readouts (represents behaviour)\n",
    "        \"\"\"\n",
    "\n",
    "        # Define the time step of the simulation\n",
    "        dt = np.diff(time)[0]\n",
    "\n",
    "        # Initialize output circuit firing rates for each trial\n",
    "        r_init = torch.ones((batch_size, self.N_recur)) * 0.1\n",
    "        r_init[:, :self.N_MBON] = 0\n",
    "        r_recur = []\n",
    "        r_recur.append(r_init)\n",
    "\n",
    "        # Initialize the eligibility traces, dynamic weights and readout\n",
    "        r_bar_KC = r_KC[:, :, 0]\n",
    "        r_bar_DAN = r_recur[-1][:, -self.N_DAN:]\n",
    "        wt = []\n",
    "        wt.append(torch.zeros((batch_size, self.N_MBON, self.N_KC)))\n",
    "        W_KC_MBON = []\n",
    "        if W0 is None:\n",
    "            W0 = torch.ones((batch_size, self.N_MBON, self.N_KC)) * self.KC_MBON_max\n",
    "        W_KC_MBON.append(W0)\n",
    "        readout = []\n",
    "        readout.append(torch.squeeze(torch.einsum('bom, bm -> bo', self.W_readout.repeat(batch_size, 1, 1), r_recur[-1][:, :self.N_MBON])))\n",
    "\n",
    "        # Set the weights DAN->MBON to zero\n",
    "        W_recur = self.W_recur.clone()\n",
    "        W_recur[:self.N_MBON, -self.N_DAN:] = 0\n",
    "\n",
    "        # Update activity for each time step\n",
    "        for t in range(time.size()[0] - 1):\n",
    "            # Define the input to the output circuitry\n",
    "            I_KC_MBON = torch.einsum('bmk, bk -> bm', W_KC_MBON[-1], r_KC[:, :, t])\n",
    "            I_FBN = torch.einsum('bfe, be -> bf', self.W_ext.repeat(batch_size, 1, 1), r_ext[:, :, t])\n",
    "            I = torch.zeros((batch_size, self.N_recur))\n",
    "            I[:, :self.N_MBON] = I_KC_MBON\n",
    "            I[:, self.N_MBON:self.N_MBON + self.N_FBN] = I_FBN\n",
    "\n",
    "            # Update the output circuitry activity (see Eq. 1)\n",
    "            Wr_prod = torch.einsum('bsr, br -> bs', W_recur.repeat(batch_size, 1, 1), r_recur[-1])\n",
    "            dr = (-r_recur[-1] + F.relu(Wr_prod + self.bias.repeat(batch_size, 1) + I)) / self.tau_r\n",
    "            r_recur.append(r_recur[-1] + dr * dt)\n",
    "\n",
    "            # Update KC->MBON plasticity variables\n",
    "            # Calculate the eligibility traces (represent LTP/LTD)\n",
    "            r_bar_KC = r_bar_KC + (r_KC[:, :, t] - r_bar_KC) * dt / self.tau_w\n",
    "            r_bar_DAN = r_bar_DAN + (r_recur[-1][:, -self.N_DAN:] - r_bar_DAN) * dt / self.tau_w\n",
    "            # Update the dynamic weight variable (see Eq. 4)\n",
    "            prod1 = torch.einsum('bd, bk -> bdk', r_bar_DAN, r_KC[:, :, t])\n",
    "            prod2 = torch.einsum('bd, bk -> bdk', r_recur[-1][:, -self.N_DAN:], r_bar_KC)\n",
    "            dw = (prod1 - prod2)\n",
    "            wt.append(wt[-1] + dw * dt)\n",
    "            # Update the KC->MBON weights (see Eq. 8)\n",
    "            dW = (-W_KC_MBON[-1] + wt[-1]) / self.tau_w\n",
    "            W_tp1 = W_KC_MBON[-1] + dW * dt\n",
    "            # Clip the KC->MBON weights to the range [0, 0.05]\n",
    "            W_KC_MBON.append(torch.clamp(W_tp1, self.KC_MBON_min, self.KC_MBON_max))\n",
    "\n",
    "            # Calculate the readout (see Eq. 2)\n",
    "            readout.append(torch.squeeze(torch.einsum('bom, bm -> bo', self.W_readout.repeat(batch_size, 1, 1), r_recur[-1][:, :self.N_MBON])))\n",
    "\n",
    "        return r_recur, W_KC_MBON, readout\n",
    "            \n",
    "        \n",
    "# Clipping weights between [0, 0.05]\n",
    "# https://discuss.pytorch.org/t/how-to-do-constrained-optimization-in-pytorch/60122\n",
    "# https://discuss.pytorch.org/t/set-constraints-on-parameters-or-layers/23620\n",
    "# https://discuss.pytorch.org/t/restrict-range-of-variable-during-gradient-descent/1933/4\n",
    "\n",
    "# Setting DAN->MBON weights to zero\n",
    "# https://pytorch.org/docs/stable/generated/torch.triu.html\n",
    "\n",
    "# Broadcasting using einsum\n",
    "# https://github.com/pytorch/pytorch/issues/15671\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function for conditioning tasks\n",
    "def cond_loss(vt, vt_opt, r_DAN, lam=0.1):\n",
    "    \"\"\" Calculates the loss for conditioning tasks.\n",
    "    \n",
    "    Composed of an MSE cost based on the difference between output and\n",
    "    target valence, and a regularization cost that penalizes excess\n",
    "    dopaminergic activity. Reference Eqs. (3) and (9) in Jiang 2020.\n",
    "    \n",
    "    Parameters\n",
    "        vt = time dependent valence output of network\n",
    "        vt_opt = target valence (must be a torch tensor)\n",
    "        r_DAN = time series of dopaminergic neuron activities\n",
    "        lam = regularization constant\n",
    "    \n",
    "    Returns\n",
    "        loss_tot = scalar loss used in backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the baseline DAN activity\n",
    "    DAN_baseline = 0.1\n",
    "    \n",
    "    # Calculate the MSE loss of the valence\n",
    "    v_sum = torch.mean((vt - vt_opt)**2, dim=1)\n",
    "    v_loss = torch.mean(v_sum)\n",
    "    \n",
    "    # Calculate regularization term\n",
    "    r_sum = torch.sum(F.relu(r_DAN - 0.1)**2, dim=1)\n",
    "    r_loss = lam * torch.mean(r_sum, dim=1)\n",
    "    \n",
    "    # Calculate the summed loss (size = n_batch)\n",
    "    loss = v_loss + r_loss\n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    loss_tot = torch.mean(loss)\n",
    "    \n",
    "    return loss_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a training function for first-order conditioning\n",
    "def train_net(network, task: str, T_int=30, T_stim=2, dt=0.5, n_epochs=5000, n_batch=30, p_ctrl=0.5, clip=0.001, train=True, plot=None):\n",
    "    \"\"\" Trains a network on classical conditioning tasks.\n",
    "    \n",
    "    Tasks include first-order or second-order conditioning, and extinction. Tasks consist of\n",
    "    two (first-order) or three (second-order and extinction) intervals. Each task has its own\n",
    "    input generating function. Stimuli are presented between 5-15s of each interval. Neuron\n",
    "    activities are reset between intervals to prevent associations being represented through\n",
    "    persistent activity.\n",
    "    \n",
    "    Parameters\n",
    "        network = RNN network to be trained or ran\n",
    "        task = type of conditioning task to be trained ('first-order', 'all_tasks')\n",
    "        T_int = length of task intervals (eg conditioning, test, extinction)\n",
    "        T_stim = length of time each stimulus is presented\n",
    "        dt = time step of simulations\n",
    "        n_epochs = number of epochs to train over\n",
    "        n_batch = number of trials in mini-batch\n",
    "        p_ctrl = the fraction of trials that are control (to prevent over-fitting)\n",
    "        clip = maximum gradient allowed during training\n",
    "        train = boolean indicating whether to perform backprop\n",
    "        plot = type of task to run (for plotting purposes)\n",
    "        \n",
    "    Returns\n",
    "        r_out_epoch = output circuit neuron activities for final epoch\n",
    "        Wt_epoch = KC->MBON weights for final epoch\n",
    "        vt_epoch = readout (i.e. valence) for final epoch\n",
    "        vt_opt = target valence for final epoch\n",
    "        loss_hist = list of losses for all epochs\n",
    "        ls_stims = list of stimulus time series for plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set variables dependent on task\n",
    "    if task == 'first-order':\n",
    "        n_ints = 2\n",
    "        f_gen_inputs = first_order_inputs\n",
    "    elif task == 'all_tasks':\n",
    "        n_ints = 3\n",
    "        f_gen_inputs = all_task_inputs\n",
    "    \n",
    "    # Present the stimulus between 5-15s of each interval\n",
    "    stim_min = 5\n",
    "    stim_max = 15 - T_stim\n",
    "    stim_range = int((stim_max - stim_min) / dt)\n",
    "    stim_wts = torch.ones(n_batch, stim_range)\n",
    "    stim_offset = int(stim_min / dt)\n",
    "    # Length of stimulus in indices\n",
    "    stim_len = int(T_stim / dt)\n",
    "    # Interval time vector\n",
    "    time_int = torch.arange(0, T_int + dt/10, dt)\n",
    "\n",
    "    # Neuron population sizes\n",
    "    n_KC = network.N_KC\n",
    "    n_ext = network.N_ext\n",
    "    n_MBON = network.N_MBON\n",
    "    # Max KC->MBON weight values\n",
    "    W_KC_MBON_max = network.KC_MBON_max\n",
    "\n",
    "    # List to store losses\n",
    "    loss_hist = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Lists to store activities, weights, readouts and target valences\n",
    "        r_outs = []\n",
    "        Wts = []\n",
    "        vts = []\n",
    "\n",
    "        # Set the intial KC->MBON weight values for each trial\n",
    "        W_KC_MBON = torch.ones((n_batch, n_MBON, n_KC)) * W_KC_MBON_max\n",
    "\n",
    "        # Randomly determine the time of each stimulus presentation\n",
    "        stim_times = torch.multinomial(stim_wts, n_ints, replacement=True) + stim_offset\n",
    "        \n",
    "        # Generate the odor (r_KC) and context (r_ext) inputs, and target valence (vt_opt)\n",
    "        r_KC, r_ext, vt_opt, ls_stims = f_gen_inputs(stim_times, stim_len, time_int.size()[0], n_KC, n_ext, n_batch, plot, p_ctrl)\n",
    "\n",
    "        # For each interval in the task\n",
    "        for i in range(n_ints):\n",
    "            # Run the forward model\n",
    "            r_int, W_KC_MBON_int, vt = network(r_KC[i], r_ext[i], time_int, W_KC_MBON, n_batch)\n",
    "            # Set the initial KC->MBON weights for the next interval\n",
    "            W_KC_MBON = W_KC_MBON_int[-1]\n",
    "            \n",
    "            # Append the interval outputs to lists\n",
    "            r_outs += r_int\n",
    "            vts += vt\n",
    "            Wts += W_KC_MBON_int\n",
    "\n",
    "        # Concatenate the activities, weights and valences\n",
    "        r_out_epoch = torch.stack(r_outs, dim=-1)\n",
    "        Wt_epoch = torch.stack(Wts, dim=-1)\n",
    "        vt_epoch = torch.stack(vts, dim=-1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = cond_loss(vt_epoch, vt_opt, r_out_epoch[:, -network.N_DAN:, :])\n",
    "\n",
    "        if train:\n",
    "            # Update the network parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(network.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print an update\n",
    "        if epoch % 500 == 0:\n",
    "            print(epoch, loss.item())\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "    return r_out_epoch, Wt_epoch, vt_epoch, vt_opt, loss_hist, ls_stims\n",
    "\n",
    "# https://discuss.pytorch.org/t/proper-way-to-do-gradient-clipping/191/13\n",
    "# https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48\n",
    "# https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for each of the conditioning tasks\n",
    "# Some detail provided in Jiang 2020 -> Methods -> Conditioning Tasks\n",
    "def first_order_inputs(stim_times, stim_len, time_len, n_KC, n_ext, n_batch, plot=None, p_omit=0.3):\n",
    "    \"\"\" Generates inputs for first-order conditioning tasks.\n",
    "    \n",
    "    All trials are either CS+, CS- (US omitted) or CS omitted (control trials to avoid over-fitting).\n",
    "    Of the trials where CS or US is omitted, a second parameter determines the relative fractions of\n",
    "    CS or US trials omitted (p_omit_CS). See Figure 2 of Jiang 2020 to determine sequencing of stimuli\n",
    "    during training. To account for the sequential nature of numerical simulations, the target valence\n",
    "    is set to begin one time step after stimulus onset.\n",
    "    \n",
    "    The mix of conditions is listed as follows:\n",
    "        probability of CS+ trials = 1 - p_omit\n",
    "        probability of CS- trials = p_omit * 0.3\n",
    "        probability of control trials = p_omit * 0.7\n",
    "    \n",
    "    Parameters\n",
    "        stim_times = randomly selected indices of stimulus presentations for each interval\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "        time_len = size of time vector\n",
    "        n_KC = number of Kenyon cell input neurons\n",
    "        n_ext = number of contextual input neurons\n",
    "        n_batch = number of trials in mini-batch\n",
    "        plot = used when plot function is called, indicates which task to plot\n",
    "        p_omit = probability of omitting either CS or US from trials\n",
    "        \n",
    "    Returns\n",
    "        r_KCt_ls = list of odor (KC) input time series arrays for each interval\n",
    "        r_extt_ls = list of context (ext) input time series arrays for each interval\n",
    "        vt_opt = time series of target valence for plotting and loss calculations\n",
    "        ls_stims = list of stimulus time series for plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Conditioned stimuli (CS) = odors\n",
    "    r_KC = torch.zeros(n_batch, n_KC)\n",
    "    for b in range(n_batch):\n",
    "        # Define an odor (CS) for each trial\n",
    "        r_KC_inds = torch.multinomial(torch.ones(n_KC), int(n_KC * 0.1))\n",
    "        r_KC[b, r_KC_inds] = 1\n",
    "    # Unconditioned stimuli (US) = context\n",
    "    r_ext = torch.multinomial(torch.ones(n_batch, n_ext), n_ext)\n",
    "            \n",
    "    # Determine whether CS or US are randomly omitted\n",
    "    omit_inds = torch.rand(n_batch) < p_omit\n",
    "    # If omitted, determine which one is omitted\n",
    "    p_omit_CS = 0.7\n",
    "    x_omit_CS = torch.rand(n_batch)\n",
    "    omit_CS_inds = torch.logical_and(omit_inds, x_omit_CS < p_omit_CS)\n",
    "    omit_US_inds = torch.logical_and(omit_inds, x_omit_CS > p_omit_CS)\n",
    "\n",
    "    # Initialize lists to store inputs, target valence and stimulus times\n",
    "    r_KCt_ls = []\n",
    "    r_extt_ls = []\n",
    "    vals = []\n",
    "    ls_CS = []\n",
    "    ls_US = []\n",
    "\n",
    "    # For each interval\n",
    "    n_ints = 2\n",
    "    for i in range(n_ints):\n",
    "        # Initialize time matrices\n",
    "        time_CS = torch.zeros(n_batch, time_len)\n",
    "        time_US = torch.zeros_like(time_CS)\n",
    "        val_int = torch.zeros_like(time_CS)\n",
    "        \n",
    "        if plot is None:\n",
    "            for b in range(n_batch):\n",
    "                stim_inds = stim_times[b, i] + torch.arange(stim_len)\n",
    "                # Set the CS input times\n",
    "                if not omit_CS_inds[b]:\n",
    "                    time_CS[b, stim_inds] = 1\n",
    "                # Set the US input times\n",
    "                if i == 0 and not omit_US_inds[b]:\n",
    "                    time_US[b, (stim_inds + stim_len)] = 1\n",
    "                # Set the target valence times\n",
    "                if i == 1 and not omit_inds[b]:\n",
    "                    if r_ext[b, 0] == 1:\n",
    "                        val_int[b, (stim_inds + 1)] = 1\n",
    "                    else:\n",
    "                        val_int[b, (stim_inds + 1)] = -1\n",
    "        else:\n",
    "            for b in range(n_batch):\n",
    "                # Set the CS input times\n",
    "                stim_inds = stim_times[b, i] + torch.arange(stim_len)\n",
    "                time_CS[b, stim_inds] = 1\n",
    "                if plot == 'CS+':\n",
    "                    # Set the US input times\n",
    "                    if i == 0:\n",
    "                        time_US[b, (stim_inds + stim_len)] = 1\n",
    "                    # Set the target valence times\n",
    "                    elif i == 1:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            val_int[b, (stim_inds + 1)] = 1\n",
    "                        else:\n",
    "                            val_int[b, (stim_inds + 1)] = -1\n",
    "\n",
    "        # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "        r_KCt = torch.einsum('bm, mbt -> bmt', r_KC, time_CS.repeat(n_KC, 1, 1))\n",
    "        r_extt = torch.einsum('bm, mbt -> bmt', r_ext, time_US.repeat(n_ext, 1, 1))\n",
    "            \n",
    "        r_KCt_ls.append(r_KCt)\n",
    "        r_extt_ls.append(r_extt)\n",
    "        vals.append(val_int)\n",
    "        ls_CS += time_CS\n",
    "        ls_US += time_US\n",
    "        \n",
    "    # Concatenate target valences\n",
    "    vt_opt = torch.cat((vals[0], vals[1]), dim=-1)\n",
    "    \n",
    "    # Make a list of stimulus times to plot\n",
    "    ls_stims = [torch.cat(ls_CS), torch.cat(ls_US)]\n",
    "        \n",
    "    return r_KCt_ls, r_extt_ls, vt_opt, ls_stims\n",
    "    \n",
    "    \n",
    "def all_task_inputs(stim_times, stim_len, time_len, n_KC, n_ext, n_batch, plot=None, p_omit=0.5):\n",
    "    \"\"\" Generates inputs for extinction and second-order tasks.\n",
    "    \n",
    "    Trials are either extinction or second-order conditioning. No strictly first-order conditioning\n",
    "    trials are included. In half of trials, CS or US are omitted (pg 28 of Jiang 2020) to prevent\n",
    "    overfitting. Of the trials where CS or US is omitted, a second parameter determines the relative\n",
    "    fractions of CS or US trials omitted (p_omit_CS). There are no explicit first-order conditioning\n",
    "    tasks included, since first-order conditioning is a necessary part of both extinction and \n",
    "    second-order conditioning. See Figure 2 of Jiang 2020 to determine sequencing of stimuli during\n",
    "    training. To account for the sequential nature of numerical simulations, the target valence\n",
    "    is set to begin one time step after stimulus onset.\n",
    "    \n",
    "    The mix of conditions is listed as follows:\n",
    "        probability of extinction trials = p_extinct = 0.5\n",
    "        probability of second-order conditioning trials = 1 - p_extinct = 0.5\n",
    "        probability of control (US omitted) trials = p_omit * 0.3\n",
    "        probability of control (CS omitted) trials = p_omit * 0.7\n",
    "    Note: extinction and second-order trials overlap arbitrarily with control trials\n",
    "    \n",
    "    Parameters\n",
    "        stim_times = randomly selected indices of stimulus presentations for each interval\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "        time_len = size of time vector\n",
    "        n_KC = number of Kenyon cell input neurons\n",
    "        n_ext = number of contextual input neurons\n",
    "        n_batch = number of trials in mini-batch\n",
    "        plot = used when plot function is called, indicates which task to plot\n",
    "        p_omit = probability of omitting either CS or US from trials\n",
    "        \n",
    "    Returns\n",
    "        r_KCt_ls = list of odor (KC) input time series arrays for each interval\n",
    "        r_extt_ls = list of context (ext) input time series arrays for each interval\n",
    "        vt_opt = time series of target valence for plotting and loss calculations\n",
    "        ls_stims = list of stimulus time series for plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Conditioned stimuli (CS) = odors\n",
    "    r_KC1 = torch.zeros(n_batch, n_KC)\n",
    "    r_KC2 = torch.zeros_like(r_KC1)\n",
    "    for b in range(n_batch):\n",
    "        # Define odors (CS1 and CS2) for each trial\n",
    "        r_KC_inds = torch.multinomial(torch.ones(n_KC), int(n_KC * 0.1))\n",
    "        r_KC1[b, r_KC_inds] = 1\n",
    "        r_KC_inds = torch.multinomial(torch.ones(n_KC), int(n_KC * 0.1))\n",
    "        r_KC2[b, r_KC_inds] = 1\n",
    "    # Unconditioned stimuli (US) = context\n",
    "    r_ext = torch.multinomial(torch.ones(n_batch, n_ext), n_ext)\n",
    "            \n",
    "    # Determine whether trials are extinction or second-order\n",
    "    p_extinct = 0.5\n",
    "    extinct_inds = torch.rand(n_batch) < p_extinct\n",
    "    \n",
    "    # Determine whether CS or US are randomly omitted\n",
    "    omit_inds = torch.rand(n_batch) < p_omit\n",
    "    # If omitted, determine which one is omitted\n",
    "    p_omit_CS = 0.7\n",
    "    x_omit_CS = torch.rand(n_batch)\n",
    "    omit_CS_inds = torch.logical_and(omit_inds, x_omit_CS < p_omit_CS)\n",
    "    omit_US_inds = torch.logical_and(omit_inds, x_omit_CS > p_omit_CS)\n",
    "\n",
    "    # Initialize lists to store inputs and target valence\n",
    "    r_KCt_ls = []\n",
    "    r_extt_ls = []\n",
    "    vals = []\n",
    "    ls_CS1 = []\n",
    "    ls_CS2 = []\n",
    "    ls_US = []\n",
    "\n",
    "    # For each interval\n",
    "    n_ints = 3\n",
    "    for i in range(n_ints):\n",
    "        # Define a binary CS and US time series to mulitply the inputs by\n",
    "        time_CS1 = torch.zeros(n_batch, time_len)\n",
    "        time_CS2 = torch.zeros_like(time_CS1)\n",
    "        time_US = torch.zeros_like(time_CS1)\n",
    "        # Define the target valences\n",
    "        val_int = torch.zeros_like(time_CS1)\n",
    "        \n",
    "        # Set the inputs for each trial\n",
    "        if plot is None:\n",
    "            for b in range(n_batch):\n",
    "                stim_inds = stim_times[b, i] + torch.arange(stim_len)\n",
    "                # Set the inputs for extinction trials\n",
    "                if extinct_inds[b]:\n",
    "                    # Set the CS input times\n",
    "                    if not omit_CS_inds[b]:\n",
    "                        time_CS1[b, stim_inds] = 1\n",
    "                    # Set the US input times\n",
    "                    if i == 0 and not omit_US_inds[b]:\n",
    "                        time_US[b, stim_inds + stim_len] = 1\n",
    "                    # Set the target valence times\n",
    "                    if i > 0 and not omit_inds[b]:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            val_int[b, (stim_inds + 1)] = 1 / i\n",
    "                        else:\n",
    "                            val_int[b, (stim_inds + 1)] = -1 / i\n",
    "                # Set the inputs for second-order conditioning trials\n",
    "                else:\n",
    "                    # Set the CS1 input times\n",
    "                    if not omit_CS_inds[b]:\n",
    "                        if i == 0:\n",
    "                            time_CS1[b, stim_inds] = 1\n",
    "                        if i == 1:\n",
    "                            time_CS1[b, stim_inds + stim_len] = 1\n",
    "                            time_CS2[b, stim_inds] = 1\n",
    "                        if i == 2:\n",
    "                            time_CS2[b, stim_inds] = 1\n",
    "                    # Set the US input times\n",
    "                    if i == 0 and not omit_US_inds[b]:\n",
    "                        time_US[b, stim_inds + stim_len] = 1\n",
    "                    # Set the target valence times\n",
    "                    if i > 0 and not omit_inds[b]:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            val_int[b, (stim_inds + (i % 2) * stim_len + 1)] = 1\n",
    "                        else:\n",
    "                            val_int[b, (stim_inds + (i % 2) * stim_len + 1)] = -1\n",
    "        else:\n",
    "            for b in range(n_batch):            \n",
    "                # Set the inputs for extinction plots\n",
    "                if plot == 'extinct':\n",
    "                    # Set the CS input times\n",
    "                    stim_inds = stim_times[b, i] + torch.arange(stim_len)\n",
    "                    time_CS1[b, stim_inds] = 1\n",
    "                    # Set the US input times\n",
    "                    if i == 0:\n",
    "                        stim_inds += stim_len\n",
    "                        time_US[b, stim_inds] = 1\n",
    "                    # Set the target valence times\n",
    "                    if i > 0:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            val_int[b, (stim_inds + 1)] = 1 / i\n",
    "                        else:\n",
    "                            val_int[b, (stim_inds + 1)] = -1 / i\n",
    "                # Set the inputs for second-order conditioning plots\n",
    "                elif plot == 'CS2':\n",
    "                    stim_inds1 = stim_times[b, i] + torch.arange(stim_len)\n",
    "                    stim_inds2 = stim_times[b, i] + torch.arange(stim_len) + stim_len\n",
    "                    # Set the CS1, CS2 and US input times\n",
    "                    if i == 0:\n",
    "                        time_CS1[b, stim_inds1] = 1\n",
    "                        time_US[b, stim_inds2] = 1\n",
    "                    elif i == 1:\n",
    "                        time_CS1[b, stim_inds2] = 1\n",
    "                        time_CS2[b, stim_inds1] = 1\n",
    "                    elif i == 2:\n",
    "                        time_CS2[b, stim_inds2] = 1\n",
    "                    # Set the target valence times\n",
    "                    if i > 0:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            val_int[b, (stim_inds2 + 1)] = 1\n",
    "                        else:\n",
    "                            val_int[b, (stim_inds2 + 1)] = -1\n",
    "\n",
    "        # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "        r_KCt = torch.einsum('bm, mbt -> bmt', r_KC1, time_CS1.repeat(n_KC, 1, 1))\n",
    "        r_KCt += torch.einsum('bm, mbt -> bmt', r_KC2, time_CS2.repeat(n_KC, 1, 1))\n",
    "        r_extt = torch.einsum('bm, mbt -> bmt', r_ext, time_US.repeat(n_ext, 1, 1))\n",
    "            \n",
    "        r_KCt_ls.append(r_KCt)\n",
    "        r_extt_ls.append(r_extt)\n",
    "        vals.append(val_int)\n",
    "        ls_CS1 += time_CS1\n",
    "        ls_CS2 += time_CS2\n",
    "        ls_US += time_US\n",
    "\n",
    "    # Concatenate target valences\n",
    "    vt_opt = torch.cat((vals[0], vals[1], vals[2]), dim=-1)\n",
    "        \n",
    "    # Make a list of stimulus times to plot\n",
    "    ls_stims = [torch.cat(ls_CS1), torch.cat(ls_US), torch.cat(ls_CS2)]\n",
    "        \n",
    "    return r_KCt_ls, r_extt_ls, vt_opt, ls_stims\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trial(network, task1:str, task2:str, dt=0.5):\n",
    "    \"\"\" Plots a figure similar to Figure 2 from Jiang 2020.\n",
    "    \n",
    "    Runs the network using a novel combination of conditioned and unconditioned stimuli,\n",
    "    then prints the results. Top: time series of the various stimuli (CS and US), as well as\n",
    "    the target valence and readout. Bottom: activity of eight randomly chosen mushroom body\n",
    "    output neurons (MBONs).\n",
    "    \n",
    "    Paramters\n",
    "        network = previously trained RNN\n",
    "        task1 = the type of task to be plotted ('first-order' or 'all_tasks')\n",
    "        task2 = the sub-category of task to be plotted ('CS+'/'CS-' or 'extinct'/'CS2')\n",
    "        dt = time step of the simulation/plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the labels for the plots\n",
    "    if task1 == 'first-order':\n",
    "        if task2 == 'CS+':\n",
    "            task_title = 'First-Order Conditioning (CS+)'\n",
    "            task_label = 'CS+'\n",
    "        elif task2 == 'CS-':\n",
    "            task_title = 'First-Order Conditioning (CS-)'\n",
    "            task_label = 'CS-'\n",
    "    elif task1 == 'all_tasks':\n",
    "        if task2 == 'extinct':\n",
    "            task_title = 'Extinction Conditioning'\n",
    "            task_label = 'CS'\n",
    "        if task2 == 'CS2':\n",
    "            task_title = 'Second-Order Conditioning'\n",
    "            task_label = 'CS1'\n",
    "    \n",
    "    # Define plot font sizes\n",
    "    label_font = 18\n",
    "    title_font = 24\n",
    "    legend_font = 12\n",
    "\n",
    "    # Run the network\n",
    "    r_out, Wt, vt, vt_opt, loss_hist, stim_ls = train_net(network, task=task1, dt=dt, n_epochs=1, n_batch=1, train=False, plot=task2)\n",
    "    r_out = r_out.detach().numpy().squeeze()\n",
    "    vt = vt.detach().numpy().squeeze()\n",
    "    vt_opt = vt_opt.detach().numpy().squeeze()\n",
    "    plot_CS = stim_ls[0].numpy().squeeze()\n",
    "    plot_US = stim_ls[1].numpy().squeeze()\n",
    "    plot_time = np.arange(plot_CS.size) * dt\n",
    "\n",
    "    # Plot the conditioning and test\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True, gridspec_kw={'height_ratios': [1, 4]})\n",
    "    ax1.plot(plot_time, vt, label='Readout')\n",
    "    ax1.plot(plot_time, vt_opt, label='Target')\n",
    "    ax1.plot(plot_time, plot_CS, label=task_label)\n",
    "    # Second-order conditioning involves an additional stimulus time series\n",
    "    if task2 == 'CS2':\n",
    "        plot_CS2 = stim_ls[2].numpy().squeeze()\n",
    "        ax1.plot(plot_time, plot_CS2, label='CS2')\n",
    "    ax1.plot(plot_time, plot_US, label='US')\n",
    "    ax1.set_ylabel('Value', fontsize=label_font)\n",
    "    ax1.set_title(task_title, fontsize=title_font)\n",
    "    ax1.legend(fontsize=legend_font)\n",
    "\n",
    "    # Plot the activities of a few MBONs\n",
    "    plot_neurs = np.random.choice(network.N_MBON, size=8, replace=False)\n",
    "    r_max = np.max(r_out)\n",
    "    for i, n in enumerate(plot_neurs):\n",
    "        ax2.plot(plot_time, (r_out[n, :] / r_max) + (i * 2 / 3), '-k')\n",
    "    ax2.set_xlabel('Time', fontsize=label_font)\n",
    "    ax2.set_ylabel('Normalized Activity', fontsize=label_font)\n",
    "    ax2.set_yticks([])\n",
    "    fig.tight_layout();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the network\n",
    "# classic_net = Drosophila_RNN(MBON_size=n_MBON**(i+1))\n",
    "# for param in classic_net.parameters():\n",
    "#     print(param.shape)\n",
    "# #     print(param)\n",
    "# # print(classic_net.N_DAN)\n",
    "\n",
    "# # Define the model's optimizer\n",
    "# lr = 0.001\n",
    "# optimizer = optim.RMSprop(classic_net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set task and training boolean\n",
    "train_task = 'first-order'\n",
    "# train_task = 'all_tasks'\n",
    "train_bool = False\n",
    "\n",
    "# Set parameters for sensitivity training\n",
    "test_num = 1\n",
    "if test_num == 1:\n",
    "    n_vals = 5\n",
    "    n_MBON_0 = 2\n",
    "elif test_num == 2:\n",
    "    n_vals = 9\n",
    "    n_MBON_0 = 2\n",
    "elif test_num == 3:\n",
    "    n_vals = 7\n",
    "    n_MBON_0 = 2\n",
    "\n",
    "n_epochs_train = 5000\n",
    "header = ''\n",
    "loss_arr = torch.zeros(n_vals, n_epochs_train)\n",
    "    \n",
    "\n",
    "if train_bool:\n",
    "    n_MBON = n_MBON_0 - 1\n",
    "    for i in range(n_vals):\n",
    "        if test_num == 1:\n",
    "            n_MBON = n_MBON_0**(i+1)\n",
    "        elif (test_num == 2) or (test_num == 3):\n",
    "            n_MBON += 1\n",
    "        # Initialize the network\n",
    "        classic_net = Drosophila_RNN(MBON_size=n_MBON, DAN_size=n_MBON)\n",
    "        # Define the model's optimizer\n",
    "        lr = 0.001\n",
    "        optimizer = optim.RMSprop(classic_net.parameters(), lr=lr)\n",
    "        # Run the network\n",
    "        print('Training network with {} MBONs'.format(n_MBON))\n",
    "        r_out, Wt, vt, vt_opt, loss_hist, _ = train_net(classic_net, task=train_task, n_epochs=n_epochs_train)\n",
    "        loss_arr[i, :] = torch.Tensor(loss_hist)\n",
    "        torch.save(classic_net.state_dict(), 'MBON_sensitivity/first-order_sensitivity_T{}_{:02d}MBONs.pt'.format(test_num, n_MBON))\n",
    "\n",
    "    # Save the loss data to csv format\n",
    "    np.savetxt(\"MBON_sensitivity/first-order_sensitivity_T{}_train_losses.csv\".format(test_num), loss_arr.T,\n",
    "               delimiter=\",\", header=header)\n",
    "\n",
    "#     # Plot the loss function\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "#     axes[0].plot(loss_hist)\n",
    "#     axes[0].set_xlabel('Epoch', fontsize=label_font)\n",
    "#     axes[0].set_ylabel('Loss', fontsize=label_font)\n",
    "#     axes[1].plot(loss_hist[2:])\n",
    "#     axes[1].set_xlabel('Epoch', fontsize=label_font)\n",
    "#     axes[1].set_ylabel('Loss', fontsize=label_font)\n",
    "#     fig.tight_layout();\n",
    "\n",
    "# https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.017486335709691048\n",
      "0 0.021857919171452522\n",
      "0 0.0022018153686076403\n",
      "0 0.0016168624861165881\n",
      "0 0.0007333551184274256\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot some results to compare to Fig 2A/B in paper\n",
    "n_epochs_test = 50\n",
    "loss_arr = np.zeros((n_vals, n_epochs_test))\n",
    "n_MBON_vec = np.zeros(n_vals)\n",
    "n_MBON = n_MBON_0 - 1\n",
    "header = ''\n",
    "\n",
    "for i in range(n_vals):\n",
    "    if test_num == 1:\n",
    "        n_MBON = n_MBON_0**(i+1)\n",
    "    elif (test_num == 2) or (test_num == 3):\n",
    "        n_MBON += 1\n",
    "    n_MBON_vec[i] = n_MBON\n",
    "    header += '{} MBONs, '.format(n_MBON)\n",
    "    # Initialize the network\n",
    "    classic_net = Drosophila_RNN(MBON_size=n_MBON, DAN_size=n_MBON)\n",
    "    # Load the network\n",
    "    classic_net.load_state_dict(torch.load('MBON_sensitivity/first-order_sensitivity_T{}_{:02d}MBONs.pt'.format(test_num, n_MBON)))\n",
    "    classic_net.eval()\n",
    "    # Run the network\n",
    "    r_out, Wt, vt, vt_opt, loss_hist, _ = train_net(classic_net, task=train_task, n_epochs=n_epochs_test, train=False)\n",
    "    loss_arr[i, :] = torch.Tensor(loss_hist).detach().numpy()\n",
    "\n",
    "# Save the loss data to csv format\n",
    "np.savetxt(\"MBON_sensitivity/first-order_sensitivity_T{}_test_losses.csv\".format(test_num), loss_arr.T,\n",
    "           delimiter=\",\", header=header)\n",
    "\n",
    "# Calculate statistics on losses\n",
    "loss_avg = np.mean(loss_arr, axis=1)\n",
    "loss_std = np.std(loss_arr, axis=1)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.errorbar(n_MBON_vec, loss_avg, loss_std, None, '-o')\n",
    "ax.set_xlabel('Number of MBONs', fontsize=label_font)\n",
    "ax.set_ylabel('Average Loss', fontsize=label_font)\n",
    "ax.set_title('MBON Sensitivity (1st-order Conditioning)', fontsize=title_font);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
