{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstOrderCondRNN(nn.Module):\n",
    "    def __init__(self, *, n_kc=200, n_mbon=20, n_fbn=60, n_ext=2, n_out=1,\n",
    "                 f_ones=0.1, n_seed=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set the seeds\n",
    "        if n_seed is not None:\n",
    "            np.random.seed(n_seed)\n",
    "            torch.manual_seed(n_seed)\n",
    "\n",
    "        # Set constants\n",
    "        W_kc_mbon_max = 0.05\n",
    "        self.kc_mbon_min = 0.  # Minimum synaptic weight\n",
    "        self.kc_mbon_max = W_kc_mbon_max  # Maximum synaptic weight\n",
    "        self.W_kc_mbon_0 = Variable(torch.ones((n_mbon, n_kc)) * W_kc_mbon_max,\n",
    "                                    requires_grad=False)\n",
    "        self.tau_w = 5  # Time scale of KC->MBON LTD/LTP (plasticity)\n",
    "        self.tau_r = 1  # Time scale of output circuitry activity\n",
    "        self.n_int = 2  # Number of task intervals\n",
    "\n",
    "        # Set the sizes of layers\n",
    "        n_dan = n_mbon\n",
    "        self.n_kc = n_kc\n",
    "        self.n_mbon = n_mbon\n",
    "        self.n_fbn = n_fbn\n",
    "        self.n_dan = n_dan\n",
    "        self.n_recur = n_mbon + n_fbn + n_dan\n",
    "        self.n_ext = n_ext\n",
    "        self.n_out = n_out\n",
    "        self.n_ones = int(n_kc * f_ones)\n",
    "\n",
    "        # Define network variables used to store data\n",
    "        # Odors\n",
    "        self.train_odors = None\n",
    "        self.eval_odors = None\n",
    "        # Training parameters (for continuation)\n",
    "        self.train_rts = None\n",
    "        self.train_Wts = None\n",
    "        self.train_wts = None\n",
    "        self.train_vts = None\n",
    "        self.train_vt_opts = None\n",
    "        self.train_CS_stim = None\n",
    "        self.train_US_stim = None\n",
    "        self.train_loss = None\n",
    "        # Evaluation parameters (for plotting and analysis)\n",
    "        self.eval_rts = None\n",
    "        self.eval_Wts = None\n",
    "        self.eval_wts = None\n",
    "        self.eval_vts = None\n",
    "        self.eval_vt_opts = None\n",
    "        self.eval_CS_stim = None\n",
    "        self.eval_US_stim = None\n",
    "        self.eval_loss = None\n",
    "\n",
    "        # Define updatable network parameters\n",
    "        sqrt2 = torch.sqrt(torch.tensor(2, dtype=torch.float))\n",
    "        mean_mbon = torch.zeros((self.n_recur, n_mbon))\n",
    "        mean_fbn = torch.zeros((self.n_recur, n_fbn))\n",
    "        mean_dan = torch.zeros((self.n_recur, n_dan))\n",
    "        W_mbon = torch.normal(mean_mbon, torch.sqrt(1 / (sqrt2 * n_mbon)),\n",
    "                              generator=n_seed)\n",
    "        W_fbn = torch.normal(mean_fbn, torch.sqrt(1 / (sqrt2 * n_fbn)),\n",
    "                             generator=n_seed)\n",
    "        W_dan = torch.normal(mean_dan, torch.sqrt(1 / (sqrt2 * n_dan)),\n",
    "                             generator=n_seed)\n",
    "        self.W_recur = nn.Parameter(torch.cat((W_mbon, W_fbn, W_dan), dim=1),\n",
    "                                    requires_grad=True)\n",
    "        self.W_ext = nn.Parameter(torch.randn(n_fbn, n_ext),\n",
    "                                  requires_grad=True)\n",
    "        mean_readout = torch.zeros((n_out, n_mbon))\n",
    "        std_readout = 1 / torch.sqrt(torch.tensor(n_mbon, dtype=torch.float))\n",
    "        self.W_readout = nn.Parameter(torch.normal(mean_readout, std_readout,\n",
    "                                                   generator=n_seed),\n",
    "                                      requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones(self.n_recur) * 0.1,\n",
    "                                 requires_grad=True)\n",
    "\n",
    "    def forward(self, r_kc, r_ext, time, n_batch=30, W0=None, r0=None, **kwargs):\n",
    "        \"\"\" Defines the forward pass of the RNN\n",
    "\n",
    "        The KC->MBON weights are constrained to the range [0, 0.05].\n",
    "        MBONs receive external input from Kenyon cells (r_kc i.e. 'odors').\n",
    "        Feedback neurons (FBNs) receive external input (r_ext i.e. 'context').\n",
    "        DAN->MBON weights are permanently set to zero.\n",
    "        DANs receive no external input.\n",
    "\n",
    "        Parameters\n",
    "            r_kc = activity of the Kenyon cell inputs (representing odors)\n",
    "            r_ext = context inputs (representing the conditioning context)\n",
    "            time = time vector for a single interval\n",
    "            n_batch = number of trials in mini-batch\n",
    "            W0 = initial weights for KC->MBON connections\n",
    "            r0 = initial activities for output circuitry neurons\n",
    "\n",
    "        Returns\n",
    "            r_recur: list of torch.ndarray(batch_size, n_mbon + n_fbn + n_dan)\n",
    "                = time series of activities in the output circuitry\n",
    "            Wt: list of torch.ndarray(batch_size, n_recur, n_recur)\n",
    "                = time series of KC->MBON weights (dopaminergic plasticity)\n",
    "            readout: list of torch.ndarray(batch_size, 1)\n",
    "                = time series of valence readouts (behaviour)\n",
    "        \"\"\"\n",
    "\n",
    "        # Define the time step of the simulation\n",
    "        dt = np.diff(time)[0]\n",
    "\n",
    "        # Initialize output circuit firing rates for each trial\n",
    "        if r0 is not None:\n",
    "            r_init = r0\n",
    "        else:\n",
    "            r_init = torch.ones(n_batch, self.n_recur) * 0.1\n",
    "            r_init[:, :self.n_mbon] = 0\n",
    "        r_recur = [r_init]\n",
    "\n",
    "        # Initialize the eligibility traces and readout\n",
    "        r_bar_kc = r_kc[:, :, 0]\n",
    "        r_bar_dan = r_recur[-1][:, -self.n_dan:]\n",
    "        readout = [torch.einsum('bom, bm -> bo',\n",
    "                                self.W_readout.repeat(n_batch, 1, 1),\n",
    "                                r_recur[-1][:, :self.n_mbon]).squeeze()]\n",
    "\n",
    "        # Set the weights DAN->MBON to zero\n",
    "        W_recur = self.W_recur.clone()\n",
    "        W_recur[:self.n_mbon, -self.n_dan:] = 0\n",
    "\n",
    "        # Initialize the KC->MBON weights\n",
    "        W_kc_mbon = [W0[0]]\n",
    "        wt = [W0[1]]\n",
    "\n",
    "        # Update activity for each time step\n",
    "        for t in range(time.shape[0] - 1):\n",
    "            # Define the input to the output circuitry\n",
    "            I_kc_mbon = torch.einsum('bmk, bk -> bm',\n",
    "                                     W_kc_mbon[-1], r_kc[:, :, t])\n",
    "            I_fbn = torch.einsum('bfe, be -> bf',\n",
    "                                 self.W_ext.repeat(n_batch, 1, 1),\n",
    "                                 r_ext[:, :, t])\n",
    "            I_tot = torch.zeros((n_batch, self.n_recur))\n",
    "            I_tot[:, :self.n_mbon] = I_kc_mbon\n",
    "            I_tot[:, self.n_mbon:self.n_mbon + self.n_fbn] = I_fbn\n",
    "\n",
    "            # Update the output circuitry activity (see Eq. 1)\n",
    "            Wr_prod = torch.einsum('bsr, br -> bs',\n",
    "                                   W_recur.repeat(n_batch, 1, 1),\n",
    "                                   r_recur[-1])\n",
    "            dr = (-r_recur[-1] + F.relu(Wr_prod + self.bias.repeat(n_batch, 1)\n",
    "                                        + I_tot)) / self.tau_r\n",
    "            r_recur.append(r_recur[-1] + dr * dt)\n",
    "\n",
    "            # Update KC->MBON plasticity variables\n",
    "            wt_out = self.wt_update(W_kc_mbon, wt, dt, r_bar_kc, r_bar_dan,\n",
    "                                    r_kc[:, :, t], r_recur[-1][:, -self.n_dan:],\n",
    "                                    n_batch, **kwargs)\n",
    "            # W_kc_mbon, wt, r_bar_kc, r_bar_dan = out\n",
    "            r_bar_kc, r_bar_dan = wt_out\n",
    "\n",
    "            # Calculate the readout (see Eq. 2)\n",
    "            readout.append(torch.einsum('bom, bm -> bo',\n",
    "                                        self.W_readout.repeat(n_batch, 1, 1),\n",
    "                                        r_recur[-1][:, :self.n_mbon]).squeeze())\n",
    "\n",
    "        return r_recur, (W_kc_mbon, wt), readout\n",
    "\n",
    "    def wt_update(self, W_kc_mbon, wt, dt, r_bar_kc, r_bar_dan, r_kc, r_dan,\n",
    "                  n_batch, **kwargs):\n",
    "        \"\"\" Updates the KC->MBON plasticity variables\n",
    "\n",
    "        Synaptic weights from the Kenyon cells to the mushroom body output neurons\n",
    "        (MBONs) are updated dynamically. All other weights are network parameters.\n",
    "        The synaptic connections between Kenyon Cells (KCs) and MBONs are updated\n",
    "        using a LTP/LTD rule (see Figure 1B of Jiang 2020), which models dopamine-\n",
    "        gated neural plasticity on short time scale (behavioural learning).\n",
    "\n",
    "        Parameters\n",
    "            W_kc_mbon: list = KC->MBON weight matrices\n",
    "            wt = dynamic plasticity update\n",
    "            dt = time step of simulation\n",
    "            r_bar_kc = eligibility trace of Kenyon cell activity\n",
    "            r_bar_dan = eligibility trace of dopaminergic cell activity\n",
    "            r_kc = current activity of Kenyon cells\n",
    "            r_dan = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the eligibility traces (represent LTP/LTD)\n",
    "        r_bar_kc = r_bar_kc + (r_kc - r_bar_kc) * dt / self.tau_w\n",
    "        r_bar_dan = r_bar_dan + (r_dan - r_bar_dan) * dt / self.tau_w\n",
    "        # Update the dynamic weight variable\n",
    "        dw = self.calc_dw(r_bar_kc, r_bar_dan, r_kc, r_dan, n_batch, **kwargs)\n",
    "        wt.append(wt[-1] + dw * dt)\n",
    "        # Update the KC->MBON weights (see Eq. 8)\n",
    "        dW = (-W_kc_mbon[-1] + wt[-1]) / self.tau_w\n",
    "        W_tp1 = W_kc_mbon[-1] + dW * dt\n",
    "        # Clip the KC->MBON weights to the range [0, 0.05]\n",
    "        W_kc_mbon.append(torch.clamp(W_tp1, self.kc_mbon_min, self.kc_mbon_max))\n",
    "\n",
    "        # return W_kc_mbon, wt, r_bar_kc, r_bar_dan\n",
    "        return r_bar_kc, r_bar_dan\n",
    "\n",
    "    def calc_dw(self, r_bar_kc, r_bar_dan, r_kc, r_dan, n_batch, **kwargs):\n",
    "        \"\"\" Calculates the dynamic weight update (see Eq 4).\n",
    "\n",
    "        Parameters\n",
    "            r_bar_kc = eligibility trace of Kenyon cell activity\n",
    "            r_bar_dan = eligibility trace of dopaminergic cell activity\n",
    "            r_kc = current activity of Kenyon cells\n",
    "            r_dan = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "\n",
    "        Returns\n",
    "            update to dynamic plasticity variables wt\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the LTD/LTP terms\n",
    "        prod1 = torch.einsum('bd, bk -> bdk', r_bar_dan, r_kc)\n",
    "        prod2 = torch.einsum('bd, bk -> bdk', r_dan, r_bar_kc)\n",
    "\n",
    "        return prod1 - prod2\n",
    "\n",
    "    def train_net(self, *, opti, T_int=30, T_stim=2, dt=0.5, n_epoch=5000,\n",
    "                  n_batch=30, reset_wts=True, clip=0.001, **kwargs):\n",
    "        \"\"\" Trains a network on classical conditioning tasks.\n",
    "\n",
    "        Tasks include first-order or second-order conditioning, and extinction.\n",
    "        Tasks consist of two (first-order) or three (second-order and extinction)\n",
    "        intervals. Each task has its own input generating function. Stimuli are\n",
    "        presented between 5-15s of each interval. Neuron activities are reset\n",
    "        between intervals to prevent associations being represented through\n",
    "        persistent activity.\n",
    "\n",
    "        Parameters\n",
    "            opti = RNN network optimizer\n",
    "            T_int = length of task intervals\n",
    "            T_stim = length of time each stimulus is presented\n",
    "            dt = time step of simulations\n",
    "            n_epoch = number of epochs to train over\n",
    "            n_batch = number of trials in mini-batch\n",
    "            reset_wts = indicates whether to reset weights between trials\n",
    "            clip = maximum gradient allowed during training\n",
    "\n",
    "        Returns\n",
    "            r_out_epoch = output circuit neuron activities for final epoch\n",
    "            Wt_epoch = KC->MBON weights for final epoch\n",
    "            vt_epoch = readout (i.e. valence) for final epoch\n",
    "            vt_opt = target valence for final epoch\n",
    "            loss_hist = list of losses for all epochs\n",
    "            ls_stims = list of stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Interval time vector\n",
    "        time_int = torch.arange(0, T_int + dt / 10, dt)\n",
    "\n",
    "        # Generate a list of stimulus presentation times\n",
    "        stim_times = self.gen_stim_times(T_stim, T_int, dt, n_epoch, n_batch)\n",
    "        # Length of stimulus in indices\n",
    "        stim_len = int(T_stim / dt)\n",
    "\n",
    "        # List to store losses\n",
    "        loss_hist = []\n",
    "\n",
    "        # Initialize the KC-MBON weights\n",
    "        W_in = None\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            # Lists to store activities, weights, readouts and target valences\n",
    "            rts = []\n",
    "            vts = []\n",
    "\n",
    "            # Set the intial KC->MBON weight values for each trial\n",
    "            if reset_wts or (W_in is None):\n",
    "                W_in = self.init_w_kc_mbon(None, n_batch, (epoch, n_epoch))\n",
    "            else:\n",
    "                W_in = (W_in[0][-1].detach(), W_in[1][-1].detach())\n",
    "            # W_kc_mbon = self.init_w_kc_mbon(W_kc_mbon, n_batch, (epoch, n_epoch))\n",
    "\n",
    "            # Generate odor (r_kc), context (r_ext), and target valence (vt_opt)\n",
    "            st_epoch = stim_times[epoch]\n",
    "            net_inputs = self.gen_inputs(st_epoch, stim_len, time_int.shape[0],\n",
    "                                         n_batch, **kwargs)\n",
    "            r_kc, r_ext, vt_opt, ls_stims = net_inputs\n",
    "\n",
    "            # For each interval in the task\n",
    "            for i in range(self.n_int):\n",
    "                # Run the forward model\n",
    "                net_out = self(r_kc[i], r_ext[i], time_int, n_batch, W_in)\n",
    "                rt_int, (Wt_int, wt_int), vt_int = net_out\n",
    "                # Pass the KC->MBON weights to the next interval\n",
    "                W_in = (Wt_int[-1], wt_int[-1])\n",
    "\n",
    "                # Append the interval outputs to lists\n",
    "                rts += rt_int\n",
    "                vts += vt_int\n",
    "\n",
    "            # Concatenate the activities, weights and valences\n",
    "            rt_epoch = torch.stack(rts, dim=-1)\n",
    "            vt_epoch = torch.stack(vts, dim=-1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = cond_loss(vt_epoch, vt_opt, rt_epoch[:, -self.n_dan:, :])\n",
    "\n",
    "            # Update the network parameters\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), clip)\n",
    "            opti.step()\n",
    "\n",
    "            # Print an update\n",
    "            if epoch % 500 == 0:\n",
    "                print(epoch, loss.item())\n",
    "            loss_hist.append(loss.item())\n",
    "\n",
    "        return loss_hist\n",
    "\n",
    "    def run_eval(self, *, trial_ls, T_int=30, dt=0.5, n_batch=1, n_trial=1,\n",
    "                 reset_wts=True, **kwargs):\n",
    "        \"\"\" Runs an evaluation based on a series of input functions\n",
    "\n",
    "        Parameters\n",
    "            trial_ls = list of interval functions that compose a trial\n",
    "            T_int = length of a task interval (in seconds)\n",
    "            dt = time step of simulation (in seconds)\n",
    "            n_batch = number of parallel trials in a batch\n",
    "            n_trials = number of trials to run\n",
    "            reset_wts = indicates whether to reset weights between trials\n",
    "        \"\"\"\n",
    "\n",
    "        # Reset lists storing evaluation data\n",
    "        self.eval_rts = []\n",
    "        self.eval_Wts = []\n",
    "        self.eval_wts = []\n",
    "        self.eval_vts = []\n",
    "        self.eval_vt_opts = []\n",
    "        self.eval_CS_stim = []\n",
    "        self.eval_US_stim = []\n",
    "        self.eval_loss = []\n",
    "\n",
    "        # Interval time vector\n",
    "        time_int = torch.arange(0, T_int + dt / 10, dt)\n",
    "        t_len = time_int.shape[0]\n",
    "\n",
    "        # Initialize the KC-MBON weights and plasticity variable\n",
    "        W_in = None\n",
    "\n",
    "        # For each function in the list, run an interval\n",
    "        # All intervals together compose a single trial\n",
    "        for trial in range(n_trial):\n",
    "            # Lists to store activities, weights, readouts and target valences\n",
    "            rts = []\n",
    "            Wts = []\n",
    "            wts = []\n",
    "            vts = []\n",
    "            vt_opts = []\n",
    "            time_CS = []\n",
    "            time_US = []\n",
    "\n",
    "            # Determine whether to reset KC->MBON weights between trials\n",
    "            if reset_wts or (W_in is None):\n",
    "                W_in = self.init_w_kc_mbon(None, n_batch, (trial, n_trial))\n",
    "            else:\n",
    "                W_in = (W_in[0][-1].detach(), W_in[1][-1].detach())\n",
    "\n",
    "            for i in range(len(trial_ls)):\n",
    "                # Calculate the CS stimulus presentation times\n",
    "                st_times, st_len = gen_int_times(dt, n_batch, **kwargs)\n",
    "\n",
    "                # Generate odors and context (odor = KC = CS, context = ext = US)\n",
    "                r_in = self.gen_r_kc_ext(n_batch, **kwargs)\n",
    "                # r_kc, r_ext = self.gen_r_kc_ext(n_batch, **kwargs)\n",
    "                if not self.static_odors:\n",
    "                    # self.eval_odors.append(r_kc)\n",
    "                    self.eval_odors.append(r_in[0])\n",
    "                # r_in = (r_kc, r_ext)\n",
    "\n",
    "                # Select the interval function to run\n",
    "                int_fnc = trial_ls[i]\n",
    "                # Calculate the interval inputs\n",
    "                f_in = int_fnc(t_len, st_times, st_len, r_in, n_batch, **kwargs)\n",
    "                r_kct, r_extt, stim_ls, vt_opt = f_in\n",
    "\n",
    "                # Run the forward pass\n",
    "                net_out = self(r_kct, r_extt, time_int, n_batch, W_in)\n",
    "                rt_int, (Wt_int, wt_int), vt_int = net_out\n",
    "                # Pass the KC->MBON weights to the next interval\n",
    "                W_in = (Wt_int[-1], wt_int[-1])\n",
    "\n",
    "                # Append the interval outputs to lists\n",
    "                rts += rt_int\n",
    "                Wts += Wt_int\n",
    "                wts += wt_int\n",
    "                vts += vt_int\n",
    "                vt_opts += vt_opt\n",
    "                time_CS += stim_ls[0]\n",
    "                time_US += stim_ls[1]\n",
    "\n",
    "            # Concatenate the activities, weights and valences\n",
    "            self.eval_rts.append(torch.stack(rts, dim=-1).detach())\n",
    "            self.eval_Wts.append(torch.stack(Wts, dim=-1).detach())\n",
    "            self.eval_wts.append(torch.stack(wts, dim=-1).detach())\n",
    "            self.eval_vts.append(torch.stack(vts, dim=-1).detach())\n",
    "            self.eval_vt_opts.append(torch.stack(vts, dim=-1).detach())\n",
    "            self.eval_CS_stim.append(torch.stack(time_CS, dim=-1).detach())\n",
    "            self.eval_US_stim.append(torch.stack(time_US, dim=-1).detach())\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = cond_loss(self.eval_vts[-1], self.eval_vt_opts[-1],\n",
    "                             self.eval_rts[-1][:, -self.n_dan:, :])\n",
    "            self.eval_loss.append(loss.item())\n",
    "\n",
    "    def init_w_kc_mbon(self, W_in, n_batch, e_tup):\n",
    "        \"\"\" Initializes the KC->MBON weights for the task.\n",
    "\n",
    "        KC->MBON weights are reset at the beginning of each epoch.\n",
    "\n",
    "        Parameters\n",
    "            W_in = specified initial weight values or None\n",
    "            n_batch = number of trials in mini-batch\n",
    "            e_tup: tuple = (current epoch, total training epochs)\n",
    "\n",
    "        Returns\n",
    "            tuple of initial KC->MBON and dynamic plasticity variables\n",
    "        \"\"\"\n",
    "\n",
    "        if W_in is None:\n",
    "            wt0 = self.W_kc_mbon_0.repeat(n_batch, 1, 1)\n",
    "            W_in = (wt0.clone(), wt0.clone())\n",
    "\n",
    "        return W_in\n",
    "\n",
    "    def gen_stim_times(self, T_stim, T_int, dt, n_epoch, n_batch):\n",
    "        \"\"\" Generates an array of stimulus presentation times for all trials\n",
    "\n",
    "        Parameters\n",
    "            T_stim = length of time each stimulus is presented\n",
    "            T_int = length of task intervals\n",
    "            dt = time step of simulations\n",
    "            n_epoch = number of epochs to train over\n",
    "            n_batch = number of trials in mini-batch\n",
    "\n",
    "        Returns\n",
    "            Array of stimulus presentation times\n",
    "        \"\"\"\n",
    "\n",
    "        # Present the stimulus between 5-15s of each interval\n",
    "        stim_min = 5\n",
    "        stim_max = 15 - T_stim\n",
    "        stim_range = int((stim_max - stim_min) / dt)\n",
    "        stim_wts = torch.ones(n_epoch, stim_range)\n",
    "        stim_offset = int(stim_min / dt)\n",
    "\n",
    "        # Initialize stimulus presentation times array\n",
    "        stim_times = torch.zeros(n_epoch, n_batch, self.n_int)\n",
    "\n",
    "        for i in range(self.n_int):\n",
    "            # Randomly determine the time of each stimulus presentation\n",
    "            stim_times[:, :, i] = torch.multinomial(stim_wts, n_batch,\n",
    "                                                    replacement=True)\n",
    "            stim_times[:, :, i] += stim_offset\n",
    "\n",
    "        return stim_times\n",
    "\n",
    "    def gen_inputs(self, stim_times, stim_len, time_len, n_batch, p_omit=0.3):\n",
    "        \"\"\" Generates inputs for first-order conditioning tasks.\n",
    "\n",
    "        All trials are either CS+, CS- (US omitted) or CS omitted (control trials\n",
    "        to avoid over-fitting). Of the trials where CS or US is omitted, a second\n",
    "        parameter determines the relative fractions of CS or US trials omitted\n",
    "        (p_omit_CS). See Fig. 2 of Jiang 2020 to determine sequencing of stimuli\n",
    "        during training. To account for the sequential nature of numerical\n",
    "        simulations, the target valence begins one time step after stimulus\n",
    "        onset. Details provided in Jiang 2020 -> Methods -> Conditioning Tasks.\n",
    "\n",
    "        The mix of conditions is listed as follows:\n",
    "            probability of CS+ trials = 1 - p_omit\n",
    "            probability of CS- trials = p_omit * 0.3\n",
    "            probability of control trials = p_omit * 0.7\n",
    "\n",
    "        Parameters\n",
    "            stim_times = indices of stimulus presentations for each interval\n",
    "            stim_len = length of stimulus presentation (in indices)\n",
    "            time_len = size of time vector\n",
    "            n_batch = number of trials in mini-batch\n",
    "            p_omit = probability of omitting either CS or US from trials\n",
    "\n",
    "        Returns\n",
    "            r_kct_ls = odor (KC) input time series arrays for each interval\n",
    "            r_extt_ls = context (ext) input time series arrays for each interval\n",
    "            vt_opt = target valence for plotting and loss calculations\n",
    "            ls_stims = stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Set stimulus presentation time dtype\n",
    "        stim_times = stim_times.int()\n",
    "\n",
    "        # Generate odors and context signals for each trial\n",
    "        r_kc, r_ext = self.gen_r_kc_ext(n_batch)\n",
    "\n",
    "        # Determine whether CS or US are randomly omitted\n",
    "        omit_inds = torch.rand(n_batch) < p_omit\n",
    "        # If omitted, determine which one is omitted\n",
    "        p_omit_CS = 0.7\n",
    "        x_omit_CS = torch.rand(n_batch)\n",
    "        omit_CS_inds = torch.logical_and(omit_inds, x_omit_CS < p_omit_CS)\n",
    "        omit_US_inds = torch.logical_and(omit_inds, x_omit_CS > p_omit_CS)\n",
    "\n",
    "        # Initialize lists to store inputs, target valence and stimulus times\n",
    "        r_kct_ls = []\n",
    "        r_extt_ls = []\n",
    "        vals = []\n",
    "        ls_CS = []\n",
    "        ls_US = []\n",
    "\n",
    "        # For each interval\n",
    "        for i in range(self.n_int):\n",
    "            # Initialize time matrices\n",
    "            time_CS = torch.zeros(n_batch, time_len)\n",
    "            time_US = torch.zeros_like(time_CS)\n",
    "            val_int = torch.zeros_like(time_CS)\n",
    "\n",
    "            for b in range(n_batch):\n",
    "                stim_inds = stim_times[b, i] + torch.arange(stim_len)\n",
    "                # Set the CS input times\n",
    "                if not omit_CS_inds[b]:\n",
    "                    time_CS[b, stim_inds] = 1\n",
    "                # Set the US input times\n",
    "                if i == 0 and not omit_US_inds[b]:\n",
    "                    time_US[b, (stim_inds + stim_len)] = 1\n",
    "                # Set the target valence times\n",
    "                if i == 1 and not omit_inds[b]:\n",
    "                    if r_ext[b, 0] == 1:\n",
    "                        val_int[b, (stim_inds + 1)] = 1\n",
    "                    else:\n",
    "                        val_int[b, (stim_inds + 1)] = -1\n",
    "\n",
    "            # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "            r_kct = torch.einsum('bm, mbt -> bmt', r_kc,\n",
    "                                 time_CS.repeat(self.n_kc, 1, 1))\n",
    "            r_extt = torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                                  time_US.repeat(self.n_ext, 1, 1))\n",
    "\n",
    "            r_kct_ls.append(r_kct)\n",
    "            r_extt_ls.append(r_extt)\n",
    "            vals.append(val_int)\n",
    "            ls_CS += time_CS\n",
    "            ls_US += time_US\n",
    "\n",
    "        # Concatenate target valences\n",
    "        vt_opt = torch.cat((vals[0], vals[1]), dim=-1)\n",
    "\n",
    "        # Make a list of stimulus times to plot\n",
    "        ls_stims = [torch.cat(ls_CS), torch.cat(ls_US)]\n",
    "\n",
    "        return r_kct_ls, r_extt_ls, vt_opt, ls_stims\n",
    "\n",
    "    def gen_r_kc_ext(self, n_batch, pos_val=None, **kwargs):\n",
    "        \"\"\" Generates neuron activations for context and odor inputs.\n",
    "\n",
    "        Parameters\n",
    "            n_batch = number of trials in eval-batch\n",
    "            pos_vt (kwarg) = indicates whether valence should be positive\n",
    "                             None: random valence\n",
    "                             True: positive valence\n",
    "                             False: negative valence\n",
    "        \"\"\"\n",
    "\n",
    "        # Determine the contextual input (r_ext)\n",
    "        if pos_val is None:\n",
    "            r_ext = torch.multinomial(torch.ones(n_batch, self.n_ext),\n",
    "                                      self.n_ext)\n",
    "        elif pos_val:\n",
    "            r_ext = torch.tensor([1, 0]).repeat(n_batch, 1)\n",
    "        elif not pos_val:\n",
    "            r_ext = torch.tensor([0, 1]).repeat(n_batch, 1)\n",
    "        else:\n",
    "            raise Exception('Not a valid value for pos_val')\n",
    "\n",
    "        # Determine odor input (r_kc)\n",
    "        r_kc = torch.zeros(n_batch, self.n_kc)\n",
    "        for b in range(n_batch):\n",
    "            # Define an odor (CS) for each trial\n",
    "            if self.train_odors is not None:\n",
    "                n_odors = self.train_odors.shape[0]\n",
    "                odor_select = torch.randint(n_odors, (1,))\n",
    "                r_kc_inds = self.train_odors[odor_select, :]\n",
    "            else:\n",
    "                r_kc_inds = torch.multinomial(torch.ones(self.n_kc), self.n_ones)\n",
    "            r_kc[b, r_kc_inds] = 1\n",
    "\n",
    "        return r_kc, r_ext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define plot font sizes\n",
    "label_font = 18\n",
    "title_font = 24\n",
    "legend_font = 12\n",
    "\n",
    "def cond_loss(vt, vt_opt, r_DAN, lam=0.1):\n",
    "    \"\"\" Calculates the loss for conditioning tasks.\n",
    "\n",
    "    Composed of an MSE cost based on the difference between output and\n",
    "    target valence, and a regularization cost that penalizes excess\n",
    "    dopaminergic activity. Reference Eqs. (3) and (9) in Jiang 2020.\n",
    "\n",
    "    Parameters\n",
    "        vt = time dependent valence output of network\n",
    "        vt_opt = target valence (must be a torch tensor)\n",
    "        r_DAN = time series of dopaminergic neuron activities\n",
    "        lam = regularization constant\n",
    "\n",
    "    Returns\n",
    "        loss_tot = scalar loss used in backprop\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the baseline DAN activity\n",
    "    DAN_baseline = 0.1\n",
    "\n",
    "    # Calculate the MSE loss of the valence\n",
    "    v_sum = torch.mean((vt - vt_opt) ** 2, dim=1)\n",
    "    v_loss = torch.mean(v_sum)\n",
    "\n",
    "    # Calculate regularization term\n",
    "    r_sum = torch.sum(F.relu(r_DAN - DAN_baseline) ** 2, dim=1)\n",
    "    r_loss = torch.mean(r_sum, dim=1) * lam\n",
    "\n",
    "    # Calculate the summed loss (size = n_batch)\n",
    "    loss = v_loss + r_loss\n",
    "\n",
    "    # Average the loss over all batches\n",
    "    loss_tot = torch.mean(loss)\n",
    "\n",
    "    return loss_tot\n",
    "\n",
    "def print_trial(network, optimizer, p_omit, task2:str, dt=0.5):\n",
    "    \"\"\" Plots a figure similar to Figure 2 from Jiang 2020.\n",
    "    \n",
    "    Runs the network using a novel combination of conditioned and unconditioned stimuli,\n",
    "    then prints the results. Top: time series of the various stimuli (CS and US), as well as\n",
    "    the target valence and readout. Bottom: activity of eight randomly chosen mushroom body\n",
    "    output neurons (MBONs).\n",
    "    \n",
    "    Paramters\n",
    "        network = previously trained RNN\n",
    "        task1 = the type of task to be plotted ('first-order' or 'all_tasks')\n",
    "        task2 = the sub-category of task to be plotted ('CS+'/'CS-' or 'extinct'/'CS2')\n",
    "        dt = time step of the simulation/plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the labels for the plots\n",
    "    if network.n_int == 2:\n",
    "        task1 = 'first-order'\n",
    "        if task2 == 'CS+':\n",
    "            task_title = 'First-Order Conditioning (CS+)'\n",
    "            task_label = 'CS+'\n",
    "        elif task2 == 'CS-':\n",
    "            task_title = 'First-Order Conditioning (CS-)'\n",
    "            task_label = 'CS-'\n",
    "    elif network.n_int == 3:\n",
    "        task1 = 'all_tasks'\n",
    "        if task2 == 'extinct':\n",
    "            task_title = 'Extinction Conditioning'\n",
    "            task_label = 'CS'\n",
    "        if task2 == 'CS2':\n",
    "            task_title = 'Second-Order Conditioning'\n",
    "            task_label = 'CS1'\n",
    "    \n",
    "    # Define plot font sizes\n",
    "    label_font = 18\n",
    "    title_font = 24\n",
    "    legend_font = 12\n",
    "\n",
    "    # Run the network\n",
    "    r_out, vt, vt_opt, loss_hist, stim_ls = network.train_net(opti=optimizer, n_epoch=1, n_batch=1, p_omit=p_omit)\n",
    "    r_out = r_out.detach().numpy().squeeze()\n",
    "    vt = vt.detach().numpy().squeeze()\n",
    "    vt_opt = vt_opt.detach().numpy().squeeze()\n",
    "    plot_CS = stim_ls[0].numpy().squeeze()\n",
    "    plot_US = stim_ls[1].numpy().squeeze()\n",
    "    plot_time = np.arange(plot_CS.size) * dt\n",
    "\n",
    "    # Plot the conditioning and test\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True, gridspec_kw={'height_ratios': [1, 4]})\n",
    "    ax1.plot(plot_time, vt, label='Readout')\n",
    "    ax1.plot(plot_time, vt_opt, label='Target')\n",
    "    ax1.plot(plot_time, plot_CS, label=task_label)\n",
    "    # Second-order conditioning involves an additional stimulus time series\n",
    "    if task2 == 'CS2':\n",
    "        plot_CS2 = stim_ls[2].numpy().squeeze()\n",
    "        ax1.plot(plot_time, plot_CS2, label='CS2')\n",
    "    ax1.plot(plot_time, plot_US, label='US')\n",
    "    ax1.set_ylabel('Value', fontsize=label_font)\n",
    "    ax1.set_title(task_title, fontsize=title_font)\n",
    "    ax1.legend(fontsize=legend_font)\n",
    "\n",
    "    # Plot the activities of a few MBONs\n",
    "    plot_neurs = np.random.choice(network.n_mbon, size=8, replace=False)\n",
    "    r_max = np.max(r_out)\n",
    "    for i, n in enumerate(plot_neurs):\n",
    "        ax2.plot(plot_time, (r_out[n, :] / r_max) + (i * 2 / 3), '-k')\n",
    "    ax2.set_xlabel('Time', fontsize=label_font)\n",
    "    ax2.set_ylabel('Normalized Activity', fontsize=label_font)\n",
    "    ax2.set_yticks([])\n",
    "    fig.tight_layout();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([60, 2])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the network\n",
    "classic_net = FirstOrderCondRNN()\n",
    "for param in classic_net.parameters():\n",
    "    print(param.shape)\n",
    "#     print(param)\n",
    "# print(classic_net.N_DAN)\n",
    "\n",
    "# Define the model's optimizer\n",
    "lr = 0.001\n",
    "optimizer = optim.RMSprop(classic_net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.051025390625\n",
      "500 0.0004451084532774985\n",
      "1000 0.00012502280878834426\n"
     ]
    }
   ],
   "source": [
    "train_bool = True\n",
    "if train_bool:\n",
    "    loss_hist = classic_net.train_net(opti=optimizer, n_epoch=1001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_bool:\n",
    "    # Plot the loss function\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "    axes[0].plot(loss_hist)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=label_font)\n",
    "    axes[0].set_ylabel('Loss', fontsize=label_font)\n",
    "    axes[1].plot(loss_hist[5:])\n",
    "    axes[1].set_xlabel('Epoch', fontsize=label_font)\n",
    "    axes[1].set_ylabel('Loss', fontsize=label_font)\n",
    "    fig.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.00015191425336524844\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7618ccabf198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassic_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_omit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CS+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-9a09b1071bd0>\u001b[0m in \u001b[0;36mprint_trial\u001b[0;34m(network, optimizer, p_omit, task2, dt)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Run the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstim_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopti\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_omit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_omit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mr_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 1)"
     ]
    }
   ],
   "source": [
    "print_trial(classic_net, optimizer, p_omit=0, task2='CS+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_out, vt, vt_opt, loss_hist, _ = classic_net.train_net(opti=optimizer, n_epoch=200)\n",
    "print(np.mean(loss_hist), np.std(loss_hist))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
