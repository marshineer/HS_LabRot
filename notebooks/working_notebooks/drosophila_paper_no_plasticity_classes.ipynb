{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "# from numpy.linalg import svd as svd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define plot font sizes\n",
    "label_font = 18\n",
    "title_font = 24\n",
    "legend_font = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model Class - Conditioning Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstOrderCondRNN(nn.Module):\n",
    "    def __init__(self, *, n_kc=200, n_mbon=20, n_fbn=60, n_ext=2, n_out=1,\n",
    "                 T_int=30, T_stim=2, dt=0.5, f_ones=0.1, n_hop=0, n_seed=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set constants\n",
    "        W_kc_mbon_max = 0.05\n",
    "        self.kc_mbon_min = 0.  # Minimum synaptic weight\n",
    "        self.kc_mbon_max = W_kc_mbon_max  # Maximum synaptic weight\n",
    "        self.W_kc_mbon_0 = Variable(torch.ones((n_mbon, n_kc)) * W_kc_mbon_max,\n",
    "                                    requires_grad=False)\n",
    "        self.tau_w = 5  # Time scale of KC->MBON LTD/LTP (plasticity)\n",
    "        self.tau_r = 1  # Time scale of output circuitry activity\n",
    "        self.n_int = 2  # Number of task intervals\n",
    "        self.T_int = T_int  # Length of task interval [seconds]\n",
    "        self.T_stim = T_stim  # Length of stimulus presentation [seconds]\n",
    "        self.dt = dt  # Length of simulation time step [seconds]\n",
    "        self.n_hop = n_hop\n",
    "\n",
    "        # Set the sizes of layers\n",
    "        n_dan = n_mbon\n",
    "        self.n_kc = n_kc\n",
    "        self.n_mbon = n_mbon\n",
    "        self.n_fbn = n_fbn\n",
    "        self.n_dan = n_dan\n",
    "        self.n_recur = n_mbon + n_fbn + n_dan\n",
    "        self.n_ext = n_ext\n",
    "        self.n_out = n_out\n",
    "        self.n_ones = int(n_kc * f_ones)\n",
    "\n",
    "        # Define network variables used to store data\n",
    "        # Odors\n",
    "        self.train_odors = None\n",
    "        self.eval_odors = None\n",
    "        # Training parameters (for continuation)\n",
    "        self.train_T_vars = None\n",
    "        self.train_rts = None\n",
    "        self.train_Wts = None\n",
    "        self.train_wts = None\n",
    "        self.train_vts = None\n",
    "        self.train_vt_opts = None\n",
    "        self.train_CS_stim = None\n",
    "        self.train_US_stim = None\n",
    "        self.train_loss = None\n",
    "        # Evaluation parameters (for plotting and analysis)\n",
    "        self.eval_rts = None\n",
    "        self.eval_Wts = None\n",
    "        self.eval_wts = None\n",
    "        self.eval_vts = None\n",
    "        self.eval_vt_opts = None\n",
    "        self.eval_CS_stim = None\n",
    "        self.eval_US_stim = None\n",
    "        self.eval_loss = None\n",
    "        self.eval_err = None\n",
    "\n",
    "        # Set the seed\n",
    "        if n_seed is None:\n",
    "            # self.gen = n_seed\n",
    "            gen = n_seed\n",
    "        else:\n",
    "            np.random.seed(n_seed)\n",
    "            torch.manual_seed(n_seed)\n",
    "            gen = torch.Generator()\n",
    "            gen = gen.manual_seed(n_seed)\n",
    "            # self.gen = gen\n",
    "\n",
    "        # Define updatable network parameters\n",
    "        sqrt2 = torch.sqrt(torch.tensor(2, dtype=torch.float))\n",
    "        mean_mbon = torch.zeros((self.n_recur, n_mbon))\n",
    "        mean_fbn = torch.zeros((self.n_recur, n_fbn))\n",
    "        mean_dan = torch.zeros((self.n_recur, n_dan))\n",
    "        W_mbon = torch.normal(mean_mbon, torch.sqrt(1 / (sqrt2 * n_mbon)),\n",
    "                              generator=gen)\n",
    "        W_fbn = torch.normal(mean_fbn, torch.sqrt(1 / (sqrt2 * n_fbn)),\n",
    "                             generator=gen)\n",
    "        W_dan = torch.normal(mean_dan, torch.sqrt(1 / (sqrt2 * n_dan)),\n",
    "                             generator=gen)\n",
    "        self.W_recur = nn.Parameter(torch.cat((W_mbon, W_fbn, W_dan), dim=1),\n",
    "                                    requires_grad=True)\n",
    "        mean_readout = torch.zeros((n_out, n_mbon))\n",
    "        std_readout = 1 / torch.sqrt(torch.tensor(n_mbon, dtype=torch.float))\n",
    "        self.W_readout = nn.Parameter(torch.normal(mean_readout, std_readout,\n",
    "                                                   generator=gen),\n",
    "                                      requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones(self.n_recur) * 0.1,\n",
    "                                 requires_grad=True)\n",
    "        # Project the context (ext) signals to the DANs if FBNs are removed\n",
    "        if n_hop == 1:\n",
    "            self.W_ext = nn.Parameter(torch.randn(n_dan, n_ext),\n",
    "                                      requires_grad=True)\n",
    "        else:\n",
    "            self.W_ext = nn.Parameter(torch.randn(n_fbn, n_ext),\n",
    "                                      requires_grad=True)\n",
    "\n",
    "    def forward(self, r_kc, r_ext, time, n_batch=30, W0=None, r0=None,\n",
    "                ud_wts=True, ko_wts=None, **kwargs):\n",
    "        \"\"\" Defines the forward pass of the RNN\n",
    "\n",
    "        The KC->MBON weights are constrained to the range [0, 0.05].\n",
    "        MBONs receive external input from Kenyon cells (r_kc i.e. 'odors').\n",
    "        Feedback neurons (FBNs) receive external input (r_ext i.e. 'context').\n",
    "        DAN->MBON weights are permanently set to zero.\n",
    "        DANs receive no external input.\n",
    "\n",
    "        Parameters\n",
    "            r_kc = activity of the Kenyon cell inputs (representing odors)\n",
    "            r_ext = context inputs (representing the conditioning context)\n",
    "            time = time vector for a single interval\n",
    "            n_batch = number of trials in mini-batch\n",
    "            W0 = initial weights for KC->MBON connections\n",
    "            r0 = initial activities for output circuitry neurons\n",
    "            ud_wts = indicate whether to update dynamic weights\n",
    "                True: KC->MBON plasticity is on\n",
    "                False: KC->MBON plasticity turned off\n",
    "            ko_wts = list of MBON indices to be knocked out\n",
    "\n",
    "        Returns\n",
    "            r_recur: list of torch.ndarray(batch_size, n_mbon + n_fbn + n_dan)\n",
    "                = time series of activities in the output circuitry\n",
    "            Wt: list of torch.ndarray(batch_size, n_recur, n_recur)\n",
    "                = time series of KC->MBON weights (dopaminergic plasticity)\n",
    "            readout: list of torch.ndarray(batch_size, 1)\n",
    "                = time series of valence readouts (behaviour)\n",
    "        \"\"\"\n",
    "\n",
    "        # Define the time step of the simulation\n",
    "        dt = np.diff(time)[0]\n",
    "\n",
    "        # Initialize output circuit firing rates for each trial\n",
    "        if r0 is not None:\n",
    "            r_init = r0\n",
    "        else:\n",
    "            r_init = torch.ones(n_batch, self.n_recur) * 0.1\n",
    "            r_init[:, :self.n_mbon] = 0\n",
    "        r_recur = [r_init]\n",
    "\n",
    "        # Initialize the eligibility traces and readout\n",
    "        r_bar_kc = r_kc[:, :, 0]\n",
    "        r_bar_dan = r_recur[-1][:, -self.n_dan:]\n",
    "        readout = [torch.einsum('bom, bm -> bo',\n",
    "                                self.W_readout.repeat(n_batch, 1, 1),\n",
    "                                r_recur[-1][:, :self.n_mbon]).squeeze()]\n",
    "\n",
    "        # Clone the readout weight in case it has to be modified\n",
    "        W_readout = self.W_readout.clone()\n",
    "        # Set the weights DAN->MBON to zero\n",
    "        W_recur = self.W_recur.clone()\n",
    "        if self.n_hop == 0:\n",
    "            W_recur[:self.n_mbon, -self.n_dan:] = 0\n",
    "        elif self.n_hop == 1:\n",
    "            W_recur[:self.n_mbon, -self.n_dan:] = 0\n",
    "            W_recur[self.n_mbon:(self.n_mbon + self.n_fbn), :] = 0\n",
    "            W_recur[:, self.n_mbon:(self.n_mbon + self.n_fbn)] = 0\n",
    "        elif self.n_hop == 2:\n",
    "            W_recur[:(self.n_mbon + self.n_fbn), -(self.n_dan + self.n_fbn):] = 0\n",
    "        # Knockout specified weights\n",
    "        if ko_wts is None:\n",
    "            ko_wts = []\n",
    "        for n in ko_wts:\n",
    "            W_recur[:, n] = 0\n",
    "            W_readout[:, n] = 0\n",
    "\n",
    "        # Initialize the KC->MBON weights\n",
    "        W_kc_mbon = [W0[0]]\n",
    "        wt = [W0[1]]\n",
    "\n",
    "        # Update activity for each time step\n",
    "        for t in range(time.shape[0] - 1):\n",
    "            # Define the input to the output circuitry\n",
    "            I_tot = torch.zeros((n_batch, self.n_recur))\n",
    "            I_kc_mbon = torch.einsum('bmk, bk -> bm',\n",
    "                                     W_kc_mbon[-1], r_kc[:, :, t])\n",
    "            I_tot[:, :self.n_mbon] = I_kc_mbon\n",
    "            # Project the context (ext) signals to the DANs if FBNs are removed\n",
    "            if self.n_hop == 1:\n",
    "                I_dan = torch.einsum('bde, be -> bd',\n",
    "                                     self.W_ext.repeat(n_batch, 1, 1),\n",
    "                                     r_ext[:, :, t])\n",
    "                I_tot[:, (self.n_mbon + self.n_fbn):] = I_dan\n",
    "            else:\n",
    "                I_fbn = torch.einsum('bfe, be -> bf',\n",
    "                                     self.W_ext.repeat(n_batch, 1, 1),\n",
    "                                     r_ext[:, :, t])\n",
    "                I_tot[:, self.n_mbon:(self.n_mbon + self.n_fbn)] = I_fbn\n",
    "\n",
    "            # Update the output circuitry activity (see Eq. 1)\n",
    "            Wr_prod = torch.einsum('bsr, br -> bs',\n",
    "                                   W_recur.repeat(n_batch, 1, 1),\n",
    "                                   r_recur[-1])\n",
    "            dr = (-r_recur[-1] + F.relu(Wr_prod + self.bias.repeat(n_batch, 1)\n",
    "                                        + I_tot)) / self.tau_r\n",
    "            r_recur.append(r_recur[-1] + dr * dt)\n",
    "\n",
    "            # Update KC->MBON plasticity variables\n",
    "            if ud_wts:\n",
    "                r_dan_dt = r_recur[-1][:, -self.n_dan:]\n",
    "                wt_out = self.wt_update(W_kc_mbon, wt, dt, r_bar_kc, r_bar_dan,\n",
    "                                        r_kc[:, :, t], r_dan_dt, n_batch,\n",
    "                                        **kwargs)\n",
    "                r_bar_kc, r_bar_dan = wt_out\n",
    "\n",
    "            # Calculate the readout (see Eq. 2)\n",
    "            readout.append(torch.einsum('bom, bm -> bo',\n",
    "                                        W_readout.repeat(n_batch, 1, 1),\n",
    "                                        r_recur[-1][:, :self.n_mbon]).squeeze())\n",
    "\n",
    "        return r_recur, (W_kc_mbon, wt), readout\n",
    "\n",
    "    def wt_update(self, W_kc_mbon, wt, dt, r_bar_kc, r_bar_dan, r_kc, r_dan,\n",
    "                  n_batch, **kwargs):\n",
    "        \"\"\" Updates the KC->MBON plasticity variables\n",
    "\n",
    "        Synaptic weights from the Kenyon cells to the mushroom body output neurons\n",
    "        (MBONs) are updated dynamically. All other weights are network parameters.\n",
    "        The synaptic connections between Kenyon Cells (KCs) and MBONs are updated\n",
    "        using a LTP/LTD rule (see Figure 1B of Jiang 2020), which models dopamine-\n",
    "        gated neural plasticity on short time scale (behavioural learning).\n",
    "\n",
    "        Parameters\n",
    "            W_kc_mbon: list = KC->MBON weight matrices\n",
    "            wt = dynamic plasticity update\n",
    "            dt = time step of simulation\n",
    "            r_bar_kc = eligibility trace of Kenyon cell activity\n",
    "            r_bar_dan = eligibility trace of dopaminergic cell activity\n",
    "            r_kc = current activity of Kenyon cells\n",
    "            r_dan = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the eligibility traces (represent LTP/LTD)\n",
    "        r_bar_kc = r_bar_kc + (r_kc - r_bar_kc) * dt / self.tau_w\n",
    "        r_bar_dan = r_bar_dan + (r_dan - r_bar_dan) * dt / self.tau_w\n",
    "        # Update the dynamic weight variable\n",
    "        dw = self.calc_dw(r_bar_kc, r_bar_dan, r_kc, r_dan, n_batch, **kwargs)\n",
    "        wt.append(wt[-1] + dw * dt)\n",
    "        # Update the KC->MBON weights (see Eq. 8)\n",
    "        dW = (-W_kc_mbon[-1] + wt[-1]) / self.tau_w\n",
    "        W_tp1 = W_kc_mbon[-1] + dW * dt\n",
    "        # Clip the KC->MBON weights to the range [0, 0.05]\n",
    "        W_kc_mbon.append(torch.clamp(W_tp1, self.kc_mbon_min, self.kc_mbon_max))\n",
    "\n",
    "        return r_bar_kc, r_bar_dan\n",
    "\n",
    "    def calc_dw(self, r_bar_kc, r_bar_dan, r_kc, r_dan, n_batch, ltp=True,\n",
    "                **kwargs):\n",
    "        \"\"\" Calculates the dynamic weight update (see Eq 4).\n",
    "\n",
    "        Parameters\n",
    "            r_bar_kc = eligibility trace of Kenyon cell activity\n",
    "            r_bar_dan = eligibility trace of dopaminergic cell activity\n",
    "            r_kc = current activity of Kenyon cells\n",
    "            r_dan = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "            ltp = indicates whether to include LTP and LTD, or just LTD\n",
    "                True: calculates both LTD and LTP\n",
    "                False: only calculates LTD\n",
    "\n",
    "        Returns\n",
    "            update to dynamic plasticity variables wt\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the LTD/LTP terms\n",
    "        prod1 = torch.einsum('bd, bk -> bdk', r_bar_dan, r_kc)\n",
    "        prod2 = torch.einsum('bd, bk -> bdk', r_dan, r_bar_kc)\n",
    "\n",
    "        if ltp:\n",
    "            return prod1 - prod2\n",
    "        else:\n",
    "            return -prod2\n",
    "\n",
    "    def run_train(self, opti, *, T_int=None, T_stim=None, dt=None, n_epoch=5000,\n",
    "                  n_batch=30, reset_wts=True, clip=0.001, **kwargs):\n",
    "        \"\"\" Trains a network on classical conditioning tasks.\n",
    "\n",
    "        Tasks include first-order or second-order conditioning, and extinction.\n",
    "        Tasks consist of two (first-order) or three (second-order and extinction)\n",
    "        intervals. Each task has its own input generating function. Stimuli are\n",
    "        presented between 5-15s of each interval. Neuron activities are reset\n",
    "        between intervals to prevent associations being represented through\n",
    "        persistent activity.\n",
    "\n",
    "        Parameters\n",
    "            opti = RNN network optimizer\n",
    "            T_int = length of task intervals\n",
    "            T_stim = length of time each stimulus is presented\n",
    "            dt = time step of simulations\n",
    "            n_epoch = number of epochs to train over\n",
    "            n_batch = number of trials in mini-batch\n",
    "            reset_wts = indicates whether to reset weights between trials\n",
    "            clip = maximum gradient allowed during training\n",
    "\n",
    "        Returns\n",
    "            r_out_epoch = output circuit neuron activities for final epoch\n",
    "            Wt_epoch = KC->MBON weights for final epoch\n",
    "            vt_epoch = readout (i.e. valence) for final epoch\n",
    "            vt_opt = target valence for final epoch\n",
    "            loss_hist = list of losses for all epochs\n",
    "            ls_stims = list of stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the time variables\n",
    "        if T_int is None:\n",
    "            T_int = self.T_int\n",
    "        if T_stim is None:\n",
    "            T_stim = self.T_stim\n",
    "        if dt is None:\n",
    "            dt = self.dt\n",
    "        time_int = torch.arange(0, T_int + dt / 10, dt)\n",
    "        T_vars = (T_int, T_stim, dt, time_int.shape[0])\n",
    "        self.train_T_vars = T_vars[:-1]\n",
    "\n",
    "        # List to store losses\n",
    "        loss_hist = []\n",
    "\n",
    "        # Initialize the KC-MBON weights\n",
    "        W_in = None\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            # Lists to store activities, weights, readouts and target valences\n",
    "            rts = []\n",
    "            vts = []\n",
    "\n",
    "            # Set the intial KC->MBON weight values for each trial\n",
    "            if reset_wts or (W_in is None):\n",
    "                W_in = self.init_w_kc_mbon(None, n_batch, (epoch, n_epoch))\n",
    "            else:\n",
    "                W_in = (W_in[0][-1].detach(), W_in[1][-1].detach())\n",
    "\n",
    "            # Generate odor (r_kc), context (r_ext), and target valence (vt_opt)\n",
    "            net_inputs = self.gen_inputs(T_vars, n_batch, **kwargs)\n",
    "            # r_kc, r_ext, vt_opt, ls_stims = net_inputs\n",
    "            r_kc, r_ext, vt_opt = net_inputs\n",
    "\n",
    "            # For each interval in the task\n",
    "            for i in range(self.n_int):\n",
    "                # Run the forward model\n",
    "                net_out = self(r_kc[i], r_ext[i], time_int, n_batch, W_in)\n",
    "                rt_int, (Wt_int, wt_int), vt_int = net_out\n",
    "                # Pass the KC->MBON weights to the next interval\n",
    "                W_in = (Wt_int[-1], wt_int[-1])\n",
    "\n",
    "                # Append the interval outputs to lists\n",
    "                rts += rt_int\n",
    "                vts += vt_int\n",
    "\n",
    "            # Convert the list of time point values to a tensor\n",
    "            #  (time is the last dimension)\n",
    "            rt_epoch = torch.stack(rts, dim=-1)\n",
    "            vt_epoch = torch.stack(vts, dim=-1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = cond_loss(vt_epoch, vt_opt, rt_epoch[:, -self.n_dan:, :])\n",
    "\n",
    "            # Update the network parameters\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), clip)\n",
    "            opti.step()\n",
    "\n",
    "            # Print an update\n",
    "            if epoch % 500 == 0:\n",
    "                print(epoch, loss.item())\n",
    "            loss_hist.append(loss.item())\n",
    "\n",
    "        self.train_loss = loss_hist\n",
    "\n",
    "        return loss_hist\n",
    "\n",
    "    def init_w_kc_mbon(self, W_in, n_batch, e_tup):\n",
    "        \"\"\" Initializes the KC->MBON weights for the task.\n",
    "\n",
    "        KC->MBON weights are reset at the beginning of each epoch.\n",
    "\n",
    "        Parameters\n",
    "            W_in = specified initial weight values or None\n",
    "            n_batch = number of trials in mini-batch\n",
    "            e_tup: tuple = (current epoch, total training epochs)\n",
    "\n",
    "        Returns\n",
    "            tuple of initial KC->MBON and dynamic plasticity variables\n",
    "        \"\"\"\n",
    "\n",
    "        if W_in is None:\n",
    "            wt0 = self.W_kc_mbon_0.repeat(n_batch, 1, 1)\n",
    "            W_in = (wt0.clone(), wt0.clone())\n",
    "\n",
    "        return W_in\n",
    "\n",
    "    def gen_inputs(self, T_vars, n_batch, p_omit=0.3, **kwargs):\n",
    "        \"\"\" Generates inputs for first-order conditioning tasks.\n",
    "\n",
    "        All trials are either CS+, CS- (US omitted) or CS omitted (control trials\n",
    "        to avoid over-fitting). Of the trials where CS or US is omitted, a second\n",
    "        parameter determines the relative fractions of CS or US trials omitted\n",
    "        (p_omit_CS). See Fig. 2 of Jiang 2020 to determine sequencing of stimuli\n",
    "        during training. To account for the sequential nature of numerical\n",
    "        simulations, the target valence begins one time step after stimulus\n",
    "        onset. Details provided in Jiang 2020 -> Methods -> Conditioning Tasks.\n",
    "\n",
    "        The mix of conditions is listed as follows:\n",
    "            probability of CS+ trials = 1 - p_omit\n",
    "            probability of CS- trials = p_omit * 0.3\n",
    "            probability of control trials = p_omit * 0.7\n",
    "\n",
    "        Parameters\n",
    "            T_vars: Tuple\n",
    "                T_vars[0] = T_int = length of trial (in seconds)\n",
    "                T_vars[1] = T_stim = length of time each stimulus is presented\n",
    "                T_vars[2] = dt = time step of simulations\n",
    "                T_vars[3] = time_len = size of time vector\n",
    "            n_batch = number of trials in mini-batch\n",
    "            p_omit = probability of omitting either CS or US from trials\n",
    "\n",
    "        Returns\n",
    "            r_kct_ls = odor (KC) input time series arrays for each interval\n",
    "            r_extt_ls = context (ext) input time series arrays for each interval\n",
    "            vt_opt = target valence for plotting and loss calculations\n",
    "            ls_stims = stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the range over which stimuli can be presented\n",
    "        T_range = (5, 15)\n",
    "        # Set the time variables\n",
    "        T_stim, dt, time_len = T_vars[1:]\n",
    "\n",
    "        # Generate odors and context signals for each trial\n",
    "        r_kc, r_ext = self.gen_r_kc_ext(n_batch)\n",
    "\n",
    "        # Determine whether CS or US are randomly omitted\n",
    "        omit_inds = torch.rand(n_batch) < p_omit\n",
    "        # If omitted, determine which one is omitted\n",
    "        p_omit_CS = 0.7\n",
    "        x_omit_CS = torch.rand(n_batch)\n",
    "        omit_CS_inds = torch.logical_and(omit_inds, x_omit_CS < p_omit_CS)\n",
    "        omit_US_inds = torch.logical_and(omit_inds, x_omit_CS > p_omit_CS)\n",
    "\n",
    "        # Initialize lists to store inputs, target valence\n",
    "        r_kct_ls = []\n",
    "        r_extt_ls = []\n",
    "        vals = []\n",
    "        # ls_CS = []\n",
    "        # ls_US = []\n",
    "\n",
    "        # For each interval\n",
    "        for i in range(self.n_int):\n",
    "            # Initialize time matrices\n",
    "            time_CS = torch.zeros(n_batch, time_len)\n",
    "            time_US = torch.zeros_like(time_CS)\n",
    "            val_int = torch.zeros_like(time_CS)\n",
    "\n",
    "            # Calculate the stimulus presentation times and length\n",
    "            st_times, st_len = gen_int_times(n_batch, dt, T_stim, T_range)\n",
    "\n",
    "            for b in range(n_batch):\n",
    "                stim_inds = st_times[b] + torch.arange(st_len)\n",
    "                # Set the CS input times\n",
    "                if not omit_CS_inds[b]:\n",
    "                    time_CS[b, stim_inds] = 1\n",
    "                # Set the US input times\n",
    "                if i == 0 and not omit_US_inds[b]:\n",
    "                    time_US[b, (stim_inds + st_len)] = 1\n",
    "                # Set the target valence times\n",
    "                if i == 1 and not omit_inds[b]:\n",
    "                    if r_ext[b, 0] == 1:\n",
    "                        val_int[b, (stim_inds + 1)] = 1\n",
    "                    elif r_ext[b, 1] == 1:\n",
    "                        val_int[b, (stim_inds + 1)] = -1\n",
    "\n",
    "            # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "            r_kct = torch.einsum('bm, mbt -> bmt', r_kc,\n",
    "                                 time_CS.repeat(self.n_kc, 1, 1))\n",
    "            r_extt = torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                                  time_US.repeat(self.n_ext, 1, 1))\n",
    "\n",
    "            r_kct_ls.append(r_kct)\n",
    "            r_extt_ls.append(r_extt)\n",
    "            vals.append(val_int)\n",
    "            # ls_CS += time_CS\n",
    "            # ls_US += time_US\n",
    "\n",
    "        # Concatenate target valences\n",
    "        vt_opt = torch.cat((vals[0], vals[1]), dim=-1)\n",
    "\n",
    "        # # Make a list of stimulus times to plot\n",
    "        # ls_stims = [torch.cat(ls_CS), torch.cat(ls_US)]\n",
    "\n",
    "        # return r_kct_ls, r_extt_ls, vt_opt, ls_stims\n",
    "        return r_kct_ls, r_extt_ls, vt_opt\n",
    "\n",
    "    def run_eval(self, trial_fnc, *, T_int=None, T_stim=None, dt=None, n_trial=1,\n",
    "                 n_batch=1, reset_wts=True, save_data=False, **kwargs):\n",
    "        \"\"\" Runs an evaluation based on a series of input functions\n",
    "\n",
    "        Parameters\n",
    "            trial_fnc = function that defines a trial\n",
    "            T_int = length of a task interval (in seconds)\n",
    "            T_stim = length of time each stimulus is presented\n",
    "            dt = time step of simulation (in seconds)\n",
    "            n_trial = number of trials to run\n",
    "            n_batch = number of parallel trials in a batch\n",
    "            reset_wts = indicates whether to reset weights between trials\n",
    "            save_all_data = save data for each trial, or just the last\n",
    "        \"\"\"\n",
    "\n",
    "        # Reset lists storing evaluation data\n",
    "        self.eval_rts = []\n",
    "        self.eval_Wts = []\n",
    "        self.eval_wts = []\n",
    "        self.eval_vts = []\n",
    "        self.eval_vt_opts = []\n",
    "        self.eval_CS_stim = []\n",
    "        self.eval_US_stim = []\n",
    "        self.eval_err = []\n",
    "\n",
    "        # Set the time variables\n",
    "        if T_int is None:\n",
    "            T_int = self.T_int\n",
    "        if T_stim is None:\n",
    "            T_stim = self.T_stim\n",
    "        if dt is None:\n",
    "            dt = self.dt\n",
    "        T_vars = (T_int, T_stim, dt)\n",
    "\n",
    "        # Initialize the KC-MBON weights and plasticity variable\n",
    "        W_in = None\n",
    "\n",
    "        # For each trial, run the given trial function\n",
    "        for trial in range(n_trial):\n",
    "            # Determine whether to reset KC->MBON weights between trials\n",
    "            if reset_wts or (W_in is None):\n",
    "                W_in = self.init_w_kc_mbon(None, n_batch, (trial, n_trial))\n",
    "            else:\n",
    "                W_in = (W_in[0][-1].detach(), W_in[1][-1].detach())\n",
    "\n",
    "            trial_outs = trial_fnc(self, W_in, T_vars, n_batch, **kwargs)\n",
    "            rts, Wts, wts, vts, vt_opt, err, odors, stim = trial_outs\n",
    "\n",
    "            # Store the network errors\n",
    "            self.eval_err.append(err)\n",
    "            if save_data:\n",
    "                # Store the network data\n",
    "                self.eval_rts.append(rts)\n",
    "                self.eval_Wts.append(Wts)\n",
    "                self.eval_wts.append(wts)\n",
    "                self.eval_vts.append(vts)\n",
    "                self.eval_vt_opts.append(vt_opt)\n",
    "                self.eval_odors.append(odors)\n",
    "                # Store the time series lists\n",
    "                self.eval_CS_stim.append(stim[0])\n",
    "                self.eval_US_stim.append(stim[1])\n",
    "            else:\n",
    "                # Store the network data\n",
    "                self.eval_rts = [rts]\n",
    "                self.eval_Wts = [Wts]\n",
    "                self.eval_wts = [wts]\n",
    "                self.eval_vts = [vts]\n",
    "                self.eval_vt_opts = [vt_opt]\n",
    "                self.eval_odors = [odors]\n",
    "                # Store the time series lists\n",
    "                self.eval_CS_stim = [stim[0]]\n",
    "                self.eval_US_stim = [stim[1]]\n",
    "\n",
    "    def gen_r_kc_ext(self, n_batch, pos_vt=None, **kwargs):\n",
    "        \"\"\" Generates neuron activations for context and odor inputs.\n",
    "\n",
    "        Parameters\n",
    "            n_batch = number of trials in eval-batch\n",
    "            pos_vt (kwarg) = indicates whether valence should be positive\n",
    "                             None: random valence\n",
    "                             True: positive valence\n",
    "                             False: negative valence\n",
    "        \"\"\"\n",
    "\n",
    "        # Determine the contextual input (r_ext)\n",
    "        if pos_vt is None:\n",
    "            r_ext = torch.multinomial(torch.ones(n_batch, self.n_ext),\n",
    "                                      self.n_ext)\n",
    "        elif pos_vt:\n",
    "            r_ext = torch.tensor([1, 0]).repeat(n_batch, 1)\n",
    "        elif not pos_vt:\n",
    "            r_ext = torch.tensor([0, 1]).repeat(n_batch, 1)\n",
    "        else:\n",
    "            raise Exception('Not a valid value for pos_vt')\n",
    "\n",
    "        # Determine odor input (r_kc)\n",
    "        r_kc = torch.zeros(n_batch, self.n_kc)\n",
    "        for b in range(n_batch):\n",
    "            # Define an odor (CS) for each trial\n",
    "            if self.train_odors is not None:\n",
    "                n_odors = self.train_odors.shape[0]\n",
    "                odor_select = torch.randint(n_odors, (1,))\n",
    "                # r_kc_inds = self.train_odors[odor_select, :]\n",
    "                r_kc[b, :] = self.train_odors[odor_select, :]\n",
    "            else:\n",
    "                r_kc_inds = torch.multinomial(torch.ones(self.n_kc), self.n_ones)\n",
    "                r_kc[b, r_kc_inds] = 1\n",
    "            # r_kc[b, r_kc_inds] = 1\n",
    "\n",
    "        return r_kc, r_ext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPlasticityRNN(FirstOrderCondRNN):\n",
    "    def __init__(self, *, n_odors=10, odor_seed=12345678, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Set the static KC->MBON weights\n",
    "        W_kc_mbon_max = 0.05\n",
    "        self.W_kc_mbon = Variable(torch.rand(self.n_mbon, self.n_kc) *\n",
    "                                  W_kc_mbon_max, requires_grad=False)\n",
    "        # Generate a static list of odors for the network to train on\n",
    "        self.n_odors = n_odors\n",
    "        gen = torch.Generator()\n",
    "        gen = gen.manual_seed(odor_seed)\n",
    "        odor_list = torch.zeros(n_odors, self.n_kc)\n",
    "        odor_inds = torch.multinomial(torch.ones(n_odors, self.n_kc),\n",
    "                                      self.n_ones, generator=gen)\n",
    "        for n in range(n_odors):\n",
    "            # Define an odor (CS)\n",
    "            odor_list[n, odor_inds[n, :]] = 1\n",
    "        self.train_odors = odor_list\n",
    "        # Set the number of task intervals\n",
    "        self.n_int = 1\n",
    "\n",
    "    def wt_update(self, W_kc_mbon, wt, dt, r_bar_kc, r_bar_dan, r_kc, r_dan,\n",
    "                  n_batch, **kwargs):\n",
    "        \"\"\" Returns directly the static KC->MBON plasticity variables\n",
    "\n",
    "        Since this class has no plasticity, the KC->MBON weights and plasticity\n",
    "        variables are not updated. Therefore, the values are returned directly.\n",
    "\n",
    "        Parameters\n",
    "            W_kc_MBON: list = KC->MBON weight matrices\n",
    "            wt = dynamic plasticity update\n",
    "            dt = time step of simulation\n",
    "            r_bar_kc = eligibility trace of Kenyon cell activity\n",
    "            r_bar_dan = eligibility trace of dopaminergic cell activity\n",
    "            r_kc = current activity of Kenyon cells\n",
    "            r_dan = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "        \"\"\"\n",
    "\n",
    "        return r_bar_kc, r_bar_dan\n",
    "\n",
    "    def gen_inputs(self, T_vars, n_batch, **kwargs):\n",
    "        \"\"\" Generates inputs for task without KC->MBON plasticity.\n",
    "\n",
    "        All trials are CS+ or control trials where CS+ is switched out for a\n",
    "        neutral CS in the second presentation (CS- trials). In the case where the\n",
    "        CS is switched, the target valence is zero. To account for the sequential\n",
    "        nature of numerical simulations, the target valence is set to begin one\n",
    "        time step after stimulus onset.\n",
    "\n",
    "        The mix of conditions is listed as follows:\n",
    "            probability of trials where CS+ is switched = 0.5\n",
    "\n",
    "        Parameters\n",
    "            T_vars: Tuple\n",
    "                T_vars[0] = T_int = length of trial (in seconds)\n",
    "                T_vars[1] = T_stim = length of time each stimulus is presented\n",
    "                T_vars[2] = dt = time step of simulations\n",
    "                T_vars[3] = time_len = size of time vector\n",
    "            n_batch = number of trials in mini-batch\n",
    "            p_omit = probability of omitting either CS or US from trials\n",
    "\n",
    "        Returns\n",
    "            r_kct_ls = odor (KC) input time series arrays for each interval\n",
    "            r_extt_ls = context (ext) input time series arrays for each interval\n",
    "            vt_opt = target valence for plotting and loss calculations\n",
    "            ls_stims = stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the range over which stimuli can be presented\n",
    "        T_range = [(5, 15), (20, 30)]\n",
    "        # Set the time variables\n",
    "        T_stim, dt, time_len = T_vars[1:]\n",
    "\n",
    "        # Generate odors and context signals for each trial\n",
    "        r_kc, r_ext = self.gen_r_kc_ext(n_batch, **kwargs)\n",
    "\n",
    "        # Determine whether CS2+ is switched (switch on half of trials)\n",
    "        switch_inds = torch.rand(n_batch) < 0.5\n",
    "\n",
    "        # Initialize activity matrices\n",
    "        r_kct = torch.zeros(n_batch, self.n_kc, time_len)\n",
    "        r_extt = torch.zeros(n_batch, self.n_ext, time_len)\n",
    "\n",
    "        # Initialize valence matrix\n",
    "        vt_opt = torch.zeros(n_batch, time_len)\n",
    "        # time_US = torch.zeros_like(vt_opt)\n",
    "        # time_CS_both = torch.zeros_like(vt_opt)\n",
    "\n",
    "        # For each stimulus presentation\n",
    "        for i in range(2):\n",
    "            # Initialize time matrices\n",
    "            time_CS = torch.zeros_like(vt_opt)\n",
    "            time_US = torch.zeros_like(vt_opt)\n",
    "\n",
    "            # Calculate the stimulus presentation times and length\n",
    "            st_times, st_len = gen_int_times(n_batch, dt, T_stim, T_range[i])\n",
    "\n",
    "            for b in range(n_batch):\n",
    "                # if i == 0:\n",
    "                #     print((r_kc[b, :] == 1).nonzero().squeeze())\n",
    "                #\n",
    "                stim_inds = st_times[b] + torch.arange(st_len)\n",
    "                # Set the CS time\n",
    "                time_CS[b, stim_inds] = 1\n",
    "                # Set the US time\n",
    "                if i == 0:\n",
    "                    time_US[b, stim_inds + st_len] = 1\n",
    "                # Set the CS+/CS2 and target valence times\n",
    "                if i == 1:\n",
    "                    # Switch the odor in half the trials (target valence is zero)\n",
    "                    if switch_inds[b]:\n",
    "                        CS2_inds = torch.multinomial(torch.ones(self.n_kc),\n",
    "                                                     self.n_kc)\n",
    "                        r_kc[b, :] = r_kc[b, CS2_inds]\n",
    "                        r_ext[b, :] = 0\n",
    "                    # If the odor is not switched, set the target valence\n",
    "                    else:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            vt_opt[b, (stim_inds + 1)] = 1\n",
    "                        elif r_ext[b, 1] == 1:\n",
    "                            vt_opt[b, (stim_inds + 1)] = -1\n",
    "\n",
    "            # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "            r_kct += torch.einsum('bm, mbt -> bmt', r_kc,\n",
    "                                  time_CS.repeat(self.n_kc, 1, 1))\n",
    "            r_extt += torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                                   time_US.repeat(self.n_ext, 1, 1))\n",
    "            # time_CS_both += time_CS\n",
    "\n",
    "        # # Make a list of stimulus times to plot\n",
    "        # ls_stims = [time_CS_both, time_US]\n",
    "\n",
    "        # return [r_kct], [r_extt], vt_opt, ls_stims\n",
    "        return [r_kct], [r_extt], vt_opt\n",
    "\n",
    "    def init_w_kc_mbon(self, W_in, n_batch, e_tup):\n",
    "        \"\"\" Initializes the KC->MBON weights for the task.\n",
    "\n",
    "        KC->MBON weights are reset at the beginning of each epoch.\n",
    "\n",
    "        Parameters\n",
    "            W_in = specified initial weight values or None\n",
    "            n_batch = number of trials in mini-batch\n",
    "            e_tup: tuple = (current epoch, total training epochs)\n",
    "\n",
    "        Returns\n",
    "            tuple of initial KC->MBON and dynamic plasticity variables\n",
    "        \"\"\"\n",
    "\n",
    "        if W_in is None:\n",
    "            wt0 = self.W_kc_mbon.repeat(n_batch, 1, 1)\n",
    "            W_in = (wt0.clone(), wt0.clone())\n",
    "\n",
    "        return W_in\n",
    "\n",
    "# test_RNN = NoPlasticityRNN(n_odors=10)\n",
    "# print(test_RNN.W_KC_MBON.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_err(vt, vt_opt):\n",
    "    \"\"\" Calculates the readout error for conditioning tasks.\n",
    "\n",
    "    Only accounts for error due to difference between target (optimal) readout\n",
    "    and the MBON readout scalar.\n",
    "\n",
    "    Parameters\n",
    "        vt = time dependent valence output of network\n",
    "        vt_opt = target valence (must be a torch tensor)\n",
    "\n",
    "    Returns\n",
    "        vt_loss = scalar readout error used in evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the MSE loss of the valence\n",
    "    vt_loss = torch.mean((vt - vt_opt) ** 2)\n",
    "\n",
    "    return vt_loss\n",
    "\n",
    "\n",
    "def cond_loss(vt, vt_opt, r_DAN, lam=0.1):\n",
    "    \"\"\" Calculates the loss for conditioning tasks.\n",
    "\n",
    "    Composed of an MSE cost based on the difference between output and\n",
    "    target valence, and a regularization cost that penalizes excess\n",
    "    dopaminergic activity. Reference Eqs. (3) and (9) in Jiang 2020.\n",
    "\n",
    "    Parameters\n",
    "        vt = time dependent valence output of network\n",
    "        vt_opt = target valence (must be a torch tensor)\n",
    "        r_DAN = time series of dopaminergic neuron activities\n",
    "        lam = regularization constant\n",
    "\n",
    "    Returns\n",
    "        loss_tot = scalar loss used in backprop\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the baseline DAN activity\n",
    "    DAN_baseline = 0.1\n",
    "\n",
    "    # Calculate the MSE loss of the valence (scalar)\n",
    "    vt_loss = cond_err(vt, vt_opt)\n",
    "\n",
    "    # Calculate DAN activity regularization term (scalar)\n",
    "    rt_sum = torch.sum(F.relu(r_DAN - DAN_baseline) ** 2, dim=1)\n",
    "    rt_loss = torch.mean(rt_sum) * lam\n",
    "\n",
    "    # Calculate the combined loss\n",
    "    loss = vt_loss + rt_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def gen_int_times(n_batch, dt, T_stim, T_range=(5, 15), **kwargs):\n",
    "    \"\"\" Generates an array of stimulus presentation times for all trials\n",
    "\n",
    "    Parameters\n",
    "        dt = time step of simulations\n",
    "        n_batch = number of trials in eval-batch\n",
    "        T_stim = length of time each stimulus is presented\n",
    "        T_range: tuple = range in which stimulus can be presented\n",
    "\n",
    "    Returns\n",
    "        st_times = array of stimulus presentation times for each trial\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "    \"\"\"\n",
    "\n",
    "    # Present the stimulus between 5-15s of each interval\n",
    "    min_ind = int(T_range[0] / dt)\n",
    "    max_ind = int((T_range[1] - 2 * T_stim) / dt)\n",
    "    st_times = torch.randint(min_ind, max_ind, (n_batch,))\n",
    "    st_len = int(T_stim / dt)\n",
    "\n",
    "    return st_times, st_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_trial(network, task1:str, task2:str, dt=0.5):\n",
    "def print_trial(network, optimizer, task, n_odors=10, dt=0.5):\n",
    "    \"\"\" Plots a figure similar to Figure 2 from Jiang 2020.\n",
    "    \n",
    "    Runs the network using a novel combination of conditioned and unconditioned stimuli,\n",
    "    then prints the results. Top: time series of the various stimuli (CS and US), as well as\n",
    "    the target valence and readout. Bottom: activity of eight randomly chosen mushroom body\n",
    "    output neurons (MBONs).\n",
    "    \n",
    "    Paramters\n",
    "        network = previously trained RNN\n",
    "        task = the type of task to be plotted ('CS+' or 'CS2')\n",
    "        dt = time step of the simulation/plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the labels for the plots\n",
    "    if task == 'CS+':\n",
    "        task_title = 'First-Order Conditioning'\n",
    "    elif task == 'CS2':\n",
    "        task_title = 'CS+ Generalization'\n",
    "    \n",
    "    # Define plot font sizes\n",
    "    label_font = 18\n",
    "    title_font = 24\n",
    "    legend_font = 12\n",
    "\n",
    "    # Run the network\n",
    "    r_out, vt, vt_opt, loss_hist, stim_ls = network.train_net(opti=optimizer, dt=dt, n_epoch=1, n_batch=1, n_odors=n_odors)\n",
    "    r_out = r_out.detach().numpy().squeeze()\n",
    "    vt = vt.detach().numpy().squeeze()\n",
    "    vt_opt = vt_opt.detach().numpy().squeeze()\n",
    "    plot_CS = stim_ls[0].numpy().squeeze()\n",
    "    plot_US = stim_ls[1].numpy().squeeze()\n",
    "    plot_time = np.arange(plot_CS.size) * dt\n",
    "\n",
    "    # Plot the conditioning and test\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True, gridspec_kw={'height_ratios': [1, 4]})\n",
    "    ax1.plot(plot_time, vt, label='Readout')\n",
    "    ax1.plot(plot_time, vt_opt, label='Target')\n",
    "    # Second-order conditioning involves an additional stimulus time series\n",
    "    if task == 'CS+':\n",
    "        ax1.plot(plot_time, plot_CS, label=task)\n",
    "    else:\n",
    "        half_ind = plot_time.size // 2\n",
    "        plot_CSp = np.zeros(plot_time.size)\n",
    "        plot_CSp[:half_ind] = plot_CS[:half_ind]\n",
    "        plot_CS2 = np.zeros(plot_time.size)\n",
    "        plot_CS2[half_ind:] = plot_CS[half_ind:]\n",
    "        ax1.plot(plot_time, plot_CSp, label='CS+')\n",
    "        ax1.plot(plot_time, plot_CS2, label=task)\n",
    "    ax1.plot(plot_time, plot_US, label='US')\n",
    "    ax1.set_ylabel('Value', fontsize=label_font)\n",
    "    ax1.set_title(task_title, fontsize=title_font)\n",
    "    ax1.legend(fontsize=legend_font)\n",
    "\n",
    "    # Plot the activities of a few MBONs\n",
    "    plot_neurs = np.random.choice(network.N_MBON, size=8, replace=False)\n",
    "    r_max = np.max(r_out)\n",
    "    for i, n in enumerate(plot_neurs):\n",
    "        ax2.plot(plot_time, (r_out[n, :] / r_max) + (i * 2 / 3), '-k')\n",
    "    ax2.set_xlabel('Time', fontsize=label_font)\n",
    "    ax2.set_ylabel('Normalized Activity', fontsize=label_font)\n",
    "    ax2.set_yticks([])\n",
    "    fig.tight_layout();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([100])\n",
      "torch.Size([60, 2])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "# classic_net = DrosophilaRNN()\n",
    "classic_net = NoPlasticityRNN(n_odors=1)\n",
    "# classic_net = NoPlasticityRNN(n_odors=1)\n",
    "for param in classic_net.parameters():\n",
    "    print(param.shape)\n",
    "#     print(param)\n",
    "# print(classic_net.N_DAN)\n",
    "\n",
    "# Define the model's optimizer\n",
    "lr = 0.001\n",
    "optimizer = optim.RMSprop(classic_net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 307.5860900878906\n",
      "500 0.014365077950060368\n",
      "1000 0.010001723654568195\n",
      "1500 0.010060250759124756\n",
      "2000 0.00676898006349802\n",
      "2500 0.004844062030315399\n",
      "3000 0.004785757511854172\n",
      "3500 0.005294321570545435\n",
      "4000 0.004724365193396807\n",
      "4500 0.003908597398549318\n"
     ]
    }
   ],
   "source": [
    "train_bool = True\n",
    "if train_bool:\n",
    "#     r_out, vt, vt_opt, loss_hist, _ = classic_net.train_net(opti=optimizer, n_epoch=2000, n_odors=1)\n",
    "    loss_hist = classic_net.run_train(opti=optimizer, n_epoch=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_bool:\n",
    "    # Plot the loss function\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "    axes[0].plot(loss_hist)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=label_font)\n",
    "    axes[0].set_ylabel('Loss', fontsize=label_font)\n",
    "    axes[1].plot(loss_hist[5:])\n",
    "    axes[1].set_xlabel('Epoch', fontsize=label_font)\n",
    "    axes[1].set_ylabel('Loss', fontsize=label_font)\n",
    "    fig.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classic_net.state_dict(), 'trained_no_plasticity_10odor_5000ep.pt')\n",
    "\n",
    "# https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some results to compare to Fig 2A/B in paper\n",
    "if not train_bool:\n",
    "#     classic_net.load_state_dict(torch.load('trained_N5_no_plasticity_10odor_2000epochs.pt'))\n",
    "#     classic_net.load_state_dict(torch.load('trained_N6_no_plasticity_1odor_2000epochs.pt'))\n",
    "    classic_net.eval()\n",
    "\n",
    "# print_trial(classic_net, optimizer, task='CS+')\n",
    "# print_trial(classic_net, task='CS2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0039345091208815575\n",
      "0.0037157740804832427 0.0004360443604457427\n"
     ]
    }
   ],
   "source": [
    "# r_out, Wt, vt, vt_opt, loss_hist, _ = train_net(classic_net, task=train_task, n_epochs=200, train=False)\n",
    "loss_hist = classic_net.run_train(opti=optimizer, n_epoch=200)\n",
    "print(np.mean(loss_hist), np.std(loss_hist))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
