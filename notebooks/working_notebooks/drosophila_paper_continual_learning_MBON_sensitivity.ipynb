{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "# from numpy.linalg import svd as svd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Define plot font sizes\n",
    "label_font = 18\n",
    "title_font = 24\n",
    "legend_font = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model Class - Classical Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drosophila_RNN(nn.Module):\n",
    "    def __init__(self, KC_size=200, MBON_size=20, DAN_size=20, FBN_size=60, ext_size=2, out_size=1, net_seed=1234):\n",
    "        super(Drosophila_RNN, self).__init__()\n",
    "        # Set the seeds\n",
    "#         np.random.seed(net_seed)\n",
    "#         torch.manual_seed(net_seed)\n",
    "        # Set constants\n",
    "        self.KC_MBON_min = 0. # Minimum synaptic weight\n",
    "        self.KC_MBON_max = 0.05 # Maximum synaptic weight\n",
    "        self.tau_w = 5 # Time scale of KC->MBON LTD/LTP (plasticity)\n",
    "        self.tau_r = 1 # Time scale of output circuitry activity\n",
    "        # Set the sizes of layers\n",
    "        self.N_KC = KC_size\n",
    "        self.N_MBON = MBON_size\n",
    "        self.N_FBN = FBN_size\n",
    "        self.N_DAN = DAN_size\n",
    "        self.N_recur = MBON_size + FBN_size + DAN_size\n",
    "        self.N_ext = ext_size\n",
    "        self.N_out = out_size\n",
    "        # Define updatable network parameters\n",
    "#         seed_num = net_seed\n",
    "        seed_num = None\n",
    "        sqrt2 = torch.sqrt(torch.tensor(2, dtype=torch.float))\n",
    "        mean_MBON = torch.zeros((self.N_recur, MBON_size))\n",
    "        mean_FBN = torch.zeros((self.N_recur, FBN_size))\n",
    "        mean_DAN = torch.zeros((self.N_recur, DAN_size))\n",
    "        W_MBON = torch.normal(mean_MBON, torch.sqrt(1 / (sqrt2 * MBON_size)), generator=seed_num)\n",
    "        W_FBN = torch.normal(mean_FBN, torch.sqrt(1 / (sqrt2 * FBN_size)), generator=seed_num)\n",
    "        W_DAN = torch.normal(mean_DAN, torch.sqrt(1 / (sqrt2 * DAN_size)), generator=seed_num)\n",
    "        self.W_recur = nn.Parameter(torch.cat((W_MBON, W_FBN, W_DAN), dim=1), requires_grad=True)\n",
    "        self.W_ext = nn.Parameter(torch.randn(FBN_size, ext_size), requires_grad=True)\n",
    "        mean_readout = torch.zeros((out_size, MBON_size))\n",
    "        std_readout = 1 / torch.sqrt(torch.tensor(MBON_size, dtype=torch.float))\n",
    "        self.W_readout = nn.Parameter(torch.normal(mean_readout, std_readout, generator=seed_num), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones(self.N_recur) * 0.1, requires_grad=True)\n",
    "        self.beta = nn.Parameter(torch.ones(KC_size) * 0.01, requires_grad=True)\n",
    "        \n",
    "            \n",
    "    def forward(self, r_KC, r_ext, time, epoch, W0=None, n_epochs=5000, n_batch=30, nps_bool=True):\n",
    "        \"\"\" Defines the forward pass of the RNN\n",
    "        \n",
    "        Synaptic weights from the Keyon cells to the mushroom body output neurons (MBONs) are updated\n",
    "        dynamically. All other weights are network parameters. The synaptic connections between Keyon\n",
    "        Cells (KCs) and MBONs are updated using a LTP/LTD rule (see Figure 1B of Jiang 2020), which\n",
    "        models dopamine-gated neural plasticity on short time scale (behavioural learning). Early\n",
    "        KC->MBON weight saturation is prevented by a linearly increasing learning rate (see Eq. 12).\n",
    "        Additionally, non-specific potentiation is included as a form of homeostasis (see Eq. 5).\n",
    "        \n",
    "        The KC->MBON weights are constrained to the range [0, 0.05].\n",
    "        MBONs receive external input from Keyon cells (r_KC i.e. 'odors').\n",
    "        Feedback neurons (FBNs) receive external contextual input (r_ext i.e. 'context').\n",
    "        DAN->MBON weights are permanently set to zero. DANs receive no external input.\n",
    "\n",
    "        Inputs\n",
    "            r_KC = activity of the Kenyon cell neurons (representing odors)\n",
    "            r_ext = context signals (representing the conditioning context)\n",
    "            time = time vector for a single interval\n",
    "            epoch = current training epoch\n",
    "            W0 = initial weights for KC->MBON connections\n",
    "            n_epochs = total number of training epochs\n",
    "            n_batch = number of trials in batch\n",
    "            nps_bool = boolean indicating whether to include non-specific potentiation\n",
    "\n",
    "        Returns\n",
    "            r_recur: list of torch.ndarray(batch_size, N_MBON + N_FBN + N_DAN)\n",
    "                = time series of activities in the output circuitry\n",
    "            Wt: list of torch.ndarray(batch_size, N_MBON + N_FBN + N_DAN, N_MBON + N_FBN + N_DAN)\n",
    "                = time series of KC->MBON weights (represent dopaminergic plasticity)\n",
    "            readout: list of torch.ndarray(batch_size, 1)\n",
    "                = time series of valence readouts (represents behaviour)\n",
    "        \"\"\"\n",
    "\n",
    "        # Define the time step of the simulation\n",
    "        dt = np.diff(time)[0]\n",
    "\n",
    "        # Initialize output circuit firing rates for each trial\n",
    "        r_init = torch.ones((n_batch, self.N_recur)) * 0.1\n",
    "        r_init[:, :self.N_MBON] = 0\n",
    "        r_recur = []\n",
    "        r_recur.append(r_init)\n",
    "\n",
    "        # Initialize the eligibility traces and readout\n",
    "        r_bar_KC = r_KC[:, :, 0]\n",
    "        r_bar_DAN = r_recur[-1][:, -self.N_DAN:]\n",
    "        readout = []\n",
    "        readout.append(torch.squeeze(torch.einsum('bom, bm -> bo', self.W_readout.repeat(n_batch, 1, 1), r_recur[-1][:, :self.N_MBON])))\n",
    "        \n",
    "        # Initialize the dynamic weight variables, accounting for early saturation (see Eq. 12)\n",
    "        wt = []\n",
    "        wt.append(torch.zeros((n_batch, self.N_MBON, self.N_KC)))\n",
    "        W_KC_MBON = []\n",
    "        W_KC_MBON_0 = torch.ones((n_batch, self.N_MBON, self.N_KC)) * self.KC_MBON_max\n",
    "        if W0 is None:\n",
    "            W0 = W_KC_MBON_0\n",
    "        # Calculate the saturation parameter and modify initial weights\n",
    "        x_sat = min(1, (epoch / (n_epochs / 2)))\n",
    "        W0 = (1 - x_sat) * W_KC_MBON_0 + x_sat * W0\n",
    "        W_KC_MBON.append(W0)\n",
    "\n",
    "        # Set the weights DAN->MBON to zero\n",
    "        W_recur = self.W_recur.clone()\n",
    "        W_recur[:self.N_MBON, -self.N_DAN:] = 0\n",
    "        \n",
    "        # Rectify the potentiation parameter\n",
    "        beta = F.relu(self.beta.clone())\n",
    "        \n",
    "        # Update activity for each time step\n",
    "        for t in range(time.size()[0] - 1):\n",
    "            # Define the input to the output circuitry\n",
    "            I_KC_MBON = torch.einsum('bmk, bk -> bm', W_KC_MBON[-1], r_KC[:, :, t])\n",
    "            I_FBN = torch.einsum('bfe, be -> bf', self.W_ext.repeat(n_batch, 1, 1), r_ext[:, :, t])\n",
    "            I = torch.zeros((n_batch, self.N_recur))\n",
    "            I[:, :self.N_MBON] = I_KC_MBON\n",
    "            I[:, self.N_MBON:self.N_MBON + self.N_FBN] = I_FBN\n",
    "\n",
    "            # Update the output circuitry activity (see Eq. 1)\n",
    "            Wr_prod = torch.einsum('bsr, br -> bs', W_recur.repeat(n_batch, 1, 1), r_recur[-1])\n",
    "            dr = (-r_recur[-1] + F.relu(Wr_prod + self.bias.repeat(n_batch, 1) + I)) / self.tau_r\n",
    "            r_recur.append(r_recur[-1] + dr * dt)\n",
    "\n",
    "            # Update KC->MBON plasticity variables\n",
    "            # Calculate the eligibility traces (represent LTP/LTD)\n",
    "            r_bar_KC = r_bar_KC + (r_KC[:, :, t] - r_bar_KC) * dt / self.tau_w\n",
    "            r_bar_DAN = r_bar_DAN + (r_recur[-1][:, -self.N_DAN:] - r_bar_DAN) * dt / self.tau_w\n",
    "            # Update the dynamic weight variable (see Eq. 5)\n",
    "            prod1 = torch.einsum('bd, bk -> bdk', r_bar_DAN, r_KC[:, :, t])\n",
    "            prod2 = torch.einsum('bd, bk -> bdk', r_recur[-1][:, -self.N_DAN:], r_bar_KC)\n",
    "            # Include non-specific potentiation (unless control condition)\n",
    "            if nps_bool:\n",
    "                # Constrain the potentiation parameter to be positive\n",
    "                prod3 = torch.einsum('bd, bk -> bdk', r_bar_DAN, beta.repeat(n_batch, 1))\n",
    "            else:\n",
    "                prod3 = torch.zeros_like(prod2)\n",
    "            dw = (prod1 - prod2 + prod3)\n",
    "            wt.append(wt[-1] + dw * dt)\n",
    "            # Update the KC->MBON weights (see Eq. 8)\n",
    "            dW = (-W_KC_MBON[-1] + wt[-1]) / self.tau_w\n",
    "            W_tp1 = W_KC_MBON[-1] + dW * dt\n",
    "            # Clip the KC->MBON weights to the range [0, 0.05]\n",
    "            W_KC_MBON.append(torch.clamp(W_tp1, self.KC_MBON_min, self.KC_MBON_max))\n",
    "\n",
    "            # Calculate the readout (see Eq. 2)\n",
    "            readout.append(torch.squeeze(torch.einsum('bom, bm -> bo', self.W_readout.repeat(n_batch, 1, 1), r_recur[-1][:, :self.N_MBON])))\n",
    "\n",
    "        return r_recur, W_KC_MBON, readout\n",
    "            \n",
    "        \n",
    "# Clipping weights between [0, 0.05]\n",
    "# https://discuss.pytorch.org/t/how-to-do-constrained-optimization-in-pytorch/60122\n",
    "# https://discuss.pytorch.org/t/set-constraints-on-parameters-or-layers/23620\n",
    "# https://discuss.pytorch.org/t/restrict-range-of-variable-during-gradient-descent/1933/4\n",
    "\n",
    "# Setting DAN->MBON weights to zero\n",
    "# https://pytorch.org/docs/stable/generated/torch.triu.html\n",
    "\n",
    "# Broadcasting using einsum\n",
    "# https://github.com/pytorch/pytorch/issues/15671\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function for conditioning tasks\n",
    "def cond_loss(vt, vt_opt, r_DAN, lam=0.1):\n",
    "    \"\"\" Calculates the loss for conditioning tasks.\n",
    "    \n",
    "    Composed of an MSE cost based on the difference between output and\n",
    "    target valence, and a regularization cost that penalizes excess\n",
    "    dopaminergic activity. Reference Eqs. (3) and (9) in Jiang 2020.\n",
    "    \n",
    "    Parameters\n",
    "        vt = time dependent valence output of network\n",
    "        vt_opt = target valence (must be a torch tensor)\n",
    "        r_DAN = time series of dopaminergic neuron activities\n",
    "        lam = regularization constant\n",
    "    \n",
    "    Returns\n",
    "        loss_tot = scalar loss used in backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the baseline DAN activity\n",
    "    DAN_baseline = 0.1\n",
    "    \n",
    "    # Calculate the MSE loss of the valence\n",
    "    v_sum = torch.mean((vt - vt_opt)**2, dim=1)\n",
    "    v_loss = torch.mean(v_sum)\n",
    "    \n",
    "    # Calculate regularization term\n",
    "    r_sum = torch.sum(F.relu(r_DAN - 0.1)**2, dim=1)\n",
    "    r_loss = lam * torch.mean(r_sum, dim=1)\n",
    "    \n",
    "    # Calculate the summed loss (size = n_batch)\n",
    "    loss = v_loss + r_loss\n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    loss_tot = torch.mean(loss)\n",
    "    \n",
    "    return loss_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a training function for first-order conditioning\n",
    "def train_net(network, T_int=200, T_stim=2, dt=0.5, n_epochs=5000, n_batch=30, clip=0.001, train=True, nps=True):\n",
    "    \"\"\" Trains a network on a continual learning task.\n",
    "    \n",
    "    Task includes repetitive presentation of four stimuli (two CS+ and two CS-) per trial.\n",
    "    The presentation times of each stimulus are drawn from a Poisson process, and the inputs\n",
    "    are generated by a function. The dopamine gated plasticity has an additional non-specific\n",
    "    potentiation factor (see Eq. 5 of Jiang 2020), which is included in the forward() function.\n",
    "    KC->MBON weights are set once at the beginning of optimization, and then continue forward\n",
    "    between each successive trial (see p14 of Jiang 2020).\n",
    "    \n",
    "    Parameters\n",
    "        network = RNN network to be trained or ran\n",
    "        T_int = length of task intervals (eg conditioning, test, extinction)\n",
    "        T_stim = length of time each stimulus is presented\n",
    "        dt = time step of simulations\n",
    "        n_epochs = number of epochs to train over\n",
    "        n_batch = number of trials in mini-batch\n",
    "        clip = maximum gradient allowed during training\n",
    "        train = boolean indicating whether to perform backprop\n",
    "        nps = boolean indicating whether to include non-specific potentiation\n",
    "        \n",
    "    Returns\n",
    "        r_out_epoch = output circuit neuron activities for final epoch\n",
    "        Wt_epoch = KC->MBON weights for final epoch\n",
    "        vt_epoch = readout (i.e. valence) for final epoch\n",
    "        vt_opt = target valence for final epoch\n",
    "        loss_hist = list of losses for all epochs\n",
    "        ls_stims = list of stimulus time series for plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Interval time vector\n",
    "    time_int = torch.arange(0, T_int + dt/10, dt)\n",
    "\n",
    "    # Neuron population sizes\n",
    "    n_KC = network.N_KC\n",
    "    n_ext = network.N_ext\n",
    "    n_MBON = network.N_MBON\n",
    "    # Set the intial KC->MBON weight values\n",
    "    W_KC_MBON = torch.ones((n_batch, n_MBON, n_KC)) * network.KC_MBON_max\n",
    "\n",
    "    # List to store losses\n",
    "    loss_hist = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Generate the odor (r_KC) and context (r_ext) inputs, and target valence (vt_opt)\n",
    "        r_KC, r_ext, vt_opt, ls_stims = continual_inputs(T_int, T_stim, dt, n_KC, n_ext, n_batch)\n",
    "\n",
    "        # Run the forward model\n",
    "        r_outs, Wts, vts = network(r_KC, r_ext, time_int, epoch, W_KC_MBON, n_epochs, n_batch, nps)\n",
    "        # Set the initial KC->MBON weights for the next trial\n",
    "        W_KC_MBON = Wts[-1].detach()\n",
    "\n",
    "        # Concatenate the activities, weights and valences\n",
    "        r_out_epoch = torch.stack(r_outs, dim=-1)\n",
    "        Wt_epoch = torch.stack(Wts, dim=-1)\n",
    "        vt_epoch = torch.stack(vts, dim=-1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = cond_loss(vt_epoch, vt_opt, r_out_epoch[:, -network.N_DAN:, :])\n",
    "\n",
    "        if train:\n",
    "            # Update the network parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(network.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print an update\n",
    "        if epoch % 500 == 0:\n",
    "            print(epoch, loss.item())\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "    return r_out_epoch, Wt_epoch, vt_epoch, vt_opt, loss_hist, ls_stims\n",
    "\n",
    "# https://discuss.pytorch.org/t/proper-way-to-do-gradient-clipping/191/13\n",
    "# https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48\n",
    "# https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for each of the conditioning tasks\n",
    "# Some detail provided in Jiang 2020 -> Methods -> Continual learning\n",
    "# def continual_inputs(stim_times, stim_len, time_len, CS_odors, n_KC, n_ext, n_batch, plot=None):\n",
    "# def continual_inputs(stim_times, stim_len, time_len, n_KC, n_ext, n_batch, plot=None):\n",
    "def continual_inputs(T_int, T_stim, dt, n_KC, n_ext, n_batch):\n",
    "    \"\"\" Generates inputs for continual learning task.\n",
    "    \n",
    "    All trials are composed of Poisson distributed presentations of two CS+ and two CS- (US omitted)\n",
    "    stimuli. CS+ are presented with a US in all cases (target valence set for CS+ after first\n",
    "    presentation). To account for the sequential nature of numerical simulations, the target valence\n",
    "    is set to begin one time step after stimulus onset.\n",
    "    \n",
    "    Parameters\n",
    "        T_int = length of task interval\n",
    "        T_stim = length of time each stimulus is presented\n",
    "        dt = time step of simulations\n",
    "        CS_odors = list of two CS+ and two CS- stimuli for each trial in batch\n",
    "        n_KC = number of Kenyon cell input neurons\n",
    "        n_ext = number of contextual input neurons\n",
    "        n_batch = number of trials in mini-batch\n",
    "        plot = used when plot function is called, indicates which task to plot\n",
    "        \n",
    "    Returns\n",
    "        r_KCt = list of odor (KC) input time series arrays for each interval\n",
    "        r_extt = list of context (ext) input time series arrays for each interval\n",
    "        vt_opt = time series of target valence for plotting and loss calculations\n",
    "        ls_stims = list of stimulus time series for plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of odors in trial\n",
    "    n_odors = 4\n",
    "    # Number of active neurons in an odor\n",
    "    n_ones = int(n_KC * 0.1)\n",
    "    # Poisson rate of stimulus presentations\n",
    "    stim_rate = 2 / T_int\n",
    "    # Length of stimulus in indices\n",
    "    stim_len = int(T_stim / dt)\n",
    "    # Length of trial in indices\n",
    "    time_len = int(T_int / dt + 1)\n",
    "    \n",
    "    # Initialize activity matrices\n",
    "    r_KCt = torch.zeros(n_batch, n_KC, time_len)\n",
    "    r_extt = torch.zeros(n_batch, n_ext, time_len)\n",
    "    \n",
    "    # Initialize lists and arrays to store stimulus time series\n",
    "    ls_CS = []\n",
    "    time_US_all = torch.zeros(n_batch, time_len)\n",
    "    vt_opt = torch.zeros(n_batch, time_len)\n",
    "    \n",
    "    # For each batch, randomly generate different odors and presentation times\n",
    "    for i in range(n_odors):\n",
    "        # Initialize the CS time matrix\n",
    "        time_CS = torch.zeros(n_batch, time_len)\n",
    "        time_US = torch.zeros_like(time_CS)\n",
    "\n",
    "        # Conditioned stimuli (CS) = odors\n",
    "        r_KC = torch.zeros(n_batch, n_KC)\n",
    "        # Unconditioned stimuli (US) = context\n",
    "        r_ext = torch.multinomial(torch.ones(n_batch, n_ext), n_ext)\n",
    "            \n",
    "        # For each trial\n",
    "        for b in range(n_batch):\n",
    "            # Define an odor (CS)\n",
    "            r_KC_inds = torch.multinomial(torch.ones(n_KC), n_ones)\n",
    "            r_KC[b, r_KC_inds] = 1\n",
    "\n",
    "            # Generate a list of stimulus presentation times\n",
    "            stim_times = []\n",
    "            last_time = 0\n",
    "            while True:\n",
    "                stim_isi = -torch.log(torch.rand(1)) / stim_rate\n",
    "                next_time = last_time + stim_isi\n",
    "                if next_time < (T_int - 2 * T_stim):\n",
    "                    # Stimulus times are indices (not times)\n",
    "                    stim_times.append((next_time / dt).int())\n",
    "                    last_time += stim_isi\n",
    "                # Ensure at least one presentation of each stimuli\n",
    "                elif last_time == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "            for j, st in enumerate(stim_times):\n",
    "                # Set the CS input times\n",
    "                stim_inds = st + torch.arange(stim_len)\n",
    "                time_CS[b, stim_inds] = 1\n",
    "\n",
    "                # For CS+ odors, set US and the valence\n",
    "                if i < 2:\n",
    "                    # Set the US input times\n",
    "                    time_US[b, (stim_inds + stim_len)] = 1\n",
    "                    # Set a target valence on every presentation but the first\n",
    "                    if j > 0:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            vt_opt[b, (stim_inds + 1)] = 1\n",
    "                        else:\n",
    "                            vt_opt[b, (stim_inds + 1)] = -1\n",
    "\n",
    "        # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "        r_KCt += torch.einsum('bm, mbt -> bmt', r_KC, time_CS.repeat(n_KC, 1, 1))\n",
    "        r_extt += torch.einsum('bm, mbt -> bmt', r_ext, time_US.repeat(n_ext, 1, 1))\n",
    "        ls_CS += time_CS\n",
    "        time_US_all += time_US\n",
    "        \n",
    "    # Make a list of stimulus times to plot\n",
    "    ls_stims = ls_CS + [time_US_all]\n",
    "        \n",
    "    return r_KCt, r_extt, vt_opt, ls_stims\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trial(network, dt=0.5, nps=True):\n",
    "    \"\"\" Plots a figure similar to Figure 2 from Jiang 2020.\n",
    "    \n",
    "    Runs the network using a novel combination of conditioned and unconditioned stimuli,\n",
    "    then prints the results. Top: time series of the various stimuli (CS and US), as well as\n",
    "    the target valence and readout. Bottom: activity of eight randomly chosen mushroom body\n",
    "    output neurons (MBONs).\n",
    "    \n",
    "    Paramters\n",
    "        network = previously trained RNN\n",
    "        dt = time step of the simulation/plot\n",
    "        nps = boolean indicating whether to include non-specific potentiation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define plot font sizes\n",
    "    label_font = 18\n",
    "    title_font = 24\n",
    "    legend_font = 12\n",
    "\n",
    "    # Run the network\n",
    "    r_out, Wt, vt, vt_opt, loss_hist, stim_ls = train_net(network, dt=dt, n_epochs=1, n_batch=1, train=False, nps=nps)\n",
    "    r_out = r_out.detach().numpy().squeeze()\n",
    "    vt = vt.detach().numpy().squeeze()\n",
    "    vt_opt = vt_opt.detach().numpy().squeeze()\n",
    "    plot_US = stim_ls[-1].numpy().squeeze()\n",
    "    plot_time = np.arange(plot_US.size) * dt\n",
    "\n",
    "    # Plot the conditioning and test\n",
    "    CS_labels = ['CS1+', 'CS2+', 'CS1-', 'CS2-']\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), sharex=True, gridspec_kw={'height_ratios': [1, 4]})\n",
    "    ax1.plot(plot_time, vt, label='Readout')\n",
    "    ax1.plot(plot_time, vt_opt, label='Target')\n",
    "    for i in range(len(stim_ls) - 1):\n",
    "        plot_CS = stim_ls[i].numpy().squeeze()\n",
    "        ax1.plot(plot_time, plot_CS, label=CS_labels[i])\n",
    "    ax1.plot(plot_time, plot_US, label='US')\n",
    "    ax1.set_ylabel('Value', fontsize=label_font)\n",
    "    ax1.set_title('Continual Learning', fontsize=title_font)\n",
    "    ax1.legend(fontsize=legend_font)\n",
    "\n",
    "    # Plot the activities of a few MBONs\n",
    "    plot_neurs = np.random.choice(network.N_MBON, size=8, replace=False)\n",
    "    r_max = np.max(r_out)\n",
    "    for i, n in enumerate(plot_neurs):\n",
    "        ax2.plot(plot_time, (r_out[n, :] / r_max) + (i * 2 / 3), '-k')\n",
    "    ax2.set_xlabel('Time', fontsize=label_font)\n",
    "    ax2.set_ylabel('Normalized Activity', fontsize=label_font)\n",
    "    ax2.set_yticks([])\n",
    "    fig.tight_layout();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network with 2 MBONs\n",
      "0 1.6981298087711096e+21\n",
      "500 2.205445670095607e+21\n",
      "1000 1.6503235357766184e+21\n",
      "1500 1.4784183232501194e+21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6a81b09b06ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Run the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training network with {} MBONs'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_MBON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassic_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mloss_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassic_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MBON_sensitivity/continual_sensitivity_T{}_{:02d}MBONs.pt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_MBON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5734f71e43e7>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(network, T_int, T_stim, dt, n_epochs, n_batch, clip, train, nps)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# Update the network parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassic_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set parameters for sensitivity training\n",
    "train_bool = True\n",
    "n_vals = 5\n",
    "n_MBON_0 = 2\n",
    "n_epochs_train = 5000\n",
    "loss_arr = torch.zeros(n_vals, n_epochs_train)\n",
    "header = ''\n",
    "test_num = 1\n",
    "\n",
    "if train_bool:\n",
    "    n_MBON = n_MBON_0 - 1\n",
    "    for i in range(n_vals):\n",
    "        if test_num == 1:\n",
    "            n_MBON = n_MBON_0**(i+1)\n",
    "        elif (test_num == 2) or (test_num == 3):\n",
    "            n_MBON += 1\n",
    "        # Initialize the network\n",
    "        classic_net = Drosophila_RNN(MBON_size=n_MBON, DAN_size=n_MBON)\n",
    "        # Define the model's optimizer\n",
    "        lr = 0.001\n",
    "        optimizer = optim.RMSprop(classic_net.parameters(), lr=lr)\n",
    "        # Run the network\n",
    "        print('Training network with {} MBONs'.format(n_MBON))\n",
    "        r_out, Wt, vt, vt_opt, loss_hist, _ = train_net(classic_net, n_epochs=n_epochs_train)\n",
    "        loss_arr[i, :] = torch.Tensor(loss_hist)\n",
    "        torch.save(classic_net.state_dict(), 'MBON_sensitivity/continual_sensitivity_T{}_{:02d}MBONs.pt'.format(test_num, n_MBON))\n",
    "\n",
    "# Save the loss data to csv format\n",
    "np.savetxt(\"MBON_sensitivity/continual_sensitivity_T{}_train_losses.csv\".format(test_num),\n",
    "           loss_arr.T, delimiter=\",\", header=header)\n",
    "\n",
    "#     # Plot the loss function\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "#     axes[0].plot(loss_hist)\n",
    "#     axes[0].set_xlabel('Epoch', fontsize=label_font)\n",
    "#     axes[0].set_ylabel('Loss', fontsize=label_font)\n",
    "#     axes[1].plot(loss_hist[5:])\n",
    "#     axes[1].set_xlabel('Epoch', fontsize=label_font)\n",
    "#     axes[1].set_ylabel('Loss', fontsize=label_font)\n",
    "#     fig.tight_layout();\n",
    "\n",
    "# https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some results to compare to Fig 2A/B in paper\n",
    "n_epochs_test = 50\n",
    "loss_arr = np.zeros((n_vals, n_epochs_test))\n",
    "n_MBON_vec = np.zeros(n_vals)\n",
    "n_MBON = n_MBON_0 - 1\n",
    "header = ''\n",
    "\n",
    "for i in range(n_vals):\n",
    "    if test_num == 1:\n",
    "        n_MBON = n_MBON_0**(i+1)\n",
    "    elif test_num == 2:\n",
    "        n_MBON += 1\n",
    "    n_MBON_vec[i] = n_MBON\n",
    "    header += '{} MBONs, '.format(n_MBON)\n",
    "    # Initialize the network\n",
    "    classic_net = Drosophila_RNN(MBON_size=n_MBON, DAN_size=n_MBON)\n",
    "    # Load the network\n",
    "    classic_net.load_state_dict(torch.load('MBON_sensitivity/continual_sensitivity_T{}_{:02d}MBONs.pt'.format(test_num, n_MBON)))\n",
    "    classic_net.eval()\n",
    "    # Run the network\n",
    "    r_out, Wt, vt, vt_opt, loss_hist, _ = train_net(classic_net, task=train_task, n_epochs=n_epochs_test, train=False)\n",
    "    loss_arr[i, :] = torch.Tensor(loss_hist).detach().numpy()\n",
    "\n",
    "# Save the loss data to csv format\n",
    "np.savetxt(\"MBON_sensitivity/continual_sensitivity_T{}_test_losses.csv\".format(test_num), loss_arr.T,\n",
    "           delimiter=\",\", header=header)\n",
    "\n",
    "# Calculate statistics on losses\n",
    "loss_avg = np.mean(loss_arr, axis=1)\n",
    "loss_std = np.std(loss_arr, axis=1)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.errorbar(n_MBON_vec, loss_avg, loss_std, None, '-o');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
