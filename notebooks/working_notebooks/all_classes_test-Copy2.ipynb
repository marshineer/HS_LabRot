{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define plot font sizes\n",
    "label_font = 18\n",
    "title_font = 24\n",
    "legend_font = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstOrderCondRNN(nn.Module):\n",
    "    def __init__(self, *, n_kc=200, n_mbon=20, n_fbn=60, n_ext=2, n_out=1,\n",
    "                 f_ones=0.1, n_seed=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set the seeds\n",
    "        if n_seed is not None:\n",
    "            np.random.seed(n_seed)\n",
    "            torch.manual_seed(n_seed)\n",
    "\n",
    "        # Set constants\n",
    "        W_kc_mbon_max = 0.05\n",
    "        self.kc_mbon_min = 0.  # Minimum synaptic weight\n",
    "        self.kc_mbon_max = W_kc_mbon_max  # Maximum synaptic weight\n",
    "        self.W_kc_mbon_0 = Variable(torch.ones((n_mbon, n_kc)) * W_kc_mbon_max,\n",
    "                                    requires_grad=False)\n",
    "        self.tau_w = 5  # Time scale of KC->MBON LTD/LTP (plasticity)\n",
    "        self.tau_r = 1  # Time scale of output circuitry activity\n",
    "        self.n_int = 2  # Number of task intervals\n",
    "\n",
    "        # Set the sizes of layers\n",
    "        n_dan = n_mbon\n",
    "        self.n_kc = n_kc\n",
    "        self.n_mbon = n_mbon\n",
    "        self.n_fbn = n_fbn\n",
    "        self.n_dan = n_dan\n",
    "        self.n_recur = n_mbon + n_fbn + n_dan\n",
    "        self.n_ext = n_ext\n",
    "        self.n_out = n_out\n",
    "        self.n_ones = int(n_kc * f_ones)\n",
    "\n",
    "        # Define network variables used to store data\n",
    "        # Odors\n",
    "        self.train_odors = None\n",
    "        self.eval_odors = None\n",
    "        # Training parameters (for continuation)\n",
    "        self.train_T_vars = None\n",
    "        self.train_rts = None\n",
    "        self.train_Wts = None\n",
    "        self.train_wts = None\n",
    "        self.train_vts = None\n",
    "        self.train_vt_opts = None\n",
    "        self.train_CS_stim = None\n",
    "        self.train_US_stim = None\n",
    "        self.train_loss = None\n",
    "        # Evaluation parameters (for plotting and analysis)\n",
    "        self.eval_rts = None\n",
    "        self.eval_Wts = None\n",
    "        self.eval_wts = None\n",
    "        self.eval_vts = None\n",
    "        self.eval_vt_opts = None\n",
    "        self.eval_CS_stim = None\n",
    "        self.eval_US_stim = None\n",
    "        self.eval_loss = None\n",
    "\n",
    "        # Define updatable network parameters\n",
    "        sqrt2 = torch.sqrt(torch.tensor(2, dtype=torch.float))\n",
    "        mean_mbon = torch.zeros((self.n_recur, n_mbon))\n",
    "        mean_fbn = torch.zeros((self.n_recur, n_fbn))\n",
    "        mean_dan = torch.zeros((self.n_recur, n_dan))\n",
    "        W_mbon = torch.normal(mean_mbon, torch.sqrt(1 / (sqrt2 * n_mbon)),\n",
    "                              generator=n_seed)\n",
    "        W_fbn = torch.normal(mean_fbn, torch.sqrt(1 / (sqrt2 * n_fbn)),\n",
    "                             generator=n_seed)\n",
    "        W_dan = torch.normal(mean_dan, torch.sqrt(1 / (sqrt2 * n_dan)),\n",
    "                             generator=n_seed)\n",
    "        self.W_recur = nn.Parameter(torch.cat((W_mbon, W_fbn, W_dan), dim=1),\n",
    "                                    requires_grad=True)\n",
    "        self.W_ext = nn.Parameter(torch.randn(n_fbn, n_ext),\n",
    "                                  requires_grad=True)\n",
    "        mean_readout = torch.zeros((n_out, n_mbon))\n",
    "        std_readout = 1 / torch.sqrt(torch.tensor(n_mbon, dtype=torch.float))\n",
    "        self.W_readout = nn.Parameter(torch.normal(mean_readout, std_readout,\n",
    "                                                   generator=n_seed),\n",
    "                                      requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones(self.n_recur) * 0.1,\n",
    "                                 requires_grad=True)\n",
    "\n",
    "    def forward(self, r_kc, r_ext, time, n_batch=30, W0=None, r0=None, **kwargs):\n",
    "        \"\"\" Defines the forward pass of the RNN\n",
    "\n",
    "        The KC->MBON weights are constrained to the range [0, 0.05].\n",
    "        MBONs receive external input from Kenyon cells (r_kc i.e. 'odors').\n",
    "        Feedback neurons (FBNs) receive external input (r_ext i.e. 'context').\n",
    "        DAN->MBON weights are permanently set to zero.\n",
    "        DANs receive no external input.\n",
    "\n",
    "        Parameters\n",
    "            r_kc = activity of the Kenyon cell inputs (representing odors)\n",
    "            r_ext = context inputs (representing the conditioning context)\n",
    "            time = time vector for a single interval\n",
    "            n_batch = number of trials in mini-batch\n",
    "            W0 = initial weights for KC->MBON connections\n",
    "            r0 = initial activities for output circuitry neurons\n",
    "\n",
    "        Returns\n",
    "            r_recur: list of torch.ndarray(batch_size, n_mbon + n_fbn + n_dan)\n",
    "                = time series of activities in the output circuitry\n",
    "            Wt: list of torch.ndarray(batch_size, n_recur, n_recur)\n",
    "                = time series of KC->MBON weights (dopaminergic plasticity)\n",
    "            readout: list of torch.ndarray(batch_size, 1)\n",
    "                = time series of valence readouts (behaviour)\n",
    "        \"\"\"\n",
    "\n",
    "        # Define the time step of the simulation\n",
    "        dt = np.diff(time)[0]\n",
    "\n",
    "        # Initialize output circuit firing rates for each trial\n",
    "        if r0 is not None:\n",
    "            r_init = r0\n",
    "        else:\n",
    "            r_init = torch.ones(n_batch, self.n_recur) * 0.1\n",
    "            r_init[:, :self.n_mbon] = 0\n",
    "        r_recur = [r_init]\n",
    "\n",
    "        # Initialize the eligibility traces and readout\n",
    "        r_bar_kc = r_kc[:, :, 0]\n",
    "        r_bar_dan = r_recur[-1][:, -self.n_dan:]\n",
    "        readout = [torch.einsum('bom, bm -> bo',\n",
    "                                self.W_readout.repeat(n_batch, 1, 1),\n",
    "                                r_recur[-1][:, :self.n_mbon]).squeeze()]\n",
    "\n",
    "        # Set the weights DAN->MBON to zero\n",
    "        W_recur = self.W_recur.clone()\n",
    "        W_recur[:self.n_mbon, -self.n_dan:] = 0\n",
    "\n",
    "        # Initialize the KC->MBON weights\n",
    "        W_kc_mbon = [W0[0]]\n",
    "        wt = [W0[1]]\n",
    "\n",
    "        # Update activity for each time step\n",
    "        for t in range(time.shape[0] - 1):\n",
    "            # Define the input to the output circuitry\n",
    "            I_kc_mbon = torch.einsum('bmk, bk -> bm',\n",
    "                                     W_kc_mbon[-1], r_kc[:, :, t])\n",
    "            I_fbn = torch.einsum('bfe, be -> bf',\n",
    "                                 self.W_ext.repeat(n_batch, 1, 1),\n",
    "                                 r_ext[:, :, t])\n",
    "            I_tot = torch.zeros((n_batch, self.n_recur))\n",
    "            I_tot[:, :self.n_mbon] = I_kc_mbon\n",
    "            I_tot[:, self.n_mbon:self.n_mbon + self.n_fbn] = I_fbn\n",
    "\n",
    "            # Update the output circuitry activity (see Eq. 1)\n",
    "            Wr_prod = torch.einsum('bsr, br -> bs',\n",
    "                                   W_recur.repeat(n_batch, 1, 1),\n",
    "                                   r_recur[-1])\n",
    "            dr = (-r_recur[-1] + F.relu(Wr_prod + self.bias.repeat(n_batch, 1)\n",
    "                                        + I_tot)) / self.tau_r\n",
    "            r_recur.append(r_recur[-1] + dr * dt)\n",
    "\n",
    "            # Update KC->MBON plasticity variables\n",
    "            wt_out = self.wt_update(W_kc_mbon, wt, dt, r_bar_kc, r_bar_dan,\n",
    "                                    r_kc[:, :, t], r_recur[-1][:, -self.n_dan:],\n",
    "                                    n_batch, **kwargs)\n",
    "            # W_kc_mbon, wt, r_bar_kc, r_bar_dan = out\n",
    "            r_bar_kc, r_bar_dan = wt_out\n",
    "\n",
    "            # Calculate the readout (see Eq. 2)\n",
    "            readout.append(torch.einsum('bom, bm -> bo',\n",
    "                                        self.W_readout.repeat(n_batch, 1, 1),\n",
    "                                        r_recur[-1][:, :self.n_mbon]).squeeze())\n",
    "\n",
    "        return r_recur, (W_kc_mbon, wt), readout\n",
    "\n",
    "    def wt_update(self, W_kc_mbon, wt, dt, r_bar_kc, r_bar_dan, r_kc, r_dan,\n",
    "                  n_batch, **kwargs):\n",
    "        \"\"\" Updates the KC->MBON plasticity variables\n",
    "\n",
    "        Synaptic weights from the Kenyon cells to the mushroom body output neurons\n",
    "        (MBONs) are updated dynamically. All other weights are network parameters.\n",
    "        The synaptic connections between Kenyon Cells (KCs) and MBONs are updated\n",
    "        using a LTP/LTD rule (see Figure 1B of Jiang 2020), which models dopamine-\n",
    "        gated neural plasticity on short time scale (behavioural learning).\n",
    "\n",
    "        Parameters\n",
    "            W_kc_mbon: list = KC->MBON weight matrices\n",
    "            wt = dynamic plasticity update\n",
    "            dt = time step of simulation\n",
    "            r_bar_kc = eligibility trace of Kenyon cell activity\n",
    "            r_bar_dan = eligibility trace of dopaminergic cell activity\n",
    "            r_kc = current activity of Kenyon cells\n",
    "            r_dan = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the eligibility traces (represent LTP/LTD)\n",
    "        r_bar_kc = r_bar_kc + (r_kc - r_bar_kc) * dt / self.tau_w\n",
    "        r_bar_dan = r_bar_dan + (r_dan - r_bar_dan) * dt / self.tau_w\n",
    "        # Update the dynamic weight variable\n",
    "        dw = self.calc_dw(r_bar_kc, r_bar_dan, r_kc, r_dan, n_batch, **kwargs)\n",
    "        wt.append(wt[-1] + dw * dt)\n",
    "        # Update the KC->MBON weights (see Eq. 8)\n",
    "        dW = (-W_kc_mbon[-1] + wt[-1]) / self.tau_w\n",
    "        W_tp1 = W_kc_mbon[-1] + dW * dt\n",
    "        # Clip the KC->MBON weights to the range [0, 0.05]\n",
    "        W_kc_mbon.append(torch.clamp(W_tp1, self.kc_mbon_min, self.kc_mbon_max))\n",
    "\n",
    "        # return W_kc_mbon, wt, r_bar_kc, r_bar_dan\n",
    "        return r_bar_kc, r_bar_dan\n",
    "\n",
    "    def calc_dw(self, r_bar_kc, r_bar_dan, r_kc, r_dan, n_batch, **kwargs):\n",
    "        \"\"\" Calculates the dynamic weight update (see Eq 4).\n",
    "\n",
    "        Parameters\n",
    "            r_bar_kc = eligibility trace of Kenyon cell activity\n",
    "            r_bar_dan = eligibility trace of dopaminergic cell activity\n",
    "            r_kc = current activity of Kenyon cells\n",
    "            r_dan = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "\n",
    "        Returns\n",
    "            update to dynamic plasticity variables wt\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the LTD/LTP terms\n",
    "        prod1 = torch.einsum('bd, bk -> bdk', r_bar_dan, r_kc)\n",
    "        prod2 = torch.einsum('bd, bk -> bdk', r_dan, r_bar_kc)\n",
    "\n",
    "        return prod1 - prod2\n",
    "\n",
    "    def run_train(self, *, opti, T_int=30, T_stim=2, dt=0.5, n_epoch=5000,\n",
    "                  n_batch=30, reset_wts=True, clip=0.001, **kwargs):\n",
    "        \"\"\" Trains a network on classical conditioning tasks.\n",
    "\n",
    "        Tasks include first-order or second-order conditioning, and extinction.\n",
    "        Tasks consist of two (first-order) or three (second-order and extinction)\n",
    "        intervals. Each task has its own input generating function. Stimuli are\n",
    "        presented between 5-15s of each interval. Neuron activities are reset\n",
    "        between intervals to prevent associations being represented through\n",
    "        persistent activity.\n",
    "\n",
    "        Parameters\n",
    "            opti = RNN network optimizer\n",
    "            T_int = length of task intervals\n",
    "            T_stim = length of time each stimulus is presented\n",
    "            dt = time step of simulations\n",
    "            n_epoch = number of epochs to train over\n",
    "            n_batch = number of trials in mini-batch\n",
    "            reset_wts = indicates whether to reset weights between trials\n",
    "            clip = maximum gradient allowed during training\n",
    "\n",
    "        Returns\n",
    "            r_out_epoch = output circuit neuron activities for final epoch\n",
    "            Wt_epoch = KC->MBON weights for final epoch\n",
    "            vt_epoch = readout (i.e. valence) for final epoch\n",
    "            vt_opt = target valence for final epoch\n",
    "            loss_hist = list of losses for all epochs\n",
    "            ls_stims = list of stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Interval time vector and time variables\n",
    "        time_int = torch.arange(0, T_int + dt / 10, dt)\n",
    "        T_vars = (T_int, T_stim, dt, time_int.shape[0])\n",
    "        self.train_T_vars = T_vars[:-1]\n",
    "\n",
    "        # List to store losses\n",
    "        loss_hist = []\n",
    "\n",
    "        # Initialize the KC-MBON weights\n",
    "        W_in = None\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            # Lists to store activities, weights, readouts and target valences\n",
    "            rts = []\n",
    "            vts = []\n",
    "\n",
    "            # Set the intial KC->MBON weight values for each trial\n",
    "            if reset_wts or (W_in is None):\n",
    "                W_in = self.init_w_kc_mbon(None, n_batch, (epoch, n_epoch))\n",
    "            else:\n",
    "                W_in = (W_in[0][-1].detach(), W_in[1][-1].detach())\n",
    "\n",
    "            # Generate odor (r_kc), context (r_ext), and target valence (vt_opt)\n",
    "            net_inputs = self.gen_inputs(T_vars, n_batch, **kwargs)\n",
    "            r_kc, r_ext, vt_opt, ls_stims = net_inputs\n",
    "\n",
    "            # For each interval in the task\n",
    "            for i in range(self.n_int):\n",
    "                # Run the forward model\n",
    "                net_out = self(r_kc[i], r_ext[i], time_int, n_batch, W_in)\n",
    "                rt_int, (Wt_int, wt_int), vt_int = net_out\n",
    "                # Pass the KC->MBON weights to the next interval\n",
    "                W_in = (Wt_int[-1], wt_int[-1])\n",
    "\n",
    "                # Append the interval outputs to lists\n",
    "                rts += rt_int\n",
    "                vts += vt_int\n",
    "\n",
    "            # Convert the list of time point values to a tensor\n",
    "            #  (time is the last dimension)\n",
    "            rt_epoch = torch.stack(rts, dim=-1)\n",
    "            vt_epoch = torch.stack(vts, dim=-1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = cond_loss(vt_epoch, vt_opt, rt_epoch[:, -self.n_dan:, :])\n",
    "\n",
    "            # Update the network parameters\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), clip)\n",
    "            opti.step()\n",
    "\n",
    "            # Print an update\n",
    "            if epoch % 500 == 0:\n",
    "                print(epoch, loss.item())\n",
    "            loss_hist.append(loss.item())\n",
    "\n",
    "        return loss_hist\n",
    "\n",
    "    def init_w_kc_mbon(self, W_in, n_batch, e_tup):\n",
    "        \"\"\" Initializes the KC->MBON weights for the task.\n",
    "\n",
    "        KC->MBON weights are reset at the beginning of each epoch.\n",
    "\n",
    "        Parameters\n",
    "            W_in = specified initial weight values or None\n",
    "            n_batch = number of trials in mini-batch\n",
    "            e_tup: tuple = (current epoch, total training epochs)\n",
    "\n",
    "        Returns\n",
    "            tuple of initial KC->MBON and dynamic plasticity variables\n",
    "        \"\"\"\n",
    "\n",
    "        if W_in is None:\n",
    "            wt0 = self.W_kc_mbon_0.repeat(n_batch, 1, 1)\n",
    "            W_in = (wt0.clone(), wt0.clone())\n",
    "\n",
    "        return W_in\n",
    "\n",
    "    def gen_inputs(self, T_vars, n_batch, p_omit=0.3):\n",
    "        \"\"\" Generates inputs for first-order conditioning tasks.\n",
    "\n",
    "        All trials are either CS+, CS- (US omitted) or CS omitted (control trials\n",
    "        to avoid over-fitting). Of the trials where CS or US is omitted, a second\n",
    "        parameter determines the relative fractions of CS or US trials omitted\n",
    "        (p_omit_CS). See Fig. 2 of Jiang 2020 to determine sequencing of stimuli\n",
    "        during training. To account for the sequential nature of numerical\n",
    "        simulations, the target valence begins one time step after stimulus\n",
    "        onset. Details provided in Jiang 2020 -> Methods -> Conditioning Tasks.\n",
    "\n",
    "        The mix of conditions is listed as follows:\n",
    "            probability of CS+ trials = 1 - p_omit\n",
    "            probability of CS- trials = p_omit * 0.3\n",
    "            probability of control trials = p_omit * 0.7\n",
    "\n",
    "        Parameters\n",
    "            T_vars: Tuple\n",
    "                T_vars[0] = T_int = length of trial (in seconds)\n",
    "                T_vars[1] = T_stim = length of time each stimulus is presented\n",
    "                T_vars[2] = dt = time step of simulations\n",
    "                T_vars[3] = time_len = size of time vector\n",
    "            n_batch = number of trials in mini-batch\n",
    "            p_omit = probability of omitting either CS or US from trials\n",
    "\n",
    "        Returns\n",
    "            r_kct_ls = odor (KC) input time series arrays for each interval\n",
    "            r_extt_ls = context (ext) input time series arrays for each interval\n",
    "            vt_opt = target valence for plotting and loss calculations\n",
    "            ls_stims = stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the range over which stimuli can be presented\n",
    "        T_range = (5, 15)\n",
    "        # Set the time variables\n",
    "        T_stim, dt, time_len = T_vars[1:]\n",
    "\n",
    "        # Generate odors and context signals for each trial\n",
    "        r_kc, r_ext = self.gen_r_kc_ext(n_batch)\n",
    "\n",
    "        # Determine whether CS or US are randomly omitted\n",
    "        omit_inds = torch.rand(n_batch) < p_omit\n",
    "        # If omitted, determine which one is omitted\n",
    "        p_omit_CS = 0.7\n",
    "        x_omit_CS = torch.rand(n_batch)\n",
    "        omit_CS_inds = torch.logical_and(omit_inds, x_omit_CS < p_omit_CS)\n",
    "        omit_US_inds = torch.logical_and(omit_inds, x_omit_CS > p_omit_CS)\n",
    "\n",
    "        # Initialize lists to store inputs, target valence and stimulus times\n",
    "        r_kct_ls = []\n",
    "        r_extt_ls = []\n",
    "        vals = []\n",
    "        ls_CS = []\n",
    "        ls_US = []\n",
    "\n",
    "        # For each interval\n",
    "        for i in range(self.n_int):\n",
    "            # Initialize time matrices\n",
    "            time_CS = torch.zeros(n_batch, time_len)\n",
    "            time_US = torch.zeros_like(time_CS)\n",
    "            val_int = torch.zeros_like(time_CS)\n",
    "\n",
    "            # Calculate the stimulus presentation times and length\n",
    "            st_times, st_len = gen_int_times(n_batch, dt, T_stim, T_range)\n",
    "\n",
    "            for b in range(n_batch):\n",
    "                stim_inds = st_times[b] + torch.arange(st_len)\n",
    "                # Set the CS input times\n",
    "                if not omit_CS_inds[b]:\n",
    "                    time_CS[b, stim_inds] = 1\n",
    "                # Set the US input times\n",
    "                if i == 0 and not omit_US_inds[b]:\n",
    "                    time_US[b, (stim_inds + st_len)] = 1\n",
    "                # Set the target valence times\n",
    "                if i == 1 and not omit_inds[b]:\n",
    "                    if r_ext[b, 0] == 1:\n",
    "                        val_int[b, (stim_inds + 1)] = 1\n",
    "                    else:\n",
    "                        val_int[b, (stim_inds + 1)] = -1\n",
    "\n",
    "            # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "            r_kct = torch.einsum('bm, mbt -> bmt', r_kc,\n",
    "                                 time_CS.repeat(self.n_kc, 1, 1))\n",
    "            r_extt = torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                                  time_US.repeat(self.n_ext, 1, 1))\n",
    "\n",
    "            r_kct_ls.append(r_kct)\n",
    "            r_extt_ls.append(r_extt)\n",
    "            vals.append(val_int)\n",
    "            ls_CS += time_CS\n",
    "            ls_US += time_US\n",
    "\n",
    "        # Concatenate target valences\n",
    "        vt_opt = torch.cat((vals[0], vals[1]), dim=-1)\n",
    "\n",
    "        # Make a list of stimulus times to plot\n",
    "        ls_stims = [torch.cat(ls_CS), torch.cat(ls_US)]\n",
    "\n",
    "        return r_kct_ls, r_extt_ls, vt_opt, ls_stims\n",
    "\n",
    "    def run_eval(self, *, trial_ls, T_int=30, T_stim=2, dt=0.5, n_trial=1,\n",
    "                 n_batch=1, reset_wts=True, **kwargs):\n",
    "        \"\"\" Runs an evaluation based on a series of input functions\n",
    "\n",
    "        Parameters\n",
    "            trial_ls = list of interval functions that compose a trial\n",
    "            T_int = length of a task interval (in seconds)\n",
    "            T_stim = length of time each stimulus is presented\n",
    "            dt = time step of simulation (in seconds)\n",
    "            n_trial = number of trials to run\n",
    "            n_batch = number of parallel trials in a batch\n",
    "            reset_wts = indicates whether to reset weights between trials\n",
    "        \"\"\"\n",
    "\n",
    "        # Reset lists storing evaluation data\n",
    "        self.eval_rts = []\n",
    "        self.eval_Wts = []\n",
    "        self.eval_wts = []\n",
    "        self.eval_vts = []\n",
    "        self.eval_vt_opts = []\n",
    "        self.eval_CS_stim = []\n",
    "        self.eval_US_stim = []\n",
    "        self.eval_loss = []\n",
    "\n",
    "        # Interval time vector\n",
    "        time_int = torch.arange(0, T_int + dt / 10, dt)\n",
    "        t_len = time_int.shape[0]\n",
    "        n_int = len(trial_ls)\n",
    "\n",
    "        # Initialize the KC-MBON weights and plasticity variable\n",
    "        W_in = None\n",
    "\n",
    "        # For each function in the list, run an interval\n",
    "        # All intervals together compose a single trial\n",
    "        for trial in range(n_trial):\n",
    "            # Lists to store activities, weights, readouts and target valences\n",
    "            rts = []\n",
    "            Wts = []\n",
    "            wts = []\n",
    "            vts = []\n",
    "            vt_opts = []\n",
    "            time_CS = []\n",
    "            time_US = []\n",
    "\n",
    "            # Determine whether to reset KC->MBON weights between trials\n",
    "            if reset_wts or (W_in is None):\n",
    "                W_in = self.init_w_kc_mbon(None, n_batch, (trial, n_trial))\n",
    "            else:\n",
    "                W_in = (W_in[0][-1].detach(), W_in[1][-1].detach())\n",
    "\n",
    "            # Generate odors and context (odor = KC = CS, context = ext = US)\n",
    "            r_kc0, r_ext0 = self.gen_r_kc_ext(n_batch, **kwargs)\n",
    "            trial_odors = [r_kc0]\n",
    "            r_in = ([r_kc0], r_ext0)\n",
    "\n",
    "            # Store the max number of CS stimuli across all intervals\n",
    "            max_num_CS = 1\n",
    "            max_num_US = 1\n",
    "\n",
    "            for i in range(n_int):\n",
    "                # Calculate the CS stimulus presentation times\n",
    "                st_times, st_len = gen_int_times(n_batch, dt, T_stim, **kwargs)\n",
    "\n",
    "                # Select the interval function to run\n",
    "                int_fnc = trial_ls[i]\n",
    "                # Calculate the interval inputs\n",
    "                f_in = int_fnc(t_len, st_times, st_len, r_in, n_batch,\n",
    "                               T_stim=T_stim, dt=dt, **kwargs)\n",
    "                r_in, r_kct, r_extt, stim_ls, vt_opt = f_in\n",
    "\n",
    "                # Run the forward pass\n",
    "                net_out = self(r_kct, r_extt, time_int, n_batch, W_in)\n",
    "                rt_int, (Wt_int, wt_int), vt_int = net_out\n",
    "                # Pass the KC->MBON weights to the next interval\n",
    "                W_in = (Wt_int[-1], wt_int[-1])\n",
    "\n",
    "                # Append the interval outputs to lists\n",
    "                rts += rt_int\n",
    "                Wts += Wt_int\n",
    "                wts += wt_int\n",
    "                vts += vt_int\n",
    "                vt_opts.append(vt_opt)\n",
    "                time_CS.append(stim_ls[0])\n",
    "                time_US.append(stim_ls[1])\n",
    "\n",
    "                # Update max number of CS\n",
    "                max_num_CS = max(max_num_CS, len(stim_ls[0]))\n",
    "                max_num_US = max(max_num_US, len(stim_ls[1]))\n",
    "\n",
    "            # # If the odors are not static for all trials, save odors\n",
    "            # if not self.static_odors:\n",
    "            #     # self.eval_odors.append(r_kc)\n",
    "            #     self.eval_odors.append(trial_odors)\n",
    "\n",
    "            # Convert the lists of time point values to a tensor,\n",
    "            # time is the last dimension\n",
    "            self.eval_rts.append(torch.stack(rts, dim=-1).detach())\n",
    "            self.eval_Wts.append(torch.stack(Wts, dim=-1).detach())\n",
    "            self.eval_wts.append(torch.stack(wts, dim=-1).detach())\n",
    "            self.eval_vts.append(torch.stack(vts, dim=-1).detach())\n",
    "            self.eval_vt_opts.append(torch.cat(vt_opts, dim=-1).detach())\n",
    "\n",
    "            # TODO: This is messy, clean up this storage method\n",
    "            # Concatenate time lists to store\n",
    "            trial_CSs = []\n",
    "            for i in range(max_num_CS):\n",
    "                CS_vec = torch.zeros(n_batch, t_len * n_int)\n",
    "                for j in range(n_int):\n",
    "                    try:\n",
    "                        CS_vec[:, j * t_len:(j + 1) * t_len] = time_CS[j][i]\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                trial_CSs.append(CS_vec)\n",
    "\n",
    "            trial_USs = []\n",
    "            for i in range(max_num_US):\n",
    "                US_vec = torch.zeros(n_batch, t_len * n_int)\n",
    "                for j in range(n_int):\n",
    "                    try:\n",
    "                        US_vec[:, j * t_len:(j + 1) * t_len] = time_US[j][i]\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                trial_USs.append(US_vec)\n",
    "\n",
    "            # Store the time series lists\n",
    "            self.eval_CS_stim.append(trial_CSs)\n",
    "            self.eval_US_stim.append(trial_USs)\n",
    "\n",
    "            # TODO: I think the loss function fails for batch sizes of 1\n",
    "            # Calculate the loss\n",
    "            # print(self.eval_vts[-1].shape)\n",
    "            # print(self.eval_vt_opts[-1].shape)\n",
    "            # print(self.eval_rts[-1][:, -self.n_dan:, :].shape)\n",
    "            loss = cond_loss(self.eval_vts[-1], self.eval_vt_opts[-1],\n",
    "                             self.eval_rts[-1][:, -self.n_dan:, :])\n",
    "            self.eval_loss.append(loss.item())\n",
    "\n",
    "    def gen_r_kc_ext(self, n_batch, pos_val=None, **kwargs):\n",
    "        \"\"\" Generates neuron activations for context and odor inputs.\n",
    "\n",
    "        Parameters\n",
    "            n_batch = number of trials in eval-batch\n",
    "            pos_vt (kwarg) = indicates whether valence should be positive\n",
    "                             None: random valence\n",
    "                             True: positive valence\n",
    "                             False: negative valence\n",
    "        \"\"\"\n",
    "\n",
    "        # Determine the contextual input (r_ext)\n",
    "        if pos_val is None:\n",
    "            r_ext = torch.multinomial(torch.ones(n_batch, self.n_ext),\n",
    "                                      self.n_ext)\n",
    "        elif pos_val:\n",
    "            r_ext = torch.tensor([1, 0]).repeat(n_batch, 1)\n",
    "        elif not pos_val:\n",
    "            r_ext = torch.tensor([0, 1]).repeat(n_batch, 1)\n",
    "        else:\n",
    "            raise Exception('Not a valid value for pos_val')\n",
    "\n",
    "        # Determine odor input (r_kc)\n",
    "        r_kc = torch.zeros(n_batch, self.n_kc)\n",
    "        for b in range(n_batch):\n",
    "            # Define an odor (CS) for each trial\n",
    "            if self.train_odors is not None:\n",
    "                n_odors = self.train_odors.shape[0]\n",
    "                odor_select = torch.randint(n_odors, (1,))\n",
    "                r_kc_inds = self.train_odors[odor_select, :]\n",
    "            else:\n",
    "                r_kc_inds = torch.multinomial(torch.ones(self.n_kc), self.n_ones)\n",
    "            r_kc[b, r_kc_inds] = 1\n",
    "\n",
    "        return r_kc, r_ext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedCondRNN(FirstOrderCondRNN):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Set the number of task intervals\n",
    "        self.n_int = 3\n",
    "\n",
    "    def gen_inputs(self, T_vars, n_batch, p_omit=0.5):\n",
    "        \"\"\" Generates inputs for extinction and second-order tasks.\n",
    "\n",
    "        Trials are either extinction or second-order conditioning. No strictly\n",
    "        first-order conditioning trials are included. In half of trials, CS or US\n",
    "        are omitted (pg 28 of Jiang 2020) to prevent over-fitting. Of the trials\n",
    "        where CS or US is omitted, a second parameter determines the relative\n",
    "        fractions of CS or US trials omitted (p_omit_CS).\n",
    "\n",
    "        There are no explicit first-order conditioning tasks included, since\n",
    "        first-order conditioning is a necessary part of both extinction and\n",
    "        second-order conditioning. See Figure 2 of Jiang 2020 to determine\n",
    "        sequencing of stimuli during training. To account for the sequential\n",
    "        nature of numerical simulations, the target valence is set to begin one\n",
    "        time step after stimulus onset.\n",
    "\n",
    "        The mix of conditions is listed as follows:\n",
    "            probability of extinction trials = p_extinct = 0.5\n",
    "            probability of second-order conditioning trials = 1 - p_extinct = 0.5\n",
    "            probability of control (US omitted = CS-) trials = p_omit * 0.3\n",
    "            probability of control (CS omitted) trials = p_omit * 0.7\n",
    "        Note: extinction and second-order trials overlap arbitrarily with\n",
    "              control trials\n",
    "\n",
    "        Parameters\n",
    "            T_vars: Tuple\n",
    "                T_vars[0] = T_int = length of trial (in seconds)\n",
    "                T_vars[1] = T_stim = length of time each stimulus is presented\n",
    "                T_vars[2] = dt = time step of simulations\n",
    "                T_vars[3] = time_len = size of time vector\n",
    "            n_batch = number of trials in mini-batch\n",
    "            p_omit = probability of omitting either CS or US from trials\n",
    "\n",
    "        Returns\n",
    "            r_kct_ls = odor (KC) input time series arrays for each interval\n",
    "            r_extt_ls = context (ext) input time series arrays for each interval\n",
    "            vt_opt = target valence for plotting and loss calculations\n",
    "            ls_stims = stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the range over which stimuli can be presented\n",
    "        T_range = (5, 15)\n",
    "        # Set the time variables\n",
    "        T_stim, dt, time_len = T_vars[1:]\n",
    "\n",
    "        # Generate odors and context signals for each trial\n",
    "        r_kc1, r_ext = self.gen_r_kc_ext(n_batch)\n",
    "        r_kc2, _ = self.gen_r_kc_ext(n_batch)\n",
    "\n",
    "        # Determine whether trials are extinction or second-order\n",
    "        p_extinct = 0.5\n",
    "        extinct_inds = torch.rand(n_batch) < p_extinct\n",
    "\n",
    "        # Determine whether CS or US are randomly omitted\n",
    "        omit_inds = torch.rand(n_batch) < p_omit\n",
    "        # If omitted, determine which one is omitted\n",
    "        p_omit_CS = 0.7\n",
    "        x_omit_CS = torch.rand(n_batch)\n",
    "        omit_CS_inds = torch.logical_and(omit_inds, x_omit_CS < p_omit_CS)\n",
    "        omit_US_inds = torch.logical_and(omit_inds, x_omit_CS > p_omit_CS)\n",
    "\n",
    "        # Initialize lists to store inputs and target valence\n",
    "        r_kct_ls = []\n",
    "        r_extt_ls = []\n",
    "        vals = []\n",
    "        ls_CS1 = []\n",
    "        ls_CS2 = []\n",
    "        ls_US = []\n",
    "\n",
    "        # For each interval\n",
    "        for i in range(self.n_int):\n",
    "            # Define a binary CS and US time series to mulitply the inputs by\n",
    "            time_CS1 = torch.zeros(n_batch, time_len)\n",
    "            time_CS2 = torch.zeros_like(time_CS1)\n",
    "            time_US = torch.zeros_like(time_CS1)\n",
    "            # Define the target valences\n",
    "            val_int = torch.zeros_like(time_CS1)\n",
    "\n",
    "            # Calculate the stimulus presentation times and length\n",
    "            st_times, st_len = gen_int_times(n_batch, dt, T_stim, T_range)\n",
    "\n",
    "            # Set the inputs for each trial\n",
    "            for b in range(n_batch):\n",
    "                stim_inds = st_times[b] + torch.arange(st_len)\n",
    "                # Set the inputs for extinction trials\n",
    "                if extinct_inds[b]:\n",
    "                    # Set the CS input times\n",
    "                    if not omit_CS_inds[b]:\n",
    "                        time_CS1[b, stim_inds] = 1\n",
    "                    # Set the US input times\n",
    "                    if i == 0 and not omit_US_inds[b]:\n",
    "                        time_US[b, stim_inds + st_len] = 1\n",
    "                    # Set the target valence times\n",
    "                    if i > 0 and not omit_inds[b]:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            val_int[b, (stim_inds + 1)] = 1 / i\n",
    "                        else:\n",
    "                            val_int[b, (stim_inds + 1)] = -1 / i\n",
    "                # Set the inputs for second-order conditioning trials\n",
    "                else:\n",
    "                    # Set the CS1 and CS2 input times\n",
    "                    if not omit_CS_inds[b]:\n",
    "                        if i == 0:\n",
    "                            time_CS1[b, stim_inds] = 1\n",
    "                        if i == 1:\n",
    "                            time_CS1[b, stim_inds + st_len] = 1\n",
    "                            time_CS2[b, stim_inds] = 1\n",
    "                        if i == 2:\n",
    "                            time_CS2[b, stim_inds] = 1\n",
    "                    # Set the US input times\n",
    "                    if i == 0 and not omit_US_inds[b]:\n",
    "                        time_US[b, stim_inds + st_len] = 1\n",
    "                    # Set the target valence times\n",
    "                    if i > 0 and not omit_inds[b]:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            val_int[b, (stim_inds + (i % 2) * st_len + 1)] = 1\n",
    "                        else:\n",
    "                            val_int[b, (stim_inds + (i % 2) * st_len + 1)] = -1\n",
    "\n",
    "            # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "            r_kct = torch.einsum('bm, mbt -> bmt', r_kc1,\n",
    "                                 time_CS1.repeat(self.n_kc, 1, 1))\n",
    "            r_kct += torch.einsum('bm, mbt -> bmt', r_kc2,\n",
    "                                  time_CS2.repeat(self.n_kc, 1, 1))\n",
    "            r_extt = torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                                  time_US.repeat(self.n_ext, 1, 1))\n",
    "\n",
    "            r_kct_ls.append(r_kct)\n",
    "            r_extt_ls.append(r_extt)\n",
    "            vals.append(val_int)\n",
    "            ls_CS1 += time_CS1\n",
    "            ls_CS2 += time_CS2\n",
    "            ls_US += time_US\n",
    "\n",
    "        # Concatenate target valences\n",
    "        vt_opt = torch.cat((vals[0], vals[1], vals[2]), dim=-1)\n",
    "\n",
    "        # Make a list of stimulus times to plot\n",
    "        ls_stims = [torch.cat(ls_CS1), torch.cat(ls_US), torch.cat(ls_CS2)]\n",
    "\n",
    "        return r_kct_ls, r_extt_ls, vt_opt, ls_stims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPlasticityRNN(FirstOrderCondRNN):\n",
    "    def __init__(self, *, n_odors=10):\n",
    "        super().__init__()\n",
    "        # Set the static KC->MBON weights\n",
    "        W_kc_mbon_max = 0.05\n",
    "        self.W_kc_mbon = Variable(torch.rand(self.n_mbon, self.n_kc) *\n",
    "                                  W_kc_mbon_max, requires_grad=False)\n",
    "        # Generate a static list of odors for the network to train on\n",
    "        self.train_odors = torch.multinomial(torch.ones(n_odors, self.n_kc),\n",
    "                                             self.n_ones)\n",
    "        # Set the number of task intervals\n",
    "        self.n_int = 1\n",
    "\n",
    "    def wt_update(self, W_kc_mbon, wt, dt, r_bar_kc, r_bar_dan, r_kc, r_dan,\n",
    "                  n_batch, **kwargs):\n",
    "        \"\"\" Returns directly the static KC->MBON plasticity variables\n",
    "\n",
    "        Since this class has no plasticity, the KC->MBON weights and plasticity\n",
    "        variables are not updated. Therefore, the values are returned directly.\n",
    "\n",
    "        Parameters\n",
    "            W_kc_MBON: list = KC->MBON weight matrices\n",
    "            wt = dynamic plasticity update\n",
    "            dt = time step of simulation\n",
    "            r_bar_kc = eligibility trace of Kenyon cell activity\n",
    "            r_bar_dan = eligibility trace of dopaminergic cell activity\n",
    "            r_kc = current activity of Kenyon cells\n",
    "            r_dan = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "        \"\"\"\n",
    "\n",
    "        return r_bar_kc, r_bar_dan\n",
    "\n",
    "    def gen_inputs(self, T_vars, n_batch, **kwargs):\n",
    "        \"\"\" Generates inputs for task without KC->MBON plasticity.\n",
    "\n",
    "        All trials are CS+ or control trials where CS+ is switched out for a\n",
    "        neutral CS in the second presentation (CS- trials). In the case where the\n",
    "        CS is switched, the target valence is zero. To account for the sequential\n",
    "        nature of numerical simulations, the target valence is set to begin one\n",
    "        time step after stimulus onset.\n",
    "\n",
    "        The mix of conditions is listed as follows:\n",
    "            probability of trials where CS+ is switched = 0.5\n",
    "\n",
    "        Parameters\n",
    "            T_vars: Tuple\n",
    "                T_vars[0] = T_int = length of trial (in seconds)\n",
    "                T_vars[1] = T_stim = length of time each stimulus is presented\n",
    "                T_vars[2] = dt = time step of simulations\n",
    "                T_vars[3] = time_len = size of time vector\n",
    "            n_batch = number of trials in mini-batch\n",
    "            p_omit = probability of omitting either CS or US from trials\n",
    "\n",
    "        Returns\n",
    "            r_kct_ls = odor (KC) input time series arrays for each interval\n",
    "            r_extt_ls = context (ext) input time series arrays for each interval\n",
    "            vt_opt = target valence for plotting and loss calculations\n",
    "            ls_stims = stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the range over which stimuli can be presented\n",
    "        T_range = [(5, 15), (20, 30)]\n",
    "        # Set the time variables\n",
    "        T_stim, dt, time_len = T_vars[1:]\n",
    "\n",
    "        # Generate odors and context signals for each trial\n",
    "        r_kc, r_ext = self.gen_r_kc_ext(n_batch, **kwargs)\n",
    "\n",
    "        # Determine whether CS2+ is switched (switch on half of trials)\n",
    "        switch_inds = torch.rand(n_batch) < 0.5\n",
    "\n",
    "        # Initialize activity matrices\n",
    "        r_kct = torch.zeros(n_batch, self.n_kc, time_len)\n",
    "        r_extt = torch.zeros(n_batch, self.n_ext, time_len)\n",
    "        time_CS_both = torch.zeros(n_batch, time_len)\n",
    "        time_US = torch.zeros_like(time_CS_both)\n",
    "        vt_opt = torch.zeros_like(time_CS_both)\n",
    "\n",
    "        # For each stimulus presentation\n",
    "        for i in range(2):\n",
    "            # Initialize time matrices\n",
    "            time_CS = torch.zeros(n_batch, time_len)\n",
    "\n",
    "            # Calculate the stimulus presentation times and length\n",
    "            st_times, st_len = gen_int_times(n_batch, dt, T_stim, T_range[i])\n",
    "\n",
    "            for b in range(n_batch):\n",
    "                stim_inds = st_times[b] + torch.arange(st_len)\n",
    "                # Set the CS time\n",
    "                time_CS[b, stim_inds] = 1\n",
    "                # Set the US time\n",
    "                if i == 0:\n",
    "                    time_US[b, stim_inds + st_len] = 1\n",
    "                # Set the CS+/CS2 and target valence times\n",
    "                if i == 1:\n",
    "                    # Switch the odor in half the trials (target valence is zero)\n",
    "                    if switch_inds[b]:\n",
    "                        # CS2_inds = torch.multinomial(torch.ones(self.n_kc),\n",
    "                        #                              self.n_ones)\n",
    "                        # r_kc[b, CS2_inds] = 1\n",
    "                        CS2_inds = torch.multinomial(torch.ones(self.n_kc),\n",
    "                                                     self.n_kc)\n",
    "                        r_kc[b, :] = r_kc[b, CS2_inds]\n",
    "                    # If the odor is not switched, set the target valence\n",
    "                    else:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            vt_opt[b, (stim_inds + 1)] = 1\n",
    "                        else:\n",
    "                            vt_opt[b, (stim_inds + 1)] = -1\n",
    "\n",
    "            # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "            r_kct += torch.einsum('bm, mbt -> bmt', r_kc,\n",
    "                                  time_CS.repeat(self.n_kc, 1, 1))\n",
    "            r_extt += torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                                   time_US.repeat(self.n_ext, 1, 1))\n",
    "            time_CS_both += time_CS\n",
    "\n",
    "        # Make a list of stimulus times to plot\n",
    "        ls_stims = [time_CS_both, time_US]\n",
    "\n",
    "        return [r_kct], [r_extt], vt_opt, ls_stims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinualRNN(FirstOrderCondRNN):\n",
    "    def __init__(self, *, n_trial_odors=4):\n",
    "        super().__init__()\n",
    "        # Set the number of task intervals\n",
    "        self.n_int = 1\n",
    "        # Set the number of odors to train over on each trial\n",
    "        self.n_trial_odors = n_trial_odors\n",
    "        # Add a non-specific potentiation parameter\n",
    "        self.beta = nn.Parameter(torch.ones(self.n_kc) * 0.01,\n",
    "                                 requires_grad=True)\n",
    "\n",
    "    def calc_dw(self, r_bar_kc, r_bar_dan, r_kc, r_dan, n_batch, nps=True,\n",
    "                **kwargs):\n",
    "        \"\"\" Calculates the dynamic weight update (see Eq 4).\n",
    "\n",
    "        Parameters\n",
    "            r_bar_kc = eligibility trace of Kenyon cell activity\n",
    "            r_bar_dan = eligibility trace of dopaminergic cell activity\n",
    "            r_kc = current activity of Kenyon cells\n",
    "            r_dan = current activity of dopamine cells\n",
    "            n_batch = number of trials in mini-batch\n",
    "            nps = indicates whether non-specific potentiation is included\n",
    "\n",
    "        Returns\n",
    "            dw = increment of dynamic plasticity variable wt\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the LTD/LTP terms\n",
    "        prod1 = torch.einsum('bd, bk -> bdk', r_bar_dan, r_kc)\n",
    "        prod2 = torch.einsum('bd, bk -> bdk', r_dan, r_bar_kc)\n",
    "\n",
    "        # Include non-specific potentiation (unless control condition)\n",
    "        if nps:\n",
    "            # Rectify the potentiation parameter\n",
    "            beta = F.relu(self.beta.clone())\n",
    "            # Constrain the potentiation parameter to be positive\n",
    "            prod3 = torch.einsum('bd, bk -> bdk', r_bar_dan,\n",
    "                                 beta.repeat(n_batch, 1))\n",
    "        else:\n",
    "            prod3 = torch.zeros_like(prod2)\n",
    "\n",
    "        return prod1 - prod2 + prod3\n",
    "\n",
    "    def init_w_kc_mbon(self, W_in, n_batch, e_tup):\n",
    "        \"\"\" Initializes the KC->MBON weights for the task.\n",
    "\n",
    "        KC->MBON weights are reset at the beginning of each epoch.\n",
    "\n",
    "        Parameters\n",
    "            W0 = specified initial weight values or None\n",
    "            n_batch = number of trials in mini-batch\n",
    "            e_tup: tuple = (current epoch, total training epochs)\n",
    "\n",
    "        Returns\n",
    "            W_kc_MBON: list = initial KC->MBON weight matrix\n",
    "            wt = initial dynamic plasticity update\n",
    "        \"\"\"\n",
    "\n",
    "        wt0 = torch.ones(n_batch, self.n_mbon, self.n_kc) * self.kc_mbon_max\n",
    "        if W_in is None:\n",
    "            W_in = (wt0.clone(), wt0.clone())\n",
    "        # Calculate the saturation parameter and modify initial weights\n",
    "        x_sat = min(1, (e_tup[0] / (e_tup[1] / 2)))\n",
    "        Wt = W_in[0]\n",
    "        wt = (1 - x_sat) * wt0 + x_sat * W_in[1]\n",
    "        W_in = (Wt, wt)\n",
    "\n",
    "        return W_in\n",
    "\n",
    "    def gen_inputs(self, T_vars, n_batch, **kwargs):\n",
    "        \"\"\" Generates inputs for continual learning task.\n",
    "\n",
    "        Trials consist of the presentation of four different odors, with\n",
    "        presentation times drawn from a Poisson distribution with a mean of two.\n",
    "        The first two odors are conditioned stimuli, with the first\n",
    "        corresponding to a positive valence (approach behaviour) and the second\n",
    "        corresponding to a negative valence (avoidance behaviour). The last two\n",
    "        are neutral odors and have zero associated valence. The conditioned\n",
    "        stimuli are trained to respond (i.e. have a non-zero target valence) to\n",
    "        each presentation of the odor AFTER the first.\n",
    "\n",
    "        Parameters\n",
    "            T_vars: Tuple\n",
    "                T_vars[0] = T_int = length of trial (in seconds)\n",
    "                T_vars[1] = T_stim = length of time each stimulus is presented\n",
    "                T_vars[2] = dt = time step of simulations\n",
    "                T_vars[3] = time_len = size of time vector\n",
    "            n_batch = number of trials in mini-batch\n",
    "\n",
    "        Returns\n",
    "            r_kct_ls = odor (KC) input time series arrays for each interval\n",
    "            r_extt_ls = context (ext) input time series arrays for each interval\n",
    "            vt_opt = target valence for plotting and loss calculations\n",
    "            ls_stims = stimulus time series for plotting\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the time variables\n",
    "        T_int, T_stim, dt, time_len = T_vars\n",
    "        # Average number of stimulus presentations\n",
    "        st_mean = 2\n",
    "        # Calculate the stimulus presentation times and length\n",
    "        st_times, st_len = gen_cont_times(n_batch, dt, T_stim, T_int, st_mean,\n",
    "                                          self.n_trial_odors)\n",
    "\n",
    "        # Initialize activity matrices\n",
    "        r_kct = torch.zeros(n_batch, self.n_kc, time_len)\n",
    "        r_extt = torch.zeros(n_batch, self.n_ext, time_len)\n",
    "\n",
    "        # Initialize lists and arrays to store stimulus time series\n",
    "        ls_CS = []\n",
    "        time_US_all = torch.zeros(n_batch, time_len)\n",
    "        vt_opt = torch.zeros(n_batch, time_len)\n",
    "\n",
    "        # For each batch, randomly generate different odors and presentation times\n",
    "        for i in range(self.n_trial_odors):\n",
    "            # Initialize the CS time matrix\n",
    "            time_CS = torch.zeros(n_batch, time_len)\n",
    "            time_US = torch.zeros_like(time_CS)\n",
    "\n",
    "            # Generate odors and context signals for each trial\n",
    "            if i == 0:\n",
    "                r_kc, r_ext = self.gen_r_kc_ext(n_batch, pos_val=True)\n",
    "            elif i == 1:\n",
    "                r_kc, r_ext = self.gen_r_kc_ext(n_batch, pos_val=False)\n",
    "            else:\n",
    "                r_kc, r_ext = self.gen_r_kc_ext(n_batch)\n",
    "                r_ext = torch.tensor([0, 0]).repeat(n_batch, 1)\n",
    "\n",
    "            # For each trial\n",
    "            for b in range(n_batch):\n",
    "                for j, st in enumerate(st_times[i][b]):\n",
    "                    # Set the CS input times\n",
    "                    stim_inds = st + torch.arange(st_len)\n",
    "                    time_CS[b, stim_inds] = 1\n",
    "\n",
    "                    # For CS+ odors, set US and the valence\n",
    "                    if i < (self.n_trial_odors / 2):\n",
    "                        # Set the US input times\n",
    "                        time_US[b, (stim_inds + st_len)] = 1\n",
    "                        # Set target valence on every presentation but the first\n",
    "                        if j > 0:\n",
    "                            if r_ext[b, 0] == 1:\n",
    "                                vt_opt[b, (stim_inds + 1)] = 1\n",
    "                            else:\n",
    "                                vt_opt[b, (stim_inds + 1)] = -1\n",
    "\n",
    "            # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "            r_kct += torch.einsum('bm, mbt -> bmt', r_kc,\n",
    "                                  time_CS.repeat(self.n_kc, 1, 1))\n",
    "            r_extt += torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                                   time_US.repeat(self.n_ext, 1, 1))\n",
    "            ls_CS += time_CS\n",
    "            time_US_all += time_US\n",
    "\n",
    "        # Make a list of stimulus times to plot\n",
    "        ls_stims = ls_CS + [time_US_all]\n",
    "\n",
    "        return [r_kct], [r_extt], vt_opt, ls_stims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_loss(vt, vt_opt, r_DAN, lam=0.1):\n",
    "    \"\"\" Calculates the loss for conditioning tasks.\n",
    "\n",
    "    Composed of an MSE cost based on the difference between output and\n",
    "    target valence, and a regularization cost that penalizes excess\n",
    "    dopaminergic activity. Reference Eqs. (3) and (9) in Jiang 2020.\n",
    "\n",
    "    Parameters\n",
    "        vt = time dependent valence output of network\n",
    "        vt_opt = target valence (must be a torch tensor)\n",
    "        r_DAN = time series of dopaminergic neuron activities\n",
    "        lam = regularization constant\n",
    "\n",
    "    Returns\n",
    "        loss_tot = scalar loss used in backprop\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the baseline DAN activity\n",
    "    DAN_baseline = 0.1\n",
    "\n",
    "    # Calculate the MSE loss of the valence\n",
    "    v_sum = torch.mean((vt - vt_opt) ** 2, dim=1)\n",
    "    v_loss = torch.mean(v_sum)\n",
    "\n",
    "    # Calculate regularization term\n",
    "    r_sum = torch.sum(F.relu(r_DAN - DAN_baseline) ** 2, dim=1)\n",
    "    r_loss = torch.mean(r_sum, dim=1) * lam\n",
    "\n",
    "    # Calculate the summed loss (size = n_batch)\n",
    "    loss = v_loss + r_loss\n",
    "\n",
    "    # Average the loss over all batches\n",
    "    loss_tot = torch.mean(loss)\n",
    "\n",
    "    return loss_tot\n",
    "\n",
    "\n",
    "def first_order_cond_csp(t_len, st_times, st_len, r_in, n_batch, **kwargs):\n",
    "    \"\"\" Runs a first-order conditioning interval (CS+ and US).\n",
    "\n",
    "    Parameters\n",
    "        t_len = length of task interval (in indices)\n",
    "        st_times = array of stimulus presentation times for each trial\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "        r_in = input neuron activations (r_kc and r_ext)\n",
    "        n_batch = number of trials in batch\n",
    "        p_ctrl (kwarg) = fraction of control trials during training\n",
    "        p_csm (kwarg) = fraction of control trials that are CS-\n",
    "    \"\"\"\n",
    "\n",
    "    # Set odors and context signals for this interval\n",
    "    r_kcs, r_ext = r_in\n",
    "    r_kc = r_kcs[0]\n",
    "    n_kc = r_kc.shape[1]\n",
    "    n_ext = r_ext.shape[1]\n",
    "\n",
    "    # Initialize stimulus time matrices\n",
    "    time_CS = torch.zeros(n_batch, t_len)\n",
    "    time_US = torch.zeros_like(time_CS)\n",
    "    vt_opt = torch.zeros_like(time_CS)\n",
    "\n",
    "    # Set the stimulus step inputs\n",
    "    for b in range(n_batch):\n",
    "        # Convert stimulus time into range of indices\n",
    "        stim_inds = st_times[b] + torch.arange(st_len)\n",
    "        # Set the CS input times\n",
    "        time_CS[b, stim_inds] = 1\n",
    "        # Set the US input times\n",
    "        time_US[b, (stim_inds + st_len)] = 1\n",
    "\n",
    "    # Calculate the input neurons' activity time series (KC = CS, ext = US)\n",
    "    r_kct = torch.einsum('bm, mbt -> bmt', r_kc,\n",
    "                         time_CS.repeat(n_kc, 1, 1))\n",
    "    r_extt = torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                          time_US.repeat(n_ext, 1, 1))\n",
    "\n",
    "    # Combine the time matrices into a list\n",
    "    stim_ls = [[time_CS], time_US]\n",
    "\n",
    "    return r_in, r_kct, r_extt, stim_ls, vt_opt\n",
    "\n",
    "\n",
    "def first_order_csm(t_len, st_times, st_len, r_in, n_batch, **kwargs):\n",
    "    \"\"\" Runs a first-order conditioning interval (CS- alone).\n",
    "\n",
    "    This function can be used for both the conditioning and test intervals\n",
    "    of CS- trials. Only the CS stimulus is presented.\n",
    "\n",
    "    Parameters\n",
    "        t_len = length of task interval (in indices)\n",
    "        st_times = array of stimulus presentation times for each trial\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "        r_in = input neuron activations (r_kc and r_ext)\n",
    "        n_batch = number of trials in batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Set odors and context signals for each trial\n",
    "    r_kcs, r_ext = r_in\n",
    "    r_kc = r_kcs[0]\n",
    "    n_kc = r_kc.shape[1]\n",
    "    n_ext = r_ext.shape[1]\n",
    "\n",
    "    # Initialize stimulus time matrices\n",
    "    time_CS = torch.zeros(n_batch, t_len)\n",
    "    time_US = torch.zeros_like(time_CS)\n",
    "    vt_opt = torch.zeros_like(time_CS)\n",
    "\n",
    "    # Set the stimulus step inputs\n",
    "    for b in range(n_batch):\n",
    "        # Convert stimulus time into range of indices\n",
    "        stim_inds = st_times[b] + torch.arange(st_len)\n",
    "        # Set the CS input times\n",
    "        time_CS[b, stim_inds] = 1\n",
    "\n",
    "    # Calculate the input neurons' activity time series (KC = CS, ext = US)\n",
    "    r_kct = torch.einsum('bm, mbt -> bmt', r_kc,\n",
    "                         time_CS.repeat(n_kc, 1, 1))\n",
    "    r_extt = torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                          time_US.repeat(n_ext, 1, 1))\n",
    "\n",
    "    # Combine the time matrices into a list\n",
    "    stim_ls = [[time_CS], time_US]\n",
    "\n",
    "    return r_in, r_kct, r_extt, stim_ls, vt_opt\n",
    "\n",
    "\n",
    "def first_order_test(t_len, st_times, st_len, r_in, n_batch, **kwargs):\n",
    "    \"\"\" Runs a first-order test interval (CS+ and target valence).\n",
    "\n",
    "    This function can be used for both first--order tests and extinction\n",
    "    conditioning. This interval must follow an interval with a single odor.\n",
    "    i.e. the input r_kc must be of size (n_batch, n_kc)\n",
    "\n",
    "    Parameters\n",
    "        t_len = length of task interval (in indices)\n",
    "        st_times = array of stimulus presentation times for each trial\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "        r_in = input neuron activations (r_kc and r_ext)\n",
    "        n_batch = number of trials in batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Set odors and context signals for each trial\n",
    "    r_kcs, r_ext = r_in\n",
    "    r_kc = r_kcs[0]\n",
    "    n_kc = r_kc.shape[1]\n",
    "    n_ext = r_ext.shape[1]\n",
    "\n",
    "    # Initialize stimulus time matrices\n",
    "    time_CS = torch.zeros(n_batch, t_len)\n",
    "    time_US = torch.zeros_like(time_CS)\n",
    "    vt_opt = torch.zeros_like(time_CS)\n",
    "\n",
    "    # Set the stimulus step inputs\n",
    "    for b in range(n_batch):\n",
    "        # Convert stimulus time into range of indices\n",
    "        stim_inds = st_times[b] + torch.arange(st_len)\n",
    "        # Set the CS input times\n",
    "        time_CS[b, stim_inds] = 1\n",
    "        # Set the target valence times\n",
    "        if r_ext[b, 0] == 1:\n",
    "            vt_opt[b, (stim_inds + 1)] = 1\n",
    "        else:\n",
    "            vt_opt[b, (stim_inds + 1)] = -1\n",
    "\n",
    "    # Calculate the input neurons' activity time series (KC = CS, ext = US)\n",
    "    r_kct = torch.einsum('bm, mbt -> bmt', r_kc,\n",
    "                         time_CS.repeat(n_kc, 1, 1))\n",
    "    r_extt = torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                          time_US.repeat(n_ext, 1, 1))\n",
    "\n",
    "    # Combine the time matrices into a list\n",
    "    stim_ls = [[time_CS], time_US]\n",
    "\n",
    "    return r_in, r_kct, r_extt, stim_ls, vt_opt\n",
    "\n",
    "\n",
    "def extinct_test(t_len, st_times, st_len, r_in, n_batch, **kwargs):\n",
    "    \"\"\" Runs an extinction interval.\n",
    "\n",
    "    Consists of CS+ stimulus presentation without the relevant US. The\n",
    "    extinction test always occurs after a first order test. In this case,\n",
    "    the target valence is half that of a first order (CS+) test.\n",
    "\n",
    "    Parameters\n",
    "        t_len = length of task interval (in indices)\n",
    "        st_times = array of stimulus presentation times for each trial\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "        r_in = input neuron activations (r_kc and r_ext)\n",
    "        n_batch = number of trials in batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Set odors and context signals for each trial\n",
    "    r_kcs, r_ext = r_in\n",
    "    r_kc = r_kcs[0]\n",
    "    n_kc = r_kc.shape[1]\n",
    "    n_ext = r_ext.shape[1]\n",
    "\n",
    "    # Initialize stimulus time matrices\n",
    "    time_CS = torch.zeros(n_batch, t_len)\n",
    "    time_US = torch.zeros_like(time_CS)\n",
    "    vt_opt = torch.zeros_like(time_CS)\n",
    "\n",
    "    # Set the stimulus step inputs\n",
    "    for b in range(n_batch):\n",
    "        # Convert stimulus time into range of indices\n",
    "        stim_inds = st_times[b] + torch.arange(st_len)\n",
    "        # Set the CS input times\n",
    "        time_CS[b, stim_inds] = 1\n",
    "        # Set the target valence times\n",
    "        if r_ext[b, 0] == 1:\n",
    "            vt_opt[b, (stim_inds + 1)] = 1 / 2\n",
    "        else:\n",
    "            vt_opt[b, (stim_inds + 1)] = -1 / 2\n",
    "\n",
    "    # Calculate the input neurons' activity time series (KC = CS, ext = US)\n",
    "    r_kct = torch.einsum('bm, mbt -> bmt', r_kc,\n",
    "                         time_CS.repeat(n_kc, 1, 1))\n",
    "    r_extt = torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                          time_US.repeat(n_ext, 1, 1))\n",
    "\n",
    "    # Combine the time matrices into a list\n",
    "    stim_ls = [[time_CS], time_US]\n",
    "\n",
    "    return r_in, r_kct, r_extt, stim_ls, vt_opt\n",
    "\n",
    "\n",
    "def second_order_cond(t_len, st_times, st_len, r_in, n_batch, **kwargs):\n",
    "    \"\"\" Runs a first-order conditioning interval (CS alone).\n",
    "\n",
    "    Parameters\n",
    "        t_len = length of task interval (in indices)\n",
    "        st_times = array of stimulus presentation times for each trial\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "        r_in = input neuron activations (r_kc and r_ext)\n",
    "        n_batch = number of trials in batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Set odors and context signals for each trial\n",
    "    r_kcs, r_ext = r_in\n",
    "    r_kc1 = r_kcs[0]\n",
    "    n_kc = r_kc1.shape[1]\n",
    "    n_ext = r_ext.shape[1]\n",
    "    # Initialize a second odor\n",
    "    r_kc2 = torch.zeros_like(r_kc1)\n",
    "    n_ones = r_kc1[0, :].sum().int()\n",
    "\n",
    "    # Initialize stimulus time matrices\n",
    "    time_CS1 = torch.zeros(n_batch, t_len)\n",
    "    time_CS2 = torch.zeros_like(time_CS1)\n",
    "    time_US = torch.zeros_like(time_CS1)\n",
    "    vt_opt = torch.zeros_like(time_CS1)\n",
    "\n",
    "    # Set the stimulus step inputs\n",
    "    for b in range(n_batch):\n",
    "        # Shuffle the indices to create a new second odor\n",
    "        new_inds = torch.multinomial(torch.ones(n_ones), n_ones)\n",
    "        r_kc2[b, :] = r_kc1[b, new_inds]\n",
    "\n",
    "        # Convert stimulus time into range of indices\n",
    "        stim_inds = st_times[b] + torch.arange(st_len)\n",
    "        # Set the CS1 input times\n",
    "        time_CS1[b, stim_inds] = 1\n",
    "        # Set the CS2 input times\n",
    "        time_CS2[b, (stim_inds + st_len)] = 1\n",
    "\n",
    "    # Calculate the input neurons' activity time series (KC = CS, ext = US)\n",
    "    r_kct = torch.einsum('bm, mbt -> bmt', r_kc1,\n",
    "                         time_CS1.repeat(n_kc, 1, 1))\n",
    "    r_kct += torch.einsum('bm, mbt -> bmt', r_kc2,\n",
    "                          time_CS2.repeat(n_kc, 1, 1))\n",
    "    r_extt = torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                          time_US.repeat(n_ext, 1, 1))\n",
    "\n",
    "    # Combine the time matrices into a list\n",
    "    time_all_CS = [time_CS1, time_CS2]\n",
    "    stim_ls = [time_all_CS, time_US]\n",
    "    r_next = ([r_kc1, r_kc2], r_ext)\n",
    "\n",
    "    return r_next, r_kct, r_extt, stim_ls, vt_opt\n",
    "\n",
    "\n",
    "def second_order_test(t_len, st_times, st_len, r_in, n_batch, **kwargs):\n",
    "    \"\"\" Runs a first-order test interval (CS+ and target valence).\n",
    "\n",
    "    This function can be used for both first- and second-order tests, as well\n",
    "    as extinction conditioning.\n",
    "\n",
    "    Parameters\n",
    "        t_len = length of task interval (in indices)\n",
    "        st_times = array of stimulus presentation times for each trial\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "        r_in = input neuron activations (r_kc and r_ext)\n",
    "        n_batch = number of trials in batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Set odors and context signals for each trial\n",
    "    r_kcs, r_ext = r_in\n",
    "    r_kc = r_kcs[1]\n",
    "    n_kc = r_kc.shape[1]\n",
    "    n_ext = r_ext.shape[1]\n",
    "\n",
    "    # Initialize stimulus time matrices\n",
    "    time_CS1 = torch.zeros(n_batch, t_len)\n",
    "    time_CS2 = torch.zeros_like(time_CS1)\n",
    "    time_US = torch.zeros_like(time_CS1)\n",
    "    vt_opt = torch.zeros_like(time_CS1)\n",
    "\n",
    "    # Set the stimulus step inputs\n",
    "    for b in range(n_batch):\n",
    "        # Convert stimulus time into range of indices\n",
    "        stim_inds = st_times[b] + torch.arange(st_len)\n",
    "        # Set the CS input times\n",
    "        time_CS2[b, stim_inds] = 1\n",
    "        # Set the target valence times\n",
    "        if r_ext[b, 0] == 1:\n",
    "            vt_opt[b, (stim_inds + 1)] = 1\n",
    "        else:\n",
    "            vt_opt[b, (stim_inds + 1)] = -1\n",
    "\n",
    "    # Calculate the input neurons' activity time series (KC = CS, ext = US)\n",
    "    r_kct = torch.einsum('bm, mbt -> bmt', r_kc,\n",
    "                         time_CS2.repeat(n_kc, 1, 1))\n",
    "    r_extt = torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                          time_US.repeat(n_ext, 1, 1))\n",
    "\n",
    "    # Combine the time matrices into a list\n",
    "    time_all_CS = [time_CS1, time_CS2]\n",
    "    stim_ls = [time_all_CS, time_US]\n",
    "\n",
    "    return r_in, r_kct, r_extt, stim_ls, vt_opt\n",
    "\n",
    "\n",
    "def no_plasticity_trial(t_len, st_times, st_len, r_in, n_batch, T_stim,\n",
    "                        dt, p_ctrl=0., **kwargs):\n",
    "    \"\"\" Runs a full no-plasticity trial (CS+ and US).\n",
    "\n",
    "    In half of the training trials, the second odor is switched to prevent\n",
    "    over-generalization.\n",
    "\n",
    "    Parameters\n",
    "        t_len = length of task interval (in indices)\n",
    "        st_times = array of stimulus presentation times for each trial\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "        r_in = input neuron activations (r_kc and r_ext)\n",
    "        n_batch = number of trials in batch\n",
    "        p_ctrl = fraction of control (generalization) trials\n",
    "    \"\"\"\n",
    "\n",
    "    # Generates a second set of stimulus times for the second presentation\n",
    "    st_times2, _ = gen_int_times(n_batch, dt, T_stim, T_range=(20, 30))\n",
    "\n",
    "    # Set odors and context signals for each trial\n",
    "    r_kcs, r_ext = r_in\n",
    "    r_kc1 = r_kcs[0]\n",
    "    n_kc = r_kc1.shape[1]\n",
    "    n_ext = r_ext.shape[1]\n",
    "    # Define a second set of odors for generalization trials\n",
    "    # r_kc2 = r_kc1.clone()\n",
    "    r_kc2 = torch.zeros_like(r_kc1)\n",
    "    n_ones = r_kc1[0, :].sum().int()\n",
    "\n",
    "    # Initialize stimulus time matrices\n",
    "    time_CS1 = torch.zeros(n_batch, t_len)\n",
    "    time_CS2 = torch.zeros_like(time_CS1)\n",
    "    time_US = torch.zeros_like(time_CS1)\n",
    "    vt_opt = torch.zeros_like(time_CS1)\n",
    "\n",
    "    # Determine whether CS+ is switched for a novel odor\n",
    "    switch_inds = torch.rand(n_batch) < p_ctrl\n",
    "\n",
    "    # Set the stimulus step inputs\n",
    "    for b in range(n_batch):\n",
    "        # Convert stimulus time into range of indices\n",
    "        stim_inds1 = st_times[b] + torch.arange(st_len)\n",
    "        stim_inds2 = st_times2[b] + torch.arange(st_len)\n",
    "        # Set the CS input times\n",
    "        time_CS1[b, stim_inds1] = 1\n",
    "\n",
    "        # If it is a control trial, switch the odor (target valence is zero)\n",
    "        if switch_inds[b]:\n",
    "            time_CS2[b, stim_inds2] = 1\n",
    "            new_inds = torch.multinomial(torch.ones(n_kc), n_ones)\n",
    "            # r_kc2[b, :] = r_kc1[b, new_inds]\n",
    "            r_kc2[b, new_inds] = 1\n",
    "        # If the odor is not switched, set the target valence\n",
    "        else:\n",
    "            time_CS1[b, stim_inds2] = 1\n",
    "            if r_ext[b, 0] == 1:\n",
    "                vt_opt[b, (stim_inds2 + 1)] = 1\n",
    "            else:\n",
    "                vt_opt[b, (stim_inds2 + 1)] = -1\n",
    "\n",
    "    # Calculate the input neurons' activity time series (KC = CS, ext = US)\n",
    "    r_kct = torch.einsum('bm, mbt -> bmt', r_kc1,\n",
    "                         time_CS1.repeat(n_kc, 1, 1))\n",
    "    r_kct += torch.einsum('bm, mbt -> bmt', r_kc2,\n",
    "                          time_CS2.repeat(n_kc, 1, 1))\n",
    "    r_extt = torch.einsum('bm, mbt -> bmt', r_ext,\n",
    "                          time_US.repeat(n_ext, 1, 1))\n",
    "\n",
    "    # Combine the time matrices into a list\n",
    "    time_all_CS = [time_CS1, time_CS2]\n",
    "    stim_ls = [time_all_CS, time_US]\n",
    "    r_next = ([r_kc1, r_kc2], r_ext)\n",
    "\n",
    "    return r_next, r_kct, r_extt, stim_ls, vt_opt\n",
    "\n",
    "\n",
    "def continual_trial(t_len, st_times, st_len, r_in, n_batch, dt, **kwargs):\n",
    "    \"\"\" Runs a continual learning trial (two CS- and two CS+).\n",
    "\n",
    "    Parameters\n",
    "        t_len = length of task interval (in indices)\n",
    "        st_times = array of stimulus presentation times for each trial\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "        r_in = input neuron activations (r_kc and r_ext)\n",
    "        n_batch = number of trials in batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Draw the CS presentation times from a Poisson distribution\n",
    "    st_times, st_len = gen_cont_times(n_batch, dt, **kwargs)\n",
    "    n_odor = len(st_times)\n",
    "\n",
    "    # Set odors and context signals\n",
    "    r_kc, r_ext0 = r_in\n",
    "    r_kc0 = r_kc[0]\n",
    "    # Define the number of Kenyon cells (n_kc) and dim of context (n_ext)\n",
    "    n_kc = r_kc0.shape[1]\n",
    "    n_ext = r_ext0.shape[1]\n",
    "    n_ones = r_kc0[0, :].sum().int()\n",
    "    # Initialize lists to store additional odors and their context signals\n",
    "    r_kcs = [torch.tensor([0])] * n_odor\n",
    "    r_exts = [torch.tensor([0])] * n_odor\n",
    "    # The first US is appetitive, while the second is aversive\n",
    "    r_exts[0] = torch.tensor([1, 0]).repeat(n_batch, 1)\n",
    "    r_exts[1] = torch.tensor([0, 1]).repeat(n_batch, 1)\n",
    "\n",
    "    # Initialize activity matrices\n",
    "    r_kct = torch.zeros(n_batch, n_kc, t_len)\n",
    "    r_extt = torch.zeros(n_batch, n_ext, t_len)\n",
    "\n",
    "    # Initialize stimulus time matrices\n",
    "    # time_CS = torch.zeros(n_odor, n_batch, t_len)\n",
    "    # time_US = torch.zeros_like(time_CS)\n",
    "    time_all_CS = []\n",
    "    time_all_US = []\n",
    "    vt_opt = torch.zeros(n_batch, t_len)\n",
    "\n",
    "    # Set the stimulus step inputs\n",
    "    for i in range(n_odor):\n",
    "        # Initialize time matrices\n",
    "        time_CS = torch.zeros(n_batch, t_len)\n",
    "        time_US = torch.zeros_like(time_CS)\n",
    "        append_US = False\n",
    "        # Set neutral stimulus contexts\n",
    "        r_kcs[i] = torch.zeros_like(r_kc0)\n",
    "        if i > 1:\n",
    "            r_exts[i] = torch.tensor([0, 0]).repeat(n_batch, 1)\n",
    "\n",
    "        for b in range(n_batch):\n",
    "            # Generate odors\n",
    "            # new_kc_inds = torch.multinomial(torch.ones(n_kc), n_kc)\n",
    "            # r_kcs[i][b, :] = r_kc0[b, new_kc_inds]\n",
    "            new_kc_inds = torch.multinomial(torch.ones(n_kc), n_ones)\n",
    "            r_kcs[i][b, new_kc_inds] = 1\n",
    "\n",
    "            for j, st in enumerate(st_times[i][b]):\n",
    "                # Convert stimulus time into range of indices\n",
    "                stim_inds = st + torch.arange(st_len)\n",
    "                # Set the CS input times\n",
    "                time_CS[b, stim_inds] = 1\n",
    "\n",
    "                if i < (n_odor / 2):\n",
    "                    # Set the US input times\n",
    "                    time_US[b, (stim_inds + st_len)] = 1\n",
    "                    append_US = True\n",
    "                    # Set target valence on each presentation after the first\n",
    "                    if j > 0:\n",
    "                        if r_exts[i][b, 0] == 1:\n",
    "                            vt_opt[b, (stim_inds + 1)] = 1\n",
    "                        else:\n",
    "                            vt_opt[b, (stim_inds + 1)] = -1\n",
    "\n",
    "        # Calculate the input neuron activity time series (KC = CS, ext = US)\n",
    "        r_kct += torch.einsum('bm, mbt -> bmt', r_kcs[i],\n",
    "                              time_CS.repeat(n_kc, 1, 1))\n",
    "        r_extt += torch.einsum('bm, mbt -> bmt', r_exts[i],\n",
    "                               time_US.repeat(n_ext, 1, 1))\n",
    "\n",
    "        # Save the CS and US stimuli time series\n",
    "        # time_all_CS[i, :, :] = time_CS\n",
    "        # time_all_US[i, :, :] = time_US\n",
    "        time_all_CS.append(time_CS)\n",
    "        if append_US:\n",
    "            time_all_US.append(time_US)\n",
    "\n",
    "    # Combine the time matrices into a list\n",
    "    stim_ls = [time_all_CS, time_all_US]\n",
    "    r_next = (r_kcs, r_exts)\n",
    "\n",
    "    return r_next, r_kct, r_extt, stim_ls, vt_opt\n",
    "\n",
    "\n",
    "def gen_int_times(n_batch, dt, T_stim, T_range=(5, 15), **kwargs):\n",
    "    \"\"\" Generates an array of stimulus presentation times for all trials\n",
    "\n",
    "    Parameters\n",
    "        dt = time step of simulations\n",
    "        n_batch = number of trials in eval-batch\n",
    "        T_stim = length of time each stimulus is presented\n",
    "        T_range: tuple = range in which stimulus can be presented\n",
    "\n",
    "    Returns\n",
    "        st_times = array of stimulus presentation times for each trial\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "    \"\"\"\n",
    "\n",
    "    # Present the stimulus between 5-15s of each interval\n",
    "    min_ind = int(T_range[0] / dt)\n",
    "    max_ind = int((T_range[1] - 2 * T_stim) / dt)\n",
    "    st_times = torch.randint(min_ind, max_ind, (n_batch,))\n",
    "    st_len = int(T_stim / dt)\n",
    "\n",
    "    return st_times, st_len\n",
    "\n",
    "\n",
    "def gen_cont_times(n_batch, dt, T_stim=2, T_int=200, stim_mean=2, n_odor=4,\n",
    "                   **kwargs):\n",
    "    \"\"\" Generates stimulus presentation times for continual learning trials.\n",
    "\n",
    "    Parameters\n",
    "        dt = time step of simulations\n",
    "        n_batch = number of trials in eval-batch\n",
    "        T_stim = length of time each stimulus is presented\n",
    "        T_int (kwarg) = length of trial (in seconds)\n",
    "        stim_mean (kwarg) = average number of stimulus presentations per trial\n",
    "        n_odor = number of odors in a trial\n",
    "\n",
    "    Returns\n",
    "        st_times = lists of stimulus presentation times for each trial\n",
    "        stim_len = length of stimulus presentation (in indices)\n",
    "    \"\"\"\n",
    "\n",
    "    # Poisson rate of stimulus presentations\n",
    "    stim_rate = stim_mean / T_int\n",
    "\n",
    "    # Initialize stimulus presentation times array\n",
    "    st_times = [torch.tensor([0])] * n_odor\n",
    "    st_len = int(T_stim / dt)\n",
    "\n",
    "    # Generate a list of stimulus presentation times for each trial\n",
    "    for i in range(n_odor):\n",
    "        batch_times = [torch.tensor([0])] * n_batch\n",
    "        for b in range(n_batch):\n",
    "            trial_times = []\n",
    "            last_time = 0\n",
    "            while True:\n",
    "                stim_isi = -torch.log(torch.rand(1)) / stim_rate\n",
    "                next_time = last_time + stim_isi\n",
    "                if next_time < (T_int - 2 * T_stim):\n",
    "                    # Stimulus times are indices (not times)\n",
    "                    trial_times.append((next_time / dt).int())\n",
    "                    last_time += stim_isi\n",
    "                # Ensure at least one presentation of each stimuli\n",
    "                elif last_time == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            batch_times[b] = torch.stack(trial_times)\n",
    "        st_times[i] = batch_times\n",
    "\n",
    "    return st_times, st_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trial(network, trial_ls, plt_ttl, plt_lbl, T_vars, **kwargs):\n",
    "    \"\"\" Plots a figure similar to Figure 2 from Jiang 2020.\n",
    "\n",
    "    Runs the network using a novel combination of stimuli, then prints the\n",
    "    result. Top: time series of the various stimuli (CS and US), as well as\n",
    "    the target valence and readout. Bottom: activity of eight randomly chosen\n",
    "    mushroom body output neurons (MBONs).\n",
    "\n",
    "    Paramters\n",
    "        network = previously trained RNN\n",
    "        trial_ls = list of interval functions that compose a trial\n",
    "        plt_ttl = title of plot\n",
    "        plt_lbl = labels for CS and US legends\n",
    "        T_vars: Tuple\n",
    "            T_vars[0] = T_int = length of trial (in seconds)\n",
    "            T_vars[1] = T_stim = length of time each stimulus is presented\n",
    "            T_vars[2] = dt = time step of simulations\n",
    "    \"\"\"\n",
    "\n",
    "    # Define plot font sizes\n",
    "    label_font = 18\n",
    "    title_font = 24\n",
    "    legend_font = 12\n",
    "\n",
    "    # Set labels\n",
    "    CS_labels = plt_lbl[0]\n",
    "    US_labels = plt_lbl[1]\n",
    "\n",
    "    # Set the time variables\n",
    "    T_int, T_stim, dt = T_vars\n",
    "    n_int = len(trial_ls)\n",
    "    plot_time = np.arange((int(T_int / dt) + 1) * n_int) * dt\n",
    "\n",
    "    # Run the network\n",
    "    network.run_eval(trial_ls=trial_ls, T_int=T_int, T_stim=T_stim, dt=dt,\n",
    "                     n_batch=1, **kwargs)\n",
    "    r_out = network.eval_rts[-1].numpy().squeeze()\n",
    "    vt = network.eval_vts[-1].numpy().squeeze()\n",
    "    vt_opt = network.eval_vt_opts[-1].numpy().squeeze()\n",
    "    CS_list = network.eval_CS_stim[-1]\n",
    "    US_list = network.eval_US_stim[-1]\n",
    "\n",
    "    # Plot the conditioning and test\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True,\n",
    "                                   gridspec_kw={'height_ratios': [1, 4]})\n",
    "    ax1.plot(plot_time, vt, label='Readout')\n",
    "    ax1.plot(plot_time, vt_opt, label='Target')\n",
    "    for i in range(len(CS_list)):\n",
    "        ax1.plot(plot_time, CS_list[i].squeeze(), label='{}'.format(CS_labels[i]))\n",
    "    for i in range(len(US_list)):\n",
    "        ax1.plot(plot_time, US_list[i].squeeze(), label='{}'.format(US_labels[i]))\n",
    "    ax1.set_ylabel('Value', fontsize=label_font)\n",
    "    ax1.set_title(plt_ttl, fontsize=title_font)\n",
    "    ax1.legend(fontsize=legend_font)\n",
    "\n",
    "    # Plot the activities of a few MBONs\n",
    "    plot_neurs = np.random.choice(network.n_mbon, size=8, replace=False)\n",
    "    r_max = np.max(r_out)\n",
    "    for i, n in enumerate(plot_neurs):\n",
    "        ax2.plot(plot_time, (r_out[n, :] / r_max) + (i * 2 / 3), '-k')\n",
    "    ax2.set_xlabel('Time', fontsize=label_font)\n",
    "    ax2.set_ylabel('Normalized Activity', fontsize=label_font)\n",
    "    ax2.set_yticks([])\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([60, 2])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the network\n",
    "net_type = 'first'\n",
    "if net_type == 'first':\n",
    "    network = FirstOrderCondRNN()\n",
    "elif net_type == 'extend':\n",
    "    network = ExtendedCondRNN()\n",
    "elif net_type == 'no_plast':\n",
    "    network = NoPlasticityRNN()\n",
    "elif net_type == 'continual':\n",
    "    network = ContinualRNN()\n",
    "\n",
    "for param in network.parameters():\n",
    "    print(param.shape)\n",
    "\n",
    "# Define the model's optimizer\n",
    "lr = 0.001\n",
    "optimizer = optim.RMSprop(network.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bool = False\n",
    "if train_bool:\n",
    "    if net_type == 'first' or net_type == 'extend':\n",
    "        # Classical conditioning\n",
    "        loss_hist = network.run_train(opti=optimizer, n_epoch=2000)\n",
    "    elif net_type == 'no_plast':\n",
    "        # No plasticity\n",
    "        loss_hist = network.run_train(opti=optimizer, T_int=40, n_epoch=2000, n_odors=10)\n",
    "    elif net_type == 'continual':\n",
    "        # Continual learning\n",
    "        loss_hist = network.run_train(opti=optimizer, T_int=200, n_epoch=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_bool:\n",
    "    # Plot the loss function\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "    axes[0].plot(loss_hist)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=label_font)\n",
    "    axes[0].set_ylabel('Loss', fontsize=label_font)\n",
    "    axes[1].plot(loss_hist[5:])\n",
    "    axes[1].set_xlabel('Epoch', fontsize=label_font)\n",
    "    axes[1].set_ylabel('Loss', fontsize=label_font)\n",
    "    fig.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(network.state_dict(), 'trained_N0_first_order_test.pt')\n",
    "# torch.save(network.state_dict(), 'trained_N0_no_plasticity_n10_test.pt')\n",
    "# torch.save(network.state_dict(), 'trained_N0_continual_test.pt')\n",
    "\n",
    "if not train_bool:\n",
    "    net_type = 'first'\n",
    "    if net_type == 'first':\n",
    "        network.load_state_dict(torch.load('trained_N0_first_order_test.pt'))\n",
    "    elif net_type == 'extend':\n",
    "        pass\n",
    "    elif net_type == 'no_plast':\n",
    "        network.load_state_dict(torch.load('trained_N0_no_plasticity_n10_test.pt'))\n",
    "    elif net_type == 'continual':\n",
    "        network.load_state_dict(torch.load('trained_N0_continual_test.pt'))\n",
    "    network.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if net_type == 'first':\n",
    "    trial_ls = [first_order_cond_csp, first_order_test]\n",
    "    plt_ttl = 'First-order conditioning (CS+)'\n",
    "    plt_lbl = (['CS+'], ['US'])\n",
    "elif net_type == 'extend':\n",
    "    plt_ttl = 'First-order conditioning (CS+)'\n",
    "    plt_lbl = (['CS+', 'CS-'], ['US'])\n",
    "elif net_type == 'no_plast':\n",
    "    plt_ttl = 'No Plasticity'\n",
    "    plt_lbl = (['CS+', 'CS-'], ['US'])\n",
    "    trial_ls = [no_plasticity_trial]\n",
    "elif net_type == 'continual':\n",
    "    network = ContinualRNN()\n",
    "T_int = 40\n",
    "T_stim = 2\n",
    "dt = 0.5\n",
    "T_vars = (T_int, T_stim, dt)\n",
    "plot_trial(network, trial_ls, plt_ttl, plt_lbl, T_vars, p_ctrl=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001071972133977397 0.0019836637511627666\n"
     ]
    }
   ],
   "source": [
    "network.run_eval(trial_ls=trial_ls, n_trial=50, T_int=40, pos_val=None)\n",
    "loss_hist = network.eval_loss\n",
    "print(np.mean(loss_hist), np.std(loss_hist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000e+00,  1.9135e-03,  2.2242e-03,  2.0691e-03,  1.4442e-03,\n",
      "         1.2696e-03,  1.3078e-03,  1.3725e-03,  1.2672e-03,  1.0603e-03,\n",
      "         9.0855e-04,  9.5701e-04,  1.0932e-03,  1.1991e-03,  1.2444e-03,\n",
      "         1.2191e-03,  1.1533e-03,  1.0765e-03,  5.1079e-03,  1.8953e-02,\n",
      "         2.4177e-02,  2.3339e-02,  1.3952e-02,  1.0924e-02,  8.8714e-03,\n",
      "         1.1909e-02,  9.3191e-03,  5.7343e-03,  4.2324e-03,  3.6625e-03,\n",
      "         2.8020e-03,  1.7353e-03,  2.0923e-03,  1.6853e-03,  6.0493e-04,\n",
      "         1.0018e-03,  1.4840e-03,  1.5628e-03,  1.4429e-03,  1.3754e-03,\n",
      "         1.4165e-03,  1.4465e-03,  1.4802e-03,  1.5888e-03,  1.6430e-03,\n",
      "         1.6133e-03,  1.5410e-03,  1.4498e-03,  1.3842e-03,  1.3432e-03,\n",
      "         1.3163e-03,  1.3030e-03,  1.3063e-03,  1.3240e-03,  1.3482e-03,\n",
      "         1.3789e-03,  1.4137e-03,  1.4110e-03,  1.3797e-03,  1.3424e-03,\n",
      "         1.3089e-03,  1.2762e-03,  1.2810e-03,  1.2552e-03,  1.4598e-03,\n",
      "         1.7496e-03,  2.0044e-03,  2.2524e-03,  2.6120e-03,  3.1187e-03,\n",
      "         3.5262e-03,  3.6542e-03,  3.4618e-03,  3.1998e-03,  2.8025e-03,\n",
      "         2.3173e-03,  1.9280e-03,  1.6360e-03,  1.4227e-03,  1.2601e-03,\n",
      "         1.1337e-03,  0.0000e+00,  1.9135e-03,  2.2242e-03,  2.0691e-03,\n",
      "         1.4442e-03,  1.2696e-03,  1.3078e-03,  1.3725e-03,  1.2672e-03,\n",
      "         1.0603e-03,  9.0855e-04,  9.5701e-04,  1.0932e-03,  1.1991e-03,\n",
      "         1.2444e-03,  1.2191e-03,  1.1533e-03,  1.0765e-03,  1.0075e-03,\n",
      "         9.5423e-04,  9.1746e-04,  8.9431e-04, -9.8862e-01, -9.8528e-01,\n",
      "        -9.8202e-01, -9.7334e-01,  1.1708e-02,  1.0550e-02,  9.0517e-03,\n",
      "         8.1819e-03,  6.7434e-03,  4.3236e-03,  3.1945e-03,  3.7058e-03,\n",
      "         2.1619e-03,  1.3328e-03,  2.0038e-03,  2.3575e-03,  1.9956e-03,\n",
      "         1.4985e-03,  1.1868e-03,  1.0171e-03,  9.6743e-04,  9.3102e-04,\n",
      "         9.0815e-04,  8.7197e-04,  8.3357e-04,  8.2094e-04,  8.2536e-04,\n",
      "         8.3877e-04,  8.5339e-04,  8.6362e-04,  8.6724e-04,  8.6499e-04,\n",
      "         8.5909e-04,  8.5196e-04,  8.4542e-04,  8.4057e-04,  8.3763e-04,\n",
      "         8.3624e-04,  8.3715e-04,  8.3971e-04,  8.4278e-04,  8.4561e-04,\n",
      "         8.4794e-04,  8.4976e-04,  8.5119e-04,  8.5233e-04,  8.5326e-04,\n",
      "         8.5399e-04,  8.5459e-04,  8.5506e-04,  8.5541e-04,  8.5569e-04,\n",
      "         8.5589e-04,  8.5606e-04,  8.5617e-04,  8.5627e-04,  8.5635e-04,\n",
      "         8.5641e-04,  8.5647e-04])\n",
      "tensor([0.0000e+00, 1.9135e-03, 2.2242e-03, 2.0691e-03, 1.4442e-03, 1.2696e-03,\n",
      "        1.3078e-03, 1.3725e-03, 1.2672e-03, 1.0603e-03, 9.0855e-04, 9.5701e-04,\n",
      "        6.6318e-03, 1.8975e-02, 2.3096e-02, 2.2655e-02, 1.4055e-02, 7.9232e-03,\n",
      "        6.1497e-03, 1.9863e-03, 4.3826e-03, 3.1973e-03, 1.3178e-03, 1.6765e-03,\n",
      "        1.8693e-03, 2.5154e-03, 2.1299e-03, 1.7741e-03, 1.8978e-03, 2.0525e-03,\n",
      "        1.9248e-03, 1.3621e-03, 1.0203e-03, 7.3323e-04, 8.3711e-04, 9.0932e-04,\n",
      "        7.9141e-04, 8.0775e-04, 9.8623e-04, 1.1504e-03, 1.1783e-03, 1.0670e-03,\n",
      "        1.0322e-03, 1.0322e-03, 1.0477e-03, 1.0546e-03, 1.0427e-03, 1.0168e-03,\n",
      "        9.8649e-04, 9.6214e-04, 9.4311e-04, 9.2733e-04, 9.1402e-04, 9.0319e-04,\n",
      "        8.9483e-04, 8.8852e-04, 8.8362e-04, 8.7958e-04, 8.7600e-04, 8.7270e-04,\n",
      "        8.6969e-04, 8.6704e-04, 8.6479e-04, 8.6299e-04, 8.6159e-04, 8.6055e-04,\n",
      "        8.5977e-04, 8.5921e-04, 8.5877e-04, 8.5843e-04, 8.5816e-04, 8.5792e-04,\n",
      "        8.5772e-04, 8.5754e-04, 8.5739e-04, 8.5726e-04, 8.5715e-04, 8.5707e-04,\n",
      "        8.5700e-04, 8.5695e-04, 8.5689e-04, 0.0000e+00, 1.9135e-03, 2.2242e-03,\n",
      "        2.0691e-03, 1.4442e-03, 1.2696e-03, 1.3078e-03, 1.3725e-03, 1.2672e-03,\n",
      "        1.0603e-03, 9.0855e-04, 9.5701e-04, 1.0932e-03, 1.1991e-03, 1.2444e-03,\n",
      "        1.2191e-03, 1.1533e-03, 1.0765e-03, 5.3313e-01, 6.1188e-01, 6.0051e-01,\n",
      "        5.4032e-01, 3.1260e-02, 1.1122e-02, 1.6681e-02, 1.4408e-02, 1.0839e-02,\n",
      "        5.8323e-03, 1.9497e-03, 6.3341e-04, 1.5456e-03, 2.2025e-03, 2.1374e-03,\n",
      "        2.3990e-03, 2.6426e-03, 2.3205e-03, 1.6566e-03, 1.0274e-03, 6.2708e-04,\n",
      "        4.6428e-04, 4.1770e-04, 4.1773e-04, 4.4296e-04, 4.8376e-04, 5.3254e-04,\n",
      "        5.8429e-04, 6.3526e-04, 6.8233e-04, 7.2316e-04, 7.5651e-04, 7.8220e-04,\n",
      "        8.0096e-04, 8.1399e-04, 8.2268e-04, 8.2886e-04, 8.3348e-04, 8.3706e-04,\n",
      "        8.3994e-04, 8.4230e-04, 8.4428e-04, 8.4599e-04, 8.4751e-04, 8.4888e-04,\n",
      "        8.5012e-04, 8.5126e-04, 8.5225e-04, 8.5311e-04, 8.5383e-04, 8.5443e-04,\n",
      "        8.5489e-04, 8.5527e-04, 8.5556e-04, 8.5578e-04, 8.5596e-04, 8.5610e-04,\n",
      "        8.5621e-04, 8.5630e-04, 8.5637e-04, 8.5643e-04, 8.5648e-04, 8.5652e-04])\n"
     ]
    }
   ],
   "source": [
    "# print(network.train_odors)\n",
    "# print(network.W_readout)\n",
    "# print(network.eval_rts[0][0, :network.n_mbon, :])\n",
    "# print(network.eval_rts[1][0, :network.n_mbon, :])\n",
    "print(network.eval_vts[0])\n",
    "print(network.eval_vts[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
