{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "# from numpy.linalg import svd as svd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Define plot font sizes\n",
    "label_font = 18\n",
    "title_font = 24\n",
    "legend_font = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model Class - Classical Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drosophila_RNN(nn.Module):\n",
    "    def __init__(self, KC_size=200, MBON_size=20, DAN_size=20, FBN_size=60, ext_size=2, out_size=1, net_seed=1234):\n",
    "        super(Drosophila_RNN, self).__init__()\n",
    "        # Set the seeds\n",
    "#         np.random.seed(net_seed)\n",
    "#         torch.manual_seed(net_seed)\n",
    "        # Set constants\n",
    "        self.KC_MBON_min = 0. # Minimum synaptic weight\n",
    "        self.KC_MBON_max = 0.05 # Maximum synaptic weight\n",
    "        self.tau_w = 5 # Time scale of KC->MBON LTD/LTP (plasticity)\n",
    "        self.tau_r = 1 # Time scale of output circuitry activity\n",
    "        # Set the sizes of layers\n",
    "        self.N_KC = KC_size\n",
    "        self.N_MBON = MBON_size\n",
    "        self.N_FBN = FBN_size\n",
    "        self.N_DAN = DAN_size\n",
    "        self.N_recur = MBON_size + FBN_size + DAN_size\n",
    "        self.N_ext = ext_size\n",
    "        self.N_out = out_size\n",
    "        # Define updatable network parameters\n",
    "#         seed_num = net_seed\n",
    "        seed_num = None\n",
    "        sqrt2 = torch.sqrt(torch.tensor(2, dtype=torch.float))\n",
    "        mean_MBON = torch.zeros((self.N_recur, MBON_size))\n",
    "        mean_FBN = torch.zeros((self.N_recur, FBN_size))\n",
    "        mean_DAN = torch.zeros((self.N_recur, DAN_size))\n",
    "        W_MBON = torch.normal(mean_MBON, torch.sqrt(1 / (sqrt2 * MBON_size)), generator=seed_num)\n",
    "        W_FBN = torch.normal(mean_FBN, torch.sqrt(1 / (sqrt2 * FBN_size)), generator=seed_num)\n",
    "        W_DAN = torch.normal(mean_DAN, torch.sqrt(1 / (sqrt2 * DAN_size)), generator=seed_num)\n",
    "        self.W_recur = nn.Parameter(torch.cat((W_MBON, W_FBN, W_DAN), dim=1), requires_grad=True)\n",
    "        self.W_ext = nn.Parameter(torch.randn(FBN_size, ext_size), requires_grad=True)\n",
    "        mean_readout = torch.zeros((out_size, MBON_size))\n",
    "        std_readout = 1 / torch.sqrt(torch.tensor(MBON_size, dtype=torch.float))\n",
    "        self.W_readout = nn.Parameter(torch.normal(mean_readout, std_readout, generator=seed_num), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.ones(self.N_recur) * 0.1, requires_grad=True)\n",
    "        self.beta = nn.Parameter(torch.ones(KC_size) * 0.01, requires_grad=True)\n",
    "        \n",
    "            \n",
    "    def forward(self, r_KC, r_ext, time, epoch, W0=None, n_epochs=5000, n_batch=30, nps_bool=True):\n",
    "        \"\"\" Defines the forward pass of the RNN\n",
    "        \n",
    "        Synaptic weights from the Keyon cells to the mushroom body output neurons (MBONs) are updated\n",
    "        dynamically. All other weights are network parameters. The synaptic connections between Keyon\n",
    "        Cells (KCs) and MBONs are updated using a LTP/LTD rule (see Figure 1B of Jiang 2020), which\n",
    "        models dopamine-gated neural plasticity on short time scale (behavioural learning). Early\n",
    "        KC->MBON weight saturation is prevented by a linearly increasing learning rate (see Eq. 12).\n",
    "        Additionally, non-specific potentiation is included as a form of homeostasis (see Eq. 5).\n",
    "        \n",
    "        The KC->MBON weights are constrained to the range [0, 0.05].\n",
    "        MBONs receive external input from Keyon cells (r_KC i.e. 'odors').\n",
    "        Feedback neurons (FBNs) receive external contextual input (r_ext i.e. 'context').\n",
    "        DAN->MBON weights are permanently set to zero. DANs receive no external input.\n",
    "\n",
    "        Inputs\n",
    "            r_KC = activity of the Kenyon cell neurons (representing odors)\n",
    "            r_ext = context signals (representing the conditioning context)\n",
    "            time = time vector for a single interval\n",
    "            epoch = current training epoch\n",
    "            W0 = initial weights for KC->MBON connections\n",
    "            n_epochs = total number of training epochs\n",
    "            n_batch = number of trials in batch\n",
    "            nps_bool = boolean indicating whether to include non-specific potentiation\n",
    "\n",
    "        Returns\n",
    "            r_recur: list of torch.ndarray(batch_size, N_MBON + N_FBN + N_DAN)\n",
    "                = time series of activities in the output circuitry\n",
    "            Wt: list of torch.ndarray(batch_size, N_MBON + N_FBN + N_DAN, N_MBON + N_FBN + N_DAN)\n",
    "                = time series of KC->MBON weights (represent dopaminergic plasticity)\n",
    "            readout: list of torch.ndarray(batch_size, 1)\n",
    "                = time series of valence readouts (represents behaviour)\n",
    "        \"\"\"\n",
    "\n",
    "        # Define the time step of the simulation\n",
    "        dt = np.diff(time)[0]\n",
    "\n",
    "        # Initialize output circuit firing rates for each trial\n",
    "        r_init = torch.ones((n_batch, self.N_recur)) * 0.1\n",
    "        r_init[:, :self.N_MBON] = 0\n",
    "        r_recur = [r_init]\n",
    "\n",
    "        # Initialize the eligibility traces and readout\n",
    "        r_bar_KC = r_KC[:, :, 0]\n",
    "        r_bar_DAN = r_recur[-1][:, -self.N_DAN:]\n",
    "        readout = [torch.einsum('bom, bm -> bo', self.W_readout.repeat(n_batch, 1, 1), r_recur[-1][:, :self.N_MBON]).squeeze()]\n",
    "\n",
    "        # Initialize the dynamic weight variables, accounting for early saturation (see Eq. 12)\n",
    "#         wt = [torch.zeros((n_batch, self.N_MBON, self.N_KC))]\n",
    "        wt = [torch.ones((n_batch, self.N_MBON, self.N_KC)) * self.KC_MBON_max]\n",
    "        W_KC_MBON_0 = torch.ones((n_batch, self.N_MBON, self.N_KC)) * self.KC_MBON_max\n",
    "        if W0 is None:\n",
    "            W0 = W_KC_MBON_0\n",
    "        # Calculate the saturation parameter and modify initial weights\n",
    "        x_sat = min(1, (epoch / (n_epochs / 2)))\n",
    "        W0 = (1 - x_sat) * W_KC_MBON_0 + x_sat * W0\n",
    "        W_KC_MBON = [W0]\n",
    "\n",
    "        # Set the weights DAN->MBON to zero\n",
    "        W_recur = self.W_recur.clone()\n",
    "        W_recur[:self.N_MBON, -self.N_DAN:] = 0\n",
    "\n",
    "        # Rectify the potentiation parameter\n",
    "        beta = F.relu(self.beta.clone())\n",
    "\n",
    "        # Update activity for each time step\n",
    "        for t in range(time.size()[0] - 1):\n",
    "            # Define the input to the output circuitry\n",
    "            I_KC_MBON = torch.einsum('bmk, bk -> bm', W_KC_MBON[-1], r_KC[:, :, t])\n",
    "            I_FBN = torch.einsum('bfe, be -> bf', self.W_ext.repeat(n_batch, 1, 1), r_ext[:, :, t])\n",
    "            I = torch.zeros((n_batch, self.N_recur))\n",
    "            I[:, :self.N_MBON] = I_KC_MBON\n",
    "            I[:, self.N_MBON:self.N_MBON + self.N_FBN] = I_FBN\n",
    "\n",
    "            # Update the output circuitry activity (see Eq. 1)\n",
    "            Wr_prod = torch.einsum('bsr, br -> bs', W_recur.repeat(n_batch, 1, 1), r_recur[-1])\n",
    "            dr = (-r_recur[-1] + F.relu(Wr_prod + self.bias.repeat(n_batch, 1) + I)) / self.tau_r\n",
    "            r_recur.append(r_recur[-1] + dr * dt)\n",
    "\n",
    "            # Update KC->MBON plasticity variables\n",
    "            # Calculate the eligibility traces (represent LTP/LTD)\n",
    "            r_bar_KC = r_bar_KC + (r_KC[:, :, t] - r_bar_KC) * dt / self.tau_w\n",
    "            r_bar_DAN = r_bar_DAN + (r_recur[-1][:, -self.N_DAN:] - r_bar_DAN) * dt / self.tau_w\n",
    "            # Update the dynamic weight variable (see Eq. 5)\n",
    "            prod1 = torch.einsum('bd, bk -> bdk', r_bar_DAN, r_KC[:, :, t])\n",
    "            prod2 = torch.einsum('bd, bk -> bdk', r_recur[-1][:, -self.N_DAN:], r_bar_KC)\n",
    "            # Include non-specific potentiation (unless control condition)\n",
    "            if nps_bool:\n",
    "                # Constrain the potentiation parameter to be positive\n",
    "                prod3 = torch.einsum('bd, bk -> bdk', r_bar_DAN, beta.repeat(n_batch, 1))\n",
    "            else:\n",
    "                prod3 = torch.zeros_like(prod2)\n",
    "            dw = (prod1 - prod2 + prod3)\n",
    "            wt.append(wt[-1] + dw * dt)\n",
    "            # Update the KC->MBON weights (see Eq. 8)\n",
    "            dW = (-W_KC_MBON[-1] + wt[-1]) / self.tau_w\n",
    "            W_tp1 = W_KC_MBON[-1] + dW * dt\n",
    "            # Clip the KC->MBON weights to the range [0, 0.05]\n",
    "            W_KC_MBON.append(torch.clamp(W_tp1, self.KC_MBON_min, self.KC_MBON_max))\n",
    "\n",
    "            # Calculate the readout (see Eq. 2)\n",
    "            readout.append(torch.squeeze(torch.einsum('bom, bm -> bo', self.W_readout.repeat(n_batch, 1, 1), r_recur[-1][:, :self.N_MBON])))\n",
    "\n",
    "        return r_recur, W_KC_MBON, readout\n",
    "\n",
    "        \n",
    "# Clipping weights between [0, 0.05]\n",
    "# https://discuss.pytorch.org/t/how-to-do-constrained-optimization-in-pytorch/60122\n",
    "# https://discuss.pytorch.org/t/set-constraints-on-parameters-or-layers/23620\n",
    "# https://discuss.pytorch.org/t/restrict-range-of-variable-during-gradient-descent/1933/4\n",
    "\n",
    "# Setting DAN->MBON weights to zero\n",
    "# https://pytorch.org/docs/stable/generated/torch.triu.html\n",
    "\n",
    "# Broadcasting using einsum\n",
    "# https://github.com/pytorch/pytorch/issues/15671\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function for conditioning tasks\n",
    "def cond_loss(vt, vt_opt, r_DAN, lam=0.1):\n",
    "    \"\"\" Calculates the loss for conditioning tasks.\n",
    "    \n",
    "    Composed of an MSE cost based on the difference between output and\n",
    "    target valence, and a regularization cost that penalizes excess\n",
    "    dopaminergic activity. Reference Eqs. (3) and (9) in Jiang 2020.\n",
    "    \n",
    "    Parameters\n",
    "        vt = time dependent valence output of network\n",
    "        vt_opt = target valence (must be a torch tensor)\n",
    "        r_DAN = time series of dopaminergic neuron activities\n",
    "        lam = regularization constant\n",
    "    \n",
    "    Returns\n",
    "        loss_tot = scalar loss used in backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the baseline DAN activity\n",
    "    DAN_baseline = 0.1\n",
    "    \n",
    "    # Calculate the MSE loss of the valence\n",
    "    v_sum = torch.mean((vt - vt_opt)**2, dim=1)\n",
    "    v_loss = torch.mean(v_sum)\n",
    "    \n",
    "    # Calculate regularization term\n",
    "    r_sum = torch.sum(F.relu(r_DAN - 0.1)**2, dim=1)\n",
    "    r_loss = lam * torch.mean(r_sum, dim=1)\n",
    "    \n",
    "    # Calculate the summed loss (size = n_batch)\n",
    "    loss = v_loss + r_loss\n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    loss_tot = torch.mean(loss)\n",
    "    \n",
    "    return loss_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a training function for first-order conditioning\n",
    "def train_net(network, T_int=200, T_stim=2, dt=0.5, n_epochs=5000, n_batch=30, clip=0.001, train=True, nps=True):\n",
    "    \"\"\" Trains a network on a continual learning task.\n",
    "    \n",
    "    Task includes repetitive presentation of four stimuli (two CS+ and two CS-) per trial.\n",
    "    The presentation times of each stimulus are drawn from a Poisson process, and the inputs\n",
    "    are generated by a function. The dopamine gated plasticity has an additional non-specific\n",
    "    potentiation factor (see Eq. 5 of Jiang 2020), which is included in the forward() function.\n",
    "    KC->MBON weights are set once at the beginning of optimization, and then continue forward\n",
    "    between each successive trial (see p14 of Jiang 2020).\n",
    "    \n",
    "    Parameters\n",
    "        network = RNN network to be trained or ran\n",
    "        T_int = length of task intervals (eg conditioning, test, extinction)\n",
    "        T_stim = length of time each stimulus is presented\n",
    "        dt = time step of simulations\n",
    "        n_epochs = number of epochs to train over\n",
    "        n_batch = number of trials in mini-batch\n",
    "        clip = maximum gradient allowed during training\n",
    "        train = boolean indicating whether to perform backprop\n",
    "        nps = boolean indicating whether to include non-specific potentiation\n",
    "        \n",
    "    Returns\n",
    "        r_out_epoch = output circuit neuron activities for final epoch\n",
    "        Wt_epoch = KC->MBON weights for final epoch\n",
    "        vt_epoch = readout (i.e. valence) for final epoch\n",
    "        vt_opt = target valence for final epoch\n",
    "        loss_hist = list of losses for all epochs\n",
    "        ls_stims = list of stimulus time series for plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Interval time vector\n",
    "    time_int = torch.arange(0, T_int + dt/10, dt)\n",
    "\n",
    "    # Neuron population sizes\n",
    "    n_KC = network.N_KC\n",
    "    n_ext = network.N_ext\n",
    "    n_MBON = network.N_MBON\n",
    "    # Set the intial KC->MBON weight values\n",
    "    W_KC_MBON = torch.ones((n_batch, n_MBON, n_KC)) * network.KC_MBON_max\n",
    "\n",
    "    # List to store losses\n",
    "    loss_hist = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Generate the odor (r_KC) and context (r_ext) inputs, and target valence (vt_opt)\n",
    "        r_KC, r_ext, vt_opt, ls_stims = continual_inputs(T_int, T_stim, dt, n_KC, n_ext, n_batch)\n",
    "\n",
    "        # Run the forward model\n",
    "        r_outs, Wts, vts = network(r_KC, r_ext, time_int, epoch, W_KC_MBON, n_epochs, n_batch, nps)\n",
    "        # Set the initial KC->MBON weights for the next trial\n",
    "        W_KC_MBON = Wts[-1].detach()\n",
    "\n",
    "        # Concatenate the activities, weights and valences\n",
    "        r_out_epoch = torch.stack(r_outs, dim=-1)\n",
    "        Wt_epoch = torch.stack(Wts, dim=-1)\n",
    "        vt_epoch = torch.stack(vts, dim=-1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = cond_loss(vt_epoch, vt_opt, r_out_epoch[:, -network.N_DAN:, :])\n",
    "\n",
    "        if train:\n",
    "            # Update the network parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(network.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print an update\n",
    "        if epoch % 500 == 0:\n",
    "            print(epoch, loss.item())\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "    return r_out_epoch, Wt_epoch, vt_epoch, vt_opt, loss_hist, ls_stims\n",
    "\n",
    "# https://discuss.pytorch.org/t/proper-way-to-do-gradient-clipping/191/13\n",
    "# https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48\n",
    "# https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for each of the conditioning tasks\n",
    "# Some detail provided in Jiang 2020 -> Methods -> Continual learning\n",
    "# def continual_inputs(stim_times, stim_len, time_len, CS_odors, n_KC, n_ext, n_batch, plot=None):\n",
    "# def continual_inputs(stim_times, stim_len, time_len, n_KC, n_ext, n_batch, plot=None):\n",
    "def continual_inputs(T_int, T_stim, dt, n_KC, n_ext, n_batch):\n",
    "    \"\"\" Generates inputs for continual learning task.\n",
    "    \n",
    "    All trials are composed of Poisson distributed presentations of two CS+ and two CS- (US omitted)\n",
    "    stimuli. CS+ are presented with a US in all cases (target valence set for CS+ after first\n",
    "    presentation). To account for the sequential nature of numerical simulations, the target valence\n",
    "    is set to begin one time step after stimulus onset.\n",
    "    \n",
    "    Parameters\n",
    "        T_int = length of task interval\n",
    "        T_stim = length of time each stimulus is presented\n",
    "        dt = time step of simulations\n",
    "        CS_odors = list of two CS+ and two CS- stimuli for each trial in batch\n",
    "        n_KC = number of Kenyon cell input neurons\n",
    "        n_ext = number of contextual input neurons\n",
    "        n_batch = number of trials in mini-batch\n",
    "        plot = used when plot function is called, indicates which task to plot\n",
    "        \n",
    "    Returns\n",
    "        r_KCt = list of odor (KC) input time series arrays for each interval\n",
    "        r_extt = list of context (ext) input time series arrays for each interval\n",
    "        vt_opt = time series of target valence for plotting and loss calculations\n",
    "        ls_stims = list of stimulus time series for plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of odors in trial\n",
    "    n_odors = 4\n",
    "    # Number of active neurons in an odor\n",
    "    n_ones = int(n_KC * 0.1)\n",
    "    # Poisson rate of stimulus presentations\n",
    "    stim_rate = 2 / T_int\n",
    "    # Length of stimulus in indices\n",
    "    stim_len = int(T_stim / dt)\n",
    "    # Length of trial in indices\n",
    "    time_len = int(T_int / dt + 1)\n",
    "    \n",
    "    # Initialize activity matrices\n",
    "    r_KCt = torch.zeros(n_batch, n_KC, time_len)\n",
    "    r_extt = torch.zeros(n_batch, n_ext, time_len)\n",
    "    \n",
    "    # Initialize lists and arrays to store stimulus time series\n",
    "    ls_CS = []\n",
    "    time_US_all = torch.zeros(n_batch, time_len)\n",
    "    vt_opt = torch.zeros(n_batch, time_len)\n",
    "    \n",
    "    # For each batch, randomly generate different odors and presentation times\n",
    "    for i in range(n_odors):\n",
    "        # Initialize the CS time matrix\n",
    "        time_CS = torch.zeros(n_batch, time_len)\n",
    "        time_US = torch.zeros_like(time_CS)\n",
    "\n",
    "        # Conditioned stimuli (CS) = odors\n",
    "        r_KC = torch.zeros(n_batch, n_KC)\n",
    "        # Unconditioned stimuli (US) = context\n",
    "        r_ext = torch.multinomial(torch.ones(n_batch, n_ext), n_ext)\n",
    "            \n",
    "        # For each trial\n",
    "        for b in range(n_batch):\n",
    "            # Define an odor (CS)\n",
    "            r_KC_inds = torch.multinomial(torch.ones(n_KC), n_ones)\n",
    "            r_KC[b, r_KC_inds] = 1\n",
    "\n",
    "            # Generate a list of stimulus presentation times\n",
    "            stim_times = []\n",
    "            last_time = 0\n",
    "            while True:\n",
    "                stim_isi = -torch.log(torch.rand(1)) / stim_rate\n",
    "                next_time = last_time + stim_isi\n",
    "                if next_time < (T_int - 2 * T_stim):\n",
    "                    # Stimulus times are indices (not times)\n",
    "                    stim_times.append((next_time / dt).int())\n",
    "                    last_time += stim_isi\n",
    "                # Ensure at least one presentation of each stimuli\n",
    "                elif last_time == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "            for j, st in enumerate(stim_times):\n",
    "                # Set the CS input times\n",
    "                stim_inds = st + torch.arange(stim_len)\n",
    "                time_CS[b, stim_inds] = 1\n",
    "\n",
    "                # For CS+ odors, set US and the valence\n",
    "                if i < 2:\n",
    "                    # Set the US input times\n",
    "                    time_US[b, (stim_inds + stim_len)] = 1\n",
    "                    # Set a target valence on every presentation but the first\n",
    "                    if j > 0:\n",
    "                        if r_ext[b, 0] == 1:\n",
    "                            vt_opt[b, (stim_inds + 1)] = 1\n",
    "                        else:\n",
    "                            vt_opt[b, (stim_inds + 1)] = -1\n",
    "\n",
    "        # Calculate the stimulus time series (KC = CS, ext = US)\n",
    "        r_KCt += torch.einsum('bm, mbt -> bmt', r_KC, time_CS.repeat(n_KC, 1, 1))\n",
    "        r_extt += torch.einsum('bm, mbt -> bmt', r_ext, time_US.repeat(n_ext, 1, 1))\n",
    "        ls_CS += time_CS\n",
    "        time_US_all += time_US\n",
    "        \n",
    "    # Make a list of stimulus times to plot\n",
    "    ls_stims = ls_CS + [time_US_all]\n",
    "        \n",
    "    return r_KCt, r_extt, vt_opt, ls_stims\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trial(network, dt=0.5, nps=True):\n",
    "    \"\"\" Plots a figure similar to Figure 2 from Jiang 2020.\n",
    "    \n",
    "    Runs the network using a novel combination of conditioned and unconditioned stimuli,\n",
    "    then prints the results. Top: time series of the various stimuli (CS and US), as well as\n",
    "    the target valence and readout. Bottom: activity of eight randomly chosen mushroom body\n",
    "    output neurons (MBONs).\n",
    "    \n",
    "    Paramters\n",
    "        network = previously trained RNN\n",
    "        dt = time step of the simulation/plot\n",
    "        nps = boolean indicating whether to include non-specific potentiation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define plot font sizes\n",
    "    label_font = 18\n",
    "    title_font = 24\n",
    "    legend_font = 12\n",
    "\n",
    "    # Run the network\n",
    "    r_out, Wt, vt, vt_opt, loss_hist, stim_ls = train_net(network, dt=dt, n_epochs=1, n_batch=1, train=False, nps=nps)\n",
    "    r_out = r_out.detach().numpy().squeeze()\n",
    "    vt = vt.detach().numpy().squeeze()\n",
    "    vt_opt = vt_opt.detach().numpy().squeeze()\n",
    "    plot_US = stim_ls[-1].numpy().squeeze()\n",
    "    plot_time = np.arange(plot_US.size) * dt\n",
    "\n",
    "    # Plot the conditioning and test\n",
    "    CS_labels = ['CS1+', 'CS2+', 'CS1-', 'CS2-']\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), sharex=True, gridspec_kw={'height_ratios': [1, 4]})\n",
    "    ax1.plot(plot_time, vt, label='Readout')\n",
    "    ax1.plot(plot_time, vt_opt, label='Target')\n",
    "    for i in range(len(stim_ls) - 1):\n",
    "        plot_CS = stim_ls[i].numpy().squeeze()\n",
    "        ax1.plot(plot_time, plot_CS, label=CS_labels[i])\n",
    "    ax1.plot(plot_time, plot_US, label='US')\n",
    "    ax1.set_ylabel('Value', fontsize=label_font)\n",
    "    ax1.set_title('Continual Learning', fontsize=title_font)\n",
    "    ax1.legend(fontsize=legend_font)\n",
    "\n",
    "    # Plot the activities of a few MBONs\n",
    "    plot_neurs = np.random.choice(network.N_MBON, size=8, replace=False)\n",
    "    r_max = np.max(r_out)\n",
    "    for i, n in enumerate(plot_neurs):\n",
    "        ax2.plot(plot_time, (r_out[n, :] / r_max) + (i * 2 / 3), '-k')\n",
    "    ax2.set_xlabel('Time', fontsize=label_font)\n",
    "    ax2.set_ylabel('Normalized Activity', fontsize=label_font)\n",
    "    ax2.set_yticks([])\n",
    "    fig.tight_layout();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([60, 2])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([100])\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "classic_net = Drosophila_RNN()\n",
    "for param in classic_net.parameters():\n",
    "    print(param.shape)\n",
    "#     print(param)\n",
    "# print(classic_net.N_DAN)\n",
    "\n",
    "# Define the model's optimizer\n",
    "lr = 0.001\n",
    "optimizer = optim.RMSprop(classic_net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3226288855075836\n",
      "500 0.003785729641094804\n",
      "1000 0.004152434412389994\n",
      "1500 0.0031479746103286743\n",
      "2000 0.0019959791097790003\n",
      "2500 0.0022149730939418077\n"
     ]
    }
   ],
   "source": [
    "train_bool = True\n",
    "if train_bool:\n",
    "    r_out, Wt, vt, vt_opt, loss_hist, _ = train_net(classic_net, n_epochs=3000)\n",
    "#     r_out, Wt, vt, vt_opt, loss_hist, _ = train_net(classic_net, nps=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "axes[0].plot(loss_hist)\n",
    "axes[0].set_xlabel('Epoch', fontsize=label_font)\n",
    "axes[0].set_ylabel('Loss', fontsize=label_font)\n",
    "axes[1].plot(loss_hist[5:])\n",
    "axes[1].set_xlabel('Epoch', fontsize=label_font)\n",
    "axes[1].set_ylabel('Loss', fontsize=label_font)\n",
    "fig.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(classic_net.state_dict(), 'trained_N7_continual_learning.pt')\n",
    "# torch.save(classic_net.state_dict(), 'trained_N8_continual_learning_no_potentiation.pt')\n",
    "\n",
    "# https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some results to compare to Fig 2A/B in paper\n",
    "if not train_bool:\n",
    "#     classic_net.load_state_dict(torch.load('trained_N7_continual_learning.pt'))\n",
    "#     classic_net.load_state_dict(torch.load('trained_N8_continual_learning_no_potentiation.pt'))\n",
    "    classic_net.eval()\n",
    "\n",
    "# print_trial(classic_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0017397092888131738\n",
      "0.002702623914228752 0.0007558835858146255\n"
     ]
    }
   ],
   "source": [
    "r_out, Wt, vt, vt_opt, loss_hist, _ = train_net(classic_net, n_epochs=200, train=False)\n",
    "# r_out, Wt, vt, vt_opt, loss_hist, _ = classic_net.train_net(opti=optimizer, n_epoch=200, n_odors=4)\n",
    "print(np.mean(loss_hist), np.std(loss_hist))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
